{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaf1reU6AQOf"
      },
      "source": [
        "Hemos optado por sólo codificar en inglés, pero las explicaciones y comentarios del código estarán en español."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv8oEM-z3l_i"
      },
      "source": [
        "Primero que nada, instalamos lo necesario e importamos las librerías necesarias. Si algún módulo no se ha importado totalmente, es porque el trabajo está desarrollado en Google Colab: por el entorno, a veces no es necesario importar todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyVS1dwR3VC0"
      },
      "outputs": [],
      "source": [
        "!pip install osmnx networkx geopandas pandas shapely pyproj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUy2Mjqr3Oup"
      },
      "outputs": [],
      "source": [
        "!pip install esda splot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E_-lo4-3Ghx"
      },
      "outputs": [],
      "source": [
        "!pip install \"h3<4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqiXiwk62_ym"
      },
      "outputs": [],
      "source": [
        "!pip install folium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s8kWpVR3jEf"
      },
      "outputs": [],
      "source": [
        "# Importamos todas las librerías a utilizar:\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm # Para barras de progreso, ayuda mucho ya que la API tiene delay de 1s por cada request!!\n",
        "from shapely.geometry import Point, Polygon\n",
        "from shapely import wkt # Lo usaremos para parsear algunas columnas de algunos datasets a geometry, en un GDF.\n",
        "import geemap,ee\n",
        "import json\n",
        "\n",
        "import h3\n",
        "import numpy as np\n",
        "import folium\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "import libpysal.weights\n",
        "from esda.moran import Moran\n",
        "from splot.esda import plot_moran\n",
        "\n",
        "import osmnx as ox\n",
        "from tqdm.auto import tqdm\n",
        "import networkx as nx\n",
        "\n",
        "from branca.colormap import linear # Para la paleta de colores\n",
        "\n",
        "import os # Para compatibilidad del SO, unión de rutas con path.join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYl6tJA4PEbB"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-aesmatias')\n",
        "# Aqui, por google colab no tuve que usar un token, pero se requiere autenticacion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX4_dJ46bfm0"
      },
      "outputs": [],
      "source": [
        "# Esto fuerza a que se actualicen los directorios y sus contenidos en Google Colab\n",
        "!ls shapefiles\n",
        "!ls datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqT6Q5pQGd2"
      },
      "source": [
        "Descargamos el Shapefile de USA desde https://gadm.org/download_country.html, eligiendo United States y descomprimiendo el .zip, eso nos dará los archivos necesarios, luego cargamos el Shapefile de USA y filtramos el AOI en Manhattan, para finalmente de transformarlo a GEOJSON y poder utilizarlo en GEE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "664WxEU8QKRE"
      },
      "outputs": [],
      "source": [
        "# Nivel 2 para elegir los condados, luego lo pasamos a EPSG:4326, compatible con GEE\n",
        "os_current_path = os.path.join(\".\", \"shapefiles\", \"gadm41_USA_2.shp\")\n",
        "gdf_USA = gpd.read_file(os_current_path).to_crs(epsg=4326)\n",
        "\n",
        "\n",
        "# Ruta al archivo de salida\n",
        "output_dir = os.path.join(\".\", \"shapefiles\")\n",
        "output_path = os.path.join(output_dir, \"manhattan.geojson\")\n",
        "\n",
        "# Creamos el directorio si no existe\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "ny_TO_GDF = gdf_USA[gdf_USA['NAME_2'] == 'New York'] # Seleccionamos New York\n",
        "ny_TO_GDF.to_file(output_path, driver=\"GeoJSON\") # Hacemos un .geojson y lo guardamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dF4zmYQSSLx"
      },
      "outputs": [],
      "source": [
        "# Cargamos el geojson creado, para que GEE lo pueda utilizar\n",
        "os_current_path = os.path.join(\".\", \"shapefiles\", \"manhattan.geojson\")\n",
        "gdf = gpd.read_file(os_current_path)\n",
        "manhattan_geojson = json.loads(gdf.to_json())\n",
        "manhattan_ee = ee.FeatureCollection(manhattan_geojson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvfjXaAHUpXI"
      },
      "source": [
        "Visualizamos manhattan con GEE, podemos ajustar los parámetros como la opacidad del AOI en el mapa interactivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpxhsKuLRQYq"
      },
      "outputs": [],
      "source": [
        "manhattanCollection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") # Mayo 2024 - Abril 2025\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con menos de 20% de nubes\n",
        "    .median() # Usamos la mediana de las imágenes de momento, sólo queremos apreciar el mapa\n",
        "    .clip(manhattan_ee)) # Recortamos en Manhattan, la AOI\n",
        "\n",
        "Map = geemap.Map(center=[40.783, -73.971], zoom=12)\n",
        "\n",
        "Map.addLayer(manhattanCollection, {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0,\n",
        "    'max': 3000\n",
        "}, 'RGB')\n",
        "\n",
        "Map.addLayer(manhattan_ee.style(**{\n",
        "    'width': 1,\n",
        "    'color': 'red', # El borde será CYAN\n",
        "    #'fillColor': '00000000',  # Color transparente de relleno\n",
        "}), {}, 'AOI Manhattan')\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6oi1iEb_D6S"
      },
      "source": [
        "Se ha utilizado el dataset de Manhattan, obtenido en https://www.nyc.gov/site/finance/property/property-rolling-sales-data.page, la fecha de los registros del dataset está entre Mayo 2024 - Abril 2025."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM8Dd5KBLAhL"
      },
      "source": [
        "Aquí, en las celdas iniciales que siguen, se muestra como se han geocodificado algunos datos con ayuda de una API, pero no hace falta usarlo, porque los datos procesados ya han sido guardados en un .csv (rollingsales_manhattan_geocoded.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgztuOHI0EU"
      },
      "outputs": [],
      "source": [
        "'''La API que usamos para geocodificar en su capa gratuita es para testing, este proyecto\n",
        "Universitario no califica como un proyecto de producción, no hay usuario final que lo utilizará.\n",
        "Además, sólo hicimos uso de la API en pocas ocasiones'''\n",
        "\n",
        "OPENCAGE_API_KEY = 'KEY'\n",
        "OPENCAGE_BASE_URL = 'https://api.opencagedata.com/geocode/v1/json'\n",
        "\n",
        "# Ruta al archivo de salida\n",
        "output_dir = os.path.join(\".\", \"datasets\")\n",
        "\n",
        "# Creamos el directorio si no existe\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "input_excel_file = os.path.join(output_dir, \"rollingsales_manhattan.xlsx\") # XLSX con rolling sales\n",
        "\n",
        "# Output y failed logs, para poder tener una reanudación cada 2500 request en la geocodificación\n",
        "output_csv_file = os.path.join(output_dir, \"rollingsales_manhattan_geocoded.csv\")\n",
        "failed_addresses_file = output_csv_file = os.path.join(output_dir, \"rollingsales_manhattan_geocoding_failures.csv\")\n",
        "\n",
        "MAX_DAILY_REQUESTS = 2500\n",
        "REQUEST_DELAY = 1.0 # seconds\n",
        "\n",
        "REQUIRED_COLUMNS = ['ADDRESS', 'BOROUGH', 'ZIP CODE', 'SALE PRICE'] # Cols necesarias del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgsut25JEsu"
      },
      "source": [
        "Creamos la función que procesará cada solicitud a la API para poder geocodificar las direcciones y los ZIP codes en coordenadas geográficas que usaremos en los GDF posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuDChyt3I0bJ"
      },
      "outputs": [],
      "source": [
        "# Esta función hace un llamado a la API, y retorna un array con la latitud, longitud y estado\n",
        "def geocode_address(address_str, api_key, borough=None, zip_code=None):\n",
        "    query = f\"{address_str}, {borough}, New York, NY {zip_code}\" if borough and zip_code else address_str\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'key': api_key,\n",
        "        'language': 'en',\n",
        "        'no_annotations': 1,\n",
        "        'limit': 1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(OPENCAGE_BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data and data['results']:\n",
        "            lat = data['results'][0]['geometry']['lat']\n",
        "            lng = data['results'][0]['geometry']['lng']\n",
        "\n",
        "            components = data['results'][0].get('components', {})\n",
        "            is_nyc = False # Empieza como false, y si lo encontramos, lo cambiamos a True:\n",
        "            if 'state_code' in components and components['state_code'] == 'NY':\n",
        "                if 'city' in components and components['city'] in ['New York', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', 'Manhattan']:\n",
        "                    is_nyc = True\n",
        "                elif 'county' in components and ('New York County' in components['county'] or \\\n",
        "                                                 'Kings County' in components['county'] or \\\n",
        "                                                 'Queens County' in components['county'] or \\\n",
        "                                                 'Bronx County' in components['county'] or \\\n",
        "                                                 'Richmond County' in components['county']):\n",
        "                    is_nyc = True\n",
        "\n",
        "            if is_nyc:\n",
        "                return lat, lng, \"Success\"\n",
        "            else:\n",
        "                return None, None, \"Not_NYC_Result\"\n",
        "        else:\n",
        "            return None, None, \"No_Results\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if response.status_code == 429:\n",
        "            return None, None, \"Rate_Limit_Exceeded\"\n",
        "        elif response.status_code == 401:\n",
        "            print(f\"API Key Error!!\")\n",
        "            return None, None, \"API_Key_Error\"\n",
        "        elif response.status_code == 402: # Entonces, llegamos al límite de la quota diaria gratis\n",
        "            print(f\"ERROR, Quota exceded!\")\n",
        "            return None, None, \"Payment_Required_Error\"\n",
        "        else:\n",
        "            print(f\"Request error: {e}\")\n",
        "            return None, None, f\"Request_Error: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None, f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yyyEAuxJYMY"
      },
      "source": [
        "Comenzamos a procesar las direcciones y ZIP codes a coordenadas geográficas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGnrrVuQ34uk"
      },
      "outputs": [],
      "source": [
        "print(f\"*** Procesando {input_excel_file} ***\")\n",
        "\n",
        "df = None\n",
        "# Agregamos las columnas requeridas que fallaron a entradas del df fallido, para re intentar si se quisiera:\n",
        "failed_df = pd.DataFrame(columns=REQUIRED_COLUMNS + ['Reason'])\n",
        "\n",
        "try:\n",
        "    if pd.io.common.file_exists(output_csv_file):\n",
        "        print(f\"Fichero '{output_csv_file}' con progreso encontrado - Resumiendo...\")\n",
        "        df = pd.read_csv(output_csv_file)\n",
        "        # Si no hay latitud, longitud o estado de la geocodificación, sabemos que la entrada no ha sido procesada:\n",
        "        for col in ['LATITUDE', 'LONGITUDE', 'GEOCODING_STATUS']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "            df[col] = df[col].astype(object)\n",
        "\n",
        "    else:\n",
        "        print(f\"No se encontró el fichero con progreso, cargando el fichero inicial XLSX: '{input_excel_file}'.\")\n",
        "        found_header = False\n",
        "        # Probamos con headers desde el 0 al 10, porque algunos datasets XLSX tienen las primeras entradas con información,\n",
        "        # hay que evitar las primeras entradas que no son los headers, para no obtener errores:\n",
        "        for header_row_index in range(10):\n",
        "            try:\n",
        "                print(f\"Intentando cargar con el header {header_row_index}\")\n",
        "                df_temp = pd.read_excel(input_excel_file, header=header_row_index)\n",
        "\n",
        "                if all(col in df_temp.columns for col in REQUIRED_COLUMNS):\n",
        "                    df = df_temp\n",
        "                    found_header = True\n",
        "                    print(f\"Header encontrado en el índice {header_row_index}!\")\n",
        "                    print(\"Columnass encontradas en el DF:\", df.columns.tolist())\n",
        "                    break # Si el header es encontrado, dejamos de loopear\n",
        "                else:\n",
        "                    print(f\"No hay header en el índice {header_row_index}, probando el siguiente...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "        if not found_header: # Si no hay header, hay un error en el fichero\n",
        "            print(f\"No se ha encontrado un header, error en el fichero XLSX\")\n",
        "            exit()\n",
        "\n",
        "        # Limpiamos las columnas del DF y las parseamos\n",
        "        df['ADDRESS'] = df['ADDRESS'].fillna('').astype(str)\n",
        "        df['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].fillna('').astype(str)\n",
        "        df['BOROUGH'] = df['BOROUGH'].fillna('').astype(str)\n",
        "        df['ZIP CODE'] = df['ZIP CODE'].fillna(0).astype(int).astype(str).replace('0', '')\n",
        "\n",
        "        # Agregamos la latitud, longitud, y estado de la geocodificación, como nuevas columnas en el DF:\n",
        "        df['LATITUDE'] = None\n",
        "        df['LONGITUDE'] = None\n",
        "        df['GEOCODING_STATUS'] = None\n",
        "\n",
        "    already_geocoded_count = df['LATITUDE'].notna().sum()\n",
        "    requests_made_now = 0\n",
        "    print(f\"Los registros geocodificados hasta el momento son: {already_geocoded_count}\")\n",
        "\n",
        "    # La siguiente línea veririfica si la latitud está vacía y el \"status\" es diferente de \"Not_NYC_Result\"\n",
        "    # Si el registro tiene GEOCODIG_STATUS = 'Not_NYC_Result', entonces ha fallado la geocodificación.\n",
        "    rows_to_geocode = df[(df['LATITUDE'].isna()) & (df['GEOCODING_STATUS'] != 'Not_NYC_Result')]\n",
        "\n",
        "    print(f\"Han fallado: {len(rows_to_geocode)} registros\")\n",
        "\n",
        "    for index, row in tqdm(rows_to_geocode.iterrows(), total=len(rows_to_geocode), desc=\"Geocoding\"): #tqdm para barra de progrso\n",
        "        if requests_made_now >= MAX_DAILY_REQUESTS:\n",
        "            print(f\"Se ha llegado al límite de {MAX_DAILY_REQUESTS} requests diarias!\")\n",
        "            break\n",
        "\n",
        "        address = row['ADDRESS']\n",
        "        borough = row['BOROUGH']\n",
        "        zip_code = row['ZIP CODE'] if row['ZIP CODE'] != '0' else ''\n",
        "\n",
        "        if not address:\n",
        "            df.loc[index, 'GEOCODING_STATUS'] = \"Empty_Address\"\n",
        "            continue\n",
        "\n",
        "        lat, lon, status = geocode_address(address, OPENCAGE_API_KEY, borough, zip_code)\n",
        "        requests_made_now += 1\n",
        "\n",
        "        df.loc[index, 'LATITUDE'] = lat\n",
        "        df.loc[index, 'LONGITUDE'] = lon\n",
        "        df.loc[index, 'GEOCODING_STATUS'] = status\n",
        "\n",
        "        if status in [\"Rate_Limit_Exceeded\", \"API_Key_Error\", \"Payment_Required_Error\"]:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY) # La documentación de la API indica un delay de 1 segundo entre cada request\n",
        "\n",
        "        # Guardamos el progreso en chunks de cada 100 solicitudes a la API:\n",
        "        if requests_made_now % 100 == 0:\n",
        "            print(f\"Guardando progreso en la request número {requests_made_now}\")\n",
        "            df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Geocodificación finalizada!!\")\n",
        "\n",
        "    failed_rows = df[df['LATITUDE'].isna()] # Si no tiene latitud, es una row fallida\n",
        "    if not failed_rows.empty:\n",
        "        failed_df_to_save = failed_rows[['BOROUGH', 'NEIGHBORHOOD', 'ADDRESS', 'ZIP CODE', 'GEOCODING_STATUS']].copy()\n",
        "        # Guardamos en failed_addresses_file sólo las rows de la variable de arriba, para poder procesarlas luego.\n",
        "        failed_df_to_save.to_csv(failed_addresses_file, index=False)\n",
        "        print(f\"Los registros fallidos se guardaron en: {failed_addresses_file}\")\n",
        "    else:\n",
        "        print(\"Atención! Finalización inesperada, posiblemente ha ocurrido un error.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Archivo '{input_excel_file}' no hallado.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyojXTMXc6L"
      },
      "source": [
        "Cargamos el fichero con la latitud y longitud agregadas en la geocodificación, para luego, filtrar los registros que tengan NaN en latitud y longitud, ya que eso es producto de errores en la geocodificación de dichos valores. También cribamos y sólo tomamos como válidos valores con precio de venta mayor a 0, ya que hay varios valores en el dataset con propiedades que se han vendido a costo 0, lo que no tiene representación estadística en nuestro contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxJdB0e94hCo"
      },
      "outputs": [],
      "source": [
        "properties_geocoded_file = os.path.join(\".\", \"datasets\", \"rollingsales_manhattan_geocoded.csv\")\n",
        "\n",
        "try:\n",
        "    df_properties = pd.read_csv(properties_geocoded_file)\n",
        "    print(f\"***Cargando fichero: {properties_geocoded_file}*** \\n\")\n",
        "    print(f\"Cantidad de registros encontrados en {str(properties_geocoded_file)}: {len(df_properties)} \\n\")\n",
        "    print(\"df_properties.info(): \\n\")\n",
        "    print(df_properties.info())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminamos los registros que sean NaN en latitud y longitud.\n",
        "df_properties.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
        "print(f\"Registros después de eliminar NaN en lat/lon: {len(df_properties)}\")\n",
        "\n",
        "# Transformamos todo SALE PRICE a numeric, para luego, con coerce, reemplazar los valores no numericos a NaN\n",
        "df_properties['SALE PRICE'] = pd.to_numeric(df_properties['SALE PRICE'], errors='coerce')\n",
        "# Luego,eliminamos todas esas filas que contienen NaN\n",
        "df_properties.dropna(subset=['SALE PRICE'], inplace=True)\n",
        "print(f\"Registros después de filtrar por NaN: {len(df_properties)}\")\n",
        "\n",
        "# Eliminamos los registros que tengan precio de venta menor o igual a 0, sin representación estadística.\n",
        "df_properties = df_properties[df_properties['SALE PRICE'] > 0]\n",
        "print(f\"Registros después de filtrar por precio de venta > 0: {len(df_properties)}\")\n",
        "\n",
        "# Filtramos duplicados\n",
        "initial_rows_before_deduplication = len(df_properties)\n",
        "df_properties.drop_duplicates(inplace=True)\n",
        "print(f\"Registros después de filtrar por posibles duplicados: {len(df_properties)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1kLfLf6cMl5"
      },
      "source": [
        "Luego, para una posterior manipulación y tener una mayor compatibilidad con GEE y librerías para graficar, \"transformamos\" el dataframe a un geodataframe, y le agregamos una nueva columna de tipo geometry, que contendrá puntos generados a través de las propiedades de latitud y longitud, los cuales están definidos en la variable geometry, haciendo uso del métetodo zip y list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAhuFKN4lFW"
      },
      "outputs": [],
      "source": [
        "# A partir de las coordenadas, creamos objetos de tipo Point, de la librería shapely.geometry, para luego manipular mejor:\n",
        "geometry = [Point(xy) for xy in zip(df_properties['LONGITUDE'], df_properties['LATITUDE'])]\n",
        "\n",
        "# Creamos el nuevo geodataframe para, a partir del dataframe anterior, llenarlo con los datos:\n",
        "gdf_properties = gpd.GeoDataFrame(df_properties, geometry=geometry, crs=\"EPSG:4326\") #CRS es EPSG:4326 para latitud y longitud.\n",
        "\n",
        "print(\"Nuevo GeoDataFrame:\")\n",
        "gdf_properties.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgWEfpVligZX"
      },
      "source": [
        "Cargamos el dataset https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic que contiene el histórico de tiroteos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1--5td5E3i"
      },
      "outputs": [],
      "source": [
        "shooting_incident_historic = os.path.join(output_dir, \"NYPD_Shooting_Incident_Data__Historic_.csv\")\n",
        "\n",
        "df_shooting_incident_historic = None\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado \\n\")\n",
        "    print(f'Información del fichero: \\n')\n",
        "    df_shooting_incident_historic.info()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjdic-wQqglJ"
      },
      "source": [
        " Al igual que antes, este dataset también debe ser filtrado por fecha de interés y ser convertido a GDF, la columna con el Point, que contiene la geometría de la latitud y longitud, está en una columna llamada \"Lon_Lat\", y contiene valores de tipo Point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RFXeYKI5Spj"
      },
      "outputs": [],
      "source": [
        "LATITUDE_COL = 'Latitude'\n",
        "LONGITUDE_COL = 'Longitude'\n",
        "INCIDENT_DATE_COL = 'OCCUR_DATE'\n",
        "BORO_COL = 'BORO'\n",
        "year_of_preference = 2024 # Nos interesan datos de tiroteos en los últimos 2 años\n",
        "\n",
        "# Cargamos el archivo y definimos una variable para su dataframe\n",
        "shooting_incident_historic = os.path.join(\".\", \"datasets\", \"NYPD_Shooting_Incident_Data__Historic_.csv\")\n",
        "\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado. Su longitud es: {len(df_shooting_incident_historic)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Eliminamos las filas con NaN en las coordenadas\n",
        "df_shooting_incident_historic.dropna(subset=[LATITUDE_COL, LONGITUDE_COL], inplace=True)\n",
        "\n",
        "# Eliminamos las filas duplicadas\n",
        "df_shooting_incident_historic.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_shooting_incident_historic)}\")\n",
        "\n",
        "# Si la columna con fecha del incidente existe, filtramos por BORO y AÑO:\n",
        "if INCIDENT_DATE_COL in df_shooting_incident_historic.columns:\n",
        "    # Convertimos la columna de OCCUR_DATE a datatime de pandas, para trabajarla con el formato de USA:\n",
        "    df_shooting_incident_historic[INCIDENT_DATE_COL] = pd.to_datetime(\n",
        "        df_shooting_incident_historic[INCIDENT_DATE_COL],\n",
        "        format='%m/%d/%Y',\n",
        "        errors='coerce' # Los valores sin fecha válida serán NaT\n",
        "    )\n",
        "    # Dropeamos valores sin fecha válida (NaT)\n",
        "    df_shooting_incident_historic.dropna(subset=[INCIDENT_DATE_COL], inplace=True)\n",
        "    df_shooting_incident_historic['INCIDENT_YEAR'] = df_shooting_incident_historic[INCIDENT_DATE_COL].dt.year\n",
        "\n",
        "    # Filtramos para sólo obtener datos de Manhattan (en la col BORO)\n",
        "    if BORO_COL in df_shooting_incident_historic.columns:\n",
        "        # Usamos .copy() para definir la variable por valor, y no por referencia en memoria:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_historic[df_shooting_incident_historic[BORO_COL] == 'MANHATTAN'].copy()\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes en Manhattan\")\n",
        "\n",
        "        # Filtramos según el año deseado:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_manhattan[df_shooting_incident_manhattan['INCIDENT_YEAR'] >= year_of_preference]\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes posteriores al año {year_of_preference}. \\n\")\n",
        "\n",
        "        # Antes de convertir el DF a GDF, necesitamos col geometry que contiene puntos, los cuales están en la\n",
        "        # columna Lon_Lat, por lo que la parseamos:\n",
        "        df_shooting_incident_manhattan['Lon_Lat'] = df_shooting_incident_manhattan['Lon_Lat'].apply(wkt.loads)\n",
        "\n",
        "        # Creamos el GeoDataFrame de los incidentes y lo asignamos en una nueva variable:\n",
        "        geometry_incidents = [Point(xy) for xy in zip(df_shooting_incident_manhattan[LONGITUDE_COL], df_shooting_incident_manhattan[LATITUDE_COL])]\n",
        "\n",
        "        # Renombramos la columna Lon_Lat a geometry, ya que sabemos que existe Lon_Lat, que es un tipo de dato Point\n",
        "        df_shooting_incident_manhattan.rename(columns={'Lon_Lat': 'geometry'}, inplace=True)\n",
        "\n",
        "        # Creamos el GDF utilizando la col geometry:\n",
        "        gdf_incidents = gpd.GeoDataFrame(df_shooting_incident_manhattan, geometry='geometry', crs=\"EPSG:4326\")  # CRS 4326 para lat/lon\n",
        "\n",
        "        print(\"Información del GDF:\")\n",
        "        gdf_incidents.info()\n",
        "    else:\n",
        "      raise KeyError(f\"Error: La columna '{BORO_COL}' no se encontró!! \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVzp_4QLGPD0"
      },
      "source": [
        "Cargamos y limpiamos el dataset de universidades y college:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD26vj-n_g5Z"
      },
      "outputs": [],
      "source": [
        "# Definimos la ruta del dataset\n",
        "college_university_file = os.path.join(\".\", \"datasets\", \"COLLEGE_UNIVERSITY_20250609.csv\")\n",
        "df_college_university = None\n",
        "\n",
        "# Esta propiedad en el dataset es un Punto siempre, según la página que lo provee:\n",
        "GEOMETRY_COL = 'the_geom'\n",
        "\n",
        "try:\n",
        "    df_college_university = pd.read_csv(college_university_file)\n",
        "    print(f\"El archivo {college_university_file} ha sido cargado. Su longitud es: {len(df_college_university)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "#Limpieza de nulos:\n",
        "df_college_university.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_college_university)}\")\n",
        "\n",
        "#Limpieza de duplicados\n",
        "df_college_university.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_college_university)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_college_university.columns:\n",
        "    #Parseamos los datos de la columna the_geom y los agregamos a la llamada 'geometry' para poder trabajarlas:\n",
        "    df_college_university['geometry'] = df_college_university[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_college_university.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_college_university)}\")\n",
        "\n",
        "    gdf_universities = gpd.GeoDataFrame(df_college_university, geometry='geometry', crs=\"EPSG:4326\") #Aplicamos la proyección correcta\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Universidades:\")\n",
        "    gdf_universities.info()\n",
        "\n",
        "    # Mostramos el mapa\n",
        "    Map = geemap.Map(center=[40.7, -74.0], zoom=12)\n",
        "    ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "    try:\n",
        "\n",
        "            # Cargamos el dataset Sentinel-2 L2A (surface reflectance) para mostrar las bandas RGB, y filtramos por AOI y fecha: Mayo 2024 - Abril 2025\n",
        "            s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "                .filterBounds(manhattan_ee) \\\n",
        "                .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con porcentaje de nubes menor a 20\n",
        "\n",
        "            # Verificamos que la colección no esté vacía para el intervalo de fechas solicitada:\n",
        "            if s2_collection.size().getInfo() > 0:\n",
        "                s2_image = s2_collection.median().clip(manhattan_ee) # Calculamos la mediana de imágenes y recortamos al AOI\n",
        "\n",
        "                s2_vis_params = {\n",
        "                    'bands': ['B4', 'B3', 'B2'], # B4=Red, B3=Green, B2=Blue\n",
        "                    'min': 0,\n",
        "                    'max': 3000,\n",
        "                    'gamma': 1.4\n",
        "                }\n",
        "\n",
        "                Map.addLayer(s2_image, s2_vis_params, 'RGB') # Agregamos la layer al mapa\n",
        "            else:\n",
        "                print(\"El dataset no tiene imágnes para la fecha de interés.\")\n",
        "\n",
        "            # Agregamos una layer con los puntos, que representan las ubicaciones de las ubicaciones de los centros de estudio:\n",
        "            Map.addLayer(ee_universities, {'color': 'red', 'opacity': 0.9, 'point_size': 2}, 'University Locations')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map) # A veces hay que usar display para mostrar el mapa\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: '{GEOMETRY_COL}' no existe en el CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPyVE1R5sifw"
      },
      "source": [
        "Cargamos y limpiamos el dataset de hospitales en NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaAskNc0slCD"
      },
      "outputs": [],
      "source": [
        "hospital_filename = os.path.join(\".\", \"datasets\", \"hospital_20250704.csv\")\n",
        "df_hospitals = None\n",
        "\n",
        "# Según el dataset, los puntos de ubicación están en el atributo llamado \"Location 1\", luego lo agregaremos como geometry:\n",
        "GEOMETRY_COL = 'Location 1'\n",
        "\n",
        "try:\n",
        "    df_hospitals = pd.read_csv(hospital_filename)\n",
        "    print(f\"El archivo {hospital_filename} ha sido cargado. Su longitud es: {len(df_hospitals)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Limpieza de nulos en la columna de geometría:\n",
        "df_hospitals.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_hospitals)}\")\n",
        "\n",
        "# Limpieza de duplicados\n",
        "df_hospitals.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_hospitals)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_hospitals.columns:\n",
        "    # Parseamos los datos de la columna de geometría y los agregamos a la nueva columna 'geometry'\n",
        "    df_hospitals['geometry'] = df_hospitals[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_hospitals.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_hospitals)}\")\n",
        "\n",
        "    # Creamos el GeoDataFrame con la proyección correcta (EPSG:4326)\n",
        "    gdf_hospitals = gpd.GeoDataFrame(df_hospitals, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Hospitales:\")\n",
        "    gdf_hospitals.info()\n",
        "\n",
        "    Map = geemap.Map(center=[40.76, -73.98], zoom=12) # Centrado en Manhattan\n",
        "\n",
        "    # Convertimos el GeoDataFrame a un Earth Engine FeatureCollection\n",
        "    ee_hospitals = geemap.geopandas_to_ee(gdf_hospitals)\n",
        "\n",
        "    try:\n",
        "        # Cargamos el dataset Sentinel-2 L2A y filtramos por AOI y fecha: Mayo 2024 - Abril 2025\n",
        "        s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "            .filterBounds(manhattan_ee) \\\n",
        "            .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con < 20% de nubes\n",
        "\n",
        "        # Verificamos que la colección no esté vacía\n",
        "        if s2_collection.size().getInfo() > 0:\n",
        "            s2_image = s2_collection.median().clip(manhattan_ee) # Calculamos la mediana y recortamos al AOI\n",
        "\n",
        "            s2_vis_params = {\n",
        "                'bands': ['B4', 'B3', 'B2'], # R, G, B\n",
        "                'min': 0,\n",
        "                'max': 3000,\n",
        "                'gamma': 1.4\n",
        "            }\n",
        "\n",
        "            Map.addLayer(s2_image, s2_vis_params, 'RGB') # Agregamos la capa de imagen satelital\n",
        "        else:\n",
        "            print(\"El dataset no tiene imágenes para la fecha de interés.\")\n",
        "\n",
        "        # Agregamos la capa con los puntos de los hospitales\n",
        "        Map.addLayer(ee_hospitals, {'color': 'blue', 'opacity': 0.9, 'point_size': 2}, 'Hospital Locations')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map)\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: La columna '{GEOMETRY_COL}' no existe en el archivo CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7G9uoqLwiGa"
      },
      "source": [
        "Cargamos y limpiamos el dataset de las entradas al metro de NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaf4IHmpwiXj"
      },
      "outputs": [],
      "source": [
        "subway_filename = os.path.join(\".\", \"datasets\", \"MTA_Subway_Entrances_and_Exits__2024.csv\")\n",
        "df_subway = None\n",
        "\n",
        "# Según el dataset, los puntos de ubicación están en el atributo llamado \"entrance_georeference\", luego lo agregaremos como geometry:\n",
        "GEOMETRY_COL = 'entrance_georeference'\n",
        "\n",
        "try:\n",
        "    df_subway = pd.read_csv(subway_filename)\n",
        "    print(f\"El archivo {subway_filename} ha sido cargado. Su longitud es: {len(df_subway)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Limpieza de nulos en la columna de geometría:\n",
        "df_subway.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_subway)}\")\n",
        "\n",
        "# Limpieza de duplicados\n",
        "df_subway.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_subway)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_subway.columns:\n",
        "    # Parseamos los datos de la columna de geometría y los agregamos a la nueva columna 'geometry'\n",
        "    df_subway['geometry'] = df_subway[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_subway.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_subway)}\")\n",
        "\n",
        "    # Creamos el GeoDataFrame con la proyección correcta (WGS84)\n",
        "    gdf_subway = gpd.GeoDataFrame(df_subway, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Entradas de Subterráneo:\")\n",
        "    gdf_subway.info()\n",
        "\n",
        "    Map = geemap.Map(center=[40.76, -73.98], zoom=12) # Centrado en Manhattan\n",
        "\n",
        "    # Convertimos el GeoDataFrame a un Earth Engine FeatureCollection\n",
        "    ee_subway = geemap.geopandas_to_ee(gdf_subway)\n",
        "\n",
        "    try:\n",
        "        # Cargamos el dataset Sentinel-2 L2A y filtramos por AOI y fecha\n",
        "        s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "            .filterBounds(manhattan_ee) \\\n",
        "            .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
        "\n",
        "        # Verificamos que la colección no esté vacía\n",
        "        if s2_collection.size().getInfo() > 0:\n",
        "            s2_image = s2_collection.median().clip(manhattan_ee)\n",
        "\n",
        "            s2_vis_params = {\n",
        "                'bands': ['B4', 'B3', 'B2'], # R, G, B\n",
        "                'min': 0,\n",
        "                'max': 3000,\n",
        "                'gamma': 1.4\n",
        "            }\n",
        "            Map.addLayer(s2_image, s2_vis_params, 'RGB')\n",
        "        else:\n",
        "            print(\"El dataset no tiene imágenes para la fecha de interés.\")\n",
        "\n",
        "        # Agregamos la capa con los puntos de las entradas al metro\n",
        "        Map.addLayer(ee_subway, {'color': 'orange', 'pointSize': 2}, 'Subway entrances')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map)\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: La columna '{GEOMETRY_COL}' no existe en el archivo CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtOPK1XQDVp"
      },
      "source": [
        "Al igual que en la tarea 2, usamos el NDBI.\n",
        "(Sitio web de interés: https://www.gisandbeers.com/calculo-indice-ndbi-analisis-urbanisticos/)\n",
        "\n",
        "Revisando el estudio de\n",
        "https://revistas.uptc.edu.co/index.php/ingenieria_sogamoso/article/view/15018/12232 Me inclino a considerar al NDBI como un buen índice espectral para utilizar. Aunque se concluye del estudio en Colombia que, si bien éste índice es el que mostró mejor resultados en las zonas mencionadas ahí, no es una regla general.\n",
        "\n",
        "El NDBI (Normalized Difference Built-up Index) es un índice espectral diseñado para resaltar las zonas construidas, de ahí el nombre de las siglas. Se basa en el cálculo mediante la fórmula presente en el sitio web de interés, como también en el estudio, ambos enlaces más arriba, la fórmula se basa en el contraste entre la reflectancia del infrarrojo de onda corta (SWIR) y del infrarrojo cercano (NIR), que en el caso de nuestro dataset utilizado (Sentinel-2), equivalen a las bandas B11 y B8, respectivamente. Si bien para el caso de una representación visual corriente representamos las capas (layers) en un mapa con las bandas RGB, que equivalen a las bandas B4, B3 y B2 respectivamente, en este caso utilizamos estas otras bandas para poder estudiar de mejor manera las zonas construidas o edificaciones. Diferentes datasets provienen de diferentes satélites con diferentes sensores, así que no todos los satélites trabajan con las mismas bandas, por lo que en imágenes muy antiguas puede ser que no estén disponbles las bandas de los datasets actuales, como las del Sentinel-2.\n",
        "\n",
        "Fórmulas:\n",
        "\n",
        "NDBI = (SWIR - NIR) / (SWIR + NIR)\n",
        "\n",
        "Misma fórmula, pero reemplazando las radiaciones electromagnéticas SWIR y NIR por sus equivalentes en bandas para el Sentinel-2:\n",
        "\n",
        "NDBI = (Banda 11 - Banda 8) / (Banda 11 + Banda 8)\n",
        "\n",
        "Las equivalencias entre SWIR y NIR con las bandas 11 y 8 fueron obtenidas de https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED#bands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsTjOEHlQDmt"
      },
      "outputs": [],
      "source": [
        "Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "#Pasamos un GDF a FeatureCollection de GEE, para poder utilizarlo en GEE posteriormente:\n",
        "#ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "\n",
        "# Esto es boilerplate de la tarea 2, código genérico para calcular el NDBI:\n",
        "def getNDBI(image):\n",
        "    # Calculamos el NDBI con las Bandas B11 (SWIR 1) y B8 (NIR). Formula: (SWIR - NIR) / (SWIR + NIR)\n",
        "    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
        "    return ndbi\n",
        "\n",
        "s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "    .filterBounds(manhattan_ee) \\\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con nubosidad menor al 20%\n",
        "\n",
        "if s2_collection.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s2_image_median = s2_collection.median().clip(manhattan_ee)\n",
        "\n",
        "    # Calculamos el NDBI para la mediana de las imágnes del dataset, previamente clippeado por el AOI.\n",
        "    ndbi_image = getNDBI(s2_image_median)\n",
        "\n",
        "    # Paleta: de áreas no urbanizadas (ej. verde/azul) a áreas urbanizadas (ej. gris/blanco/morado)\n",
        "    ndbi_vis_params = {\n",
        "        'min': -0.5,\n",
        "        'max': 0.5,\n",
        "        'palette': [\n",
        "            'blue',    # Agua (NDBI muy bajo)\n",
        "            'green',   # Vegetación sana\n",
        "            'yellow',  # Suelo desnudo / vegetación dispersa\n",
        "            'red',     # Transición / áreas urbanizadas menos densas\n",
        "            'white'    # Áreas altamente urbanizadas (NDBI alto)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    s2_rgb_vis_params = {\n",
        "        'bands': ['B4', 'B3', 'B2'],\n",
        "        'min': 0,\n",
        "        'max': 3000,\n",
        "        'gamma': 1.4\n",
        "    }\n",
        "\n",
        "    # Agregamos la layer RGB y la del NDBI para mostrar el mapa\n",
        "    Map.addLayer(s2_image_median, s2_rgb_vis_params, 'Sentinel-2 RGB')\n",
        "    Map.addLayer(ndbi_image, ndbi_vis_params, 'NDBI ')\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado im[agenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDogZMoThlSv"
      },
      "source": [
        "Aquí, hacemos reproyecciones de los datasets para pasarlos del sistema angular a unidades medidas en pies(ft), para así hacer las mediciones posteriores de distancias más facilmente. Para ello, hemos utilizado el sitio web https://epsg.io/, que nos ayuda a encontrar la proyección adecuada para cada lugar del mundo, en este caso New York City. Usamos, en vez de la la proyeccion EPSG:32118 que mide en metros, la proyecciön 2263 medida en pies, ya que está diseñada específicamente para NYC. Probando con el CRS EPSG:32118 los resultados fueron mucho menos precisos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJhjzS5oo6xS"
      },
      "outputs": [],
      "source": [
        "crs_meters_nyc = \"EPSG:2263\"\n",
        "\n",
        "print(f\"Haciendo reproyección de los datasets a {crs_meters_nyc}...\")\n",
        "\n",
        "gdf_properties_proj_ft = gdf_properties.to_crs(crs_meters_nyc)\n",
        "gdf_incidents_proj_ft = gdf_incidents.to_crs(crs_meters_nyc)\n",
        "gdf_subway_proj_ft = gdf_subway.to_crs(crs_meters_nyc)\n",
        "gdf_hospitals_proj_ft = gdf_hospitals.to_crs(crs_meters_nyc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZw3DO4JffFV"
      },
      "source": [
        "Hacemos un agrupamiento (clustering) de los lugares donde han ocurrido tiroteos registrados en el dataset cargado anteriormente (gdf_incidents*). Para ello, usamos el algoritmo DBSCAN.\n",
        "\n",
        "Se debe limpiar el GDF de incidentes con .drop_duplicates() por duplicados, y también para nulos, pero el dataset ya ha sido limpiado al cargarse, por lo que no es necesario, se ha puesto un print para verificar que no hay duplicados.\n",
        "\n",
        "Recordar que la región de agrupamiento epsilon=eps está en ft (foots, pies), no en metros.\n",
        "\n",
        "Es importante considerar que los verdaderos repetidos no necesariamente consisten en las coordenadas repetidas, sino en registros que pueden tener fechas similares, ya que no es para nada imposible que se registren tiroteos en zonas donde ya han ocurrido previamente tiroteos, lo correcto sería asegurarse que no hay repetición temporal sobre la misma zona, idealmente con un epsilon en cada punto para verificar si hay duplicado del mismo registro, pero en nuestra pequeña base de datos de incidentes no es relevante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QkRs09VdKFk"
      },
      "outputs": [],
      "source": [
        "gdf_incidents_proj_ft_cleaned = gdf_incidents_proj_ft.drop_duplicates() # Eliminamos duplicados\n",
        "gdf_incidents_proj_ft_cleaned.reset_index(drop=True) # Reseteamos el índice luego de dropear duplicados.\n",
        "\n",
        "print(f\"Número de filas completamente duplicadas eliminadas: {len(gdf_incidents_proj_ft) - len(gdf_incidents_proj_ft_cleaned)}\")\n",
        "\n",
        "# Iniciamos el algoritmo de clustering DBSCAN para un epsilon de 700ft y un mínimo de 2 incidentes.\n",
        "model = DBSCAN(eps=700, min_samples=2)\n",
        "labels = model.fit_predict(\n",
        "    pd.DataFrame({\n",
        "        'x': gdf_incidents_proj_ft_cleaned.geometry.x,\n",
        "        'y': gdf_incidents_proj_ft_cleaned.geometry.y\n",
        "    })\n",
        ")\n",
        "\n",
        "gdf_incidents_proj_ft_cleaned['label'] = labels\n",
        "\n",
        "set_labels = set(labels) # El set elimina duplicados\n",
        "#Al set de labels, le restamos 1 si es que existen puntos sin cluster (label -1), de lo contrario, no.\n",
        "number_clusters = len(set_labels) - (1 if -1 in labels else 0)\n",
        "\n",
        "list_labels = list(labels) # La lista conserva el orden y no elimina duplicados.\n",
        "number_noises = list_labels.count(-1) # Contamos cantidad de labels que son -1\n",
        "\n",
        "print(f\"Se han hallado {number_clusters} clusters.\") # Los valores sin -1 como label, son clusters\n",
        "print(f\"Numero de puntos ruido: {number_noises}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Y_5RP6pW4R"
      },
      "source": [
        "Luego de haber aplicado el algoritmo para los puntos y obtenido los labels para cada punto, podemos hacer uso de los puntos que están agrupados en clusters y los que son ruido, pera poder visualizarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-i0LreJdKco"
      },
      "outputs": [],
      "source": [
        "# Reproyectamos el GDF para usar ángulos (latitud, longitud)\n",
        "gdf_incidents_proj_ft_cleaned_epsg4326 = gdf_incidents_proj_ft_cleaned.to_crs(\"EPSG:4326\")\n",
        "gdf_map_epsg4326 = gdf_incidents_proj_ft_cleaned_epsg4326.copy() # Copiamos para acortar el nombre\n",
        "\n",
        "# Creamos el mapa centrado en NYC para mostrar los puntos de tiroteos sobre Manhattan.\n",
        "mapa = folium.Map(location=[40.73, -73.9], zoom_start=11, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Seleccionamos nuevamente los puntos ruido para poder graficarlos en gris en el folium.\n",
        "noise_points = gdf_map_epsg4326[gdf_map_epsg4326['label'] == -1]\n",
        "\n",
        "for _, row in noise_points.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=2,\n",
        "        color='gray',\n",
        "        fill=True,\n",
        "        fill_color='gray',\n",
        "        fill_opacity=0.5,\n",
        "        popup='Puntos de ruido'\n",
        "    ).add_to(mapa)\n",
        "\n",
        "# Extraemos los labels únicos, excluyendo los que son -1 (ruido)\n",
        "unique_labels = gdf_map_epsg4326['label'].unique()\n",
        "unique_labels = unique_labels[unique_labels != -1]\n",
        "\n",
        "# Creamos una paleta de 20 colores para ver los clusters.\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_labels))\n",
        "\n",
        "# Folium necesita que se parseen los colores a hexadecimal, loopeamos con enumerate y parseamos:\n",
        "label_to_color = {label: mcolors.rgb2hex(colors(i)) for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Hacemos lo mismo que con los puntos de ruido pero con los clusters, los seleccionamos y los graficamos en el folium:\n",
        "cluster_points = gdf_map_epsg4326[gdf_map_epsg4326['label'] != -1]\n",
        "\n",
        "for _, row in cluster_points.iterrows():\n",
        "\n",
        "    # El color dependerá de la label del punto, a cuál cluster pertenece:\n",
        "    color = label_to_color[row['label']]\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        popup=f\"Cluster perteneciente al label {row['label']}\"\n",
        "    ).add_to(mapa)\n",
        "\n",
        "display(mapa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiu2xLkWplEe"
      },
      "source": [
        "Aquí, se ha tomado como referencia esta explicación de Moran:\n",
        "https://pro.arcgis.com/es/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm, además de https://pro.arcgis.com/es/pro-app/latest/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm, junto con la clase online grabada donde se menciona el algoritmo K-means y el DBSCAN, como también HDBSCAN y diferentes variantes de algoritmos de clustering o agrupación de la librería [scikit-learn](https://scikit-learn.org). Parte de este código fue tomado tomando como referencia la tarea 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KYzcs_S_Hmy"
      },
      "source": [
        "Entonces, para la verificación estadística, usamos los datos del dataset de propiedades; primero que nada, agrupamos el promedio de los precios de venta de las propiedades bajo el 'ZIP CODE', y lo guardamos en median_prices: muchas casas pueden tener el mismo código postal, pero el código postal cambia según cambia la ubicación geográfica, así que podemos agruparlas, pero calculamos el promedio para tener una representación estadística del precio promedio de esa agrupación de propiedades.\n",
        "\n",
        "Luego, creamos un nuevo DF y, con el método .dissolve (https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.dissolve.html), asignamos una geometría a cada entrada que tenga un \"ZIP CODE\" distinto, y lo almacenamos en la variable gdf_zipcodes, hay que tener en cuenta que este nuevo dataset es de tipo GeoDataFrame, ya que contiene la geometría.\n",
        "\n",
        "Finalmente, unimos los dos DF en uno solo, renombrando la columna de \"SALE PRICE\" por \"MEDIAN_SALE_PRICE\" para evitar confusiones, esto lo utilizaremos en la siguiente celda para el análisis I de Moran."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltVPsVBKniCV"
      },
      "outputs": [],
      "source": [
        "# Limpieza preventiva, probablemente no es necesario porque ya se hizo limpieza antes.\n",
        "df = gdf_properties.copy()\n",
        "df['SALE PRICE'] = pd.to_numeric(df['SALE PRICE'], errors='coerce')\n",
        "df = df.dropna(subset=['SALE PRICE'])\n",
        "df = df[df['SALE PRICE'] > 0]\n",
        "\n",
        "# Agregamos los datos por código postal para obtener el precio promedio y la geometría\n",
        "median_prices = df.groupby('ZIP CODE')['SALE PRICE'].median()\n",
        "gdf_zipcodes = df[['ZIP CODE', 'geometry']].dissolve(by='ZIP CODE')\n",
        "gdf_zipcodes = gdf_zipcodes.merge(median_prices, on='ZIP CODE').rename(columns={'SALE PRICE': 'MEDIAN_SALE_PRICE'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOz4_6-nEwex"
      },
      "source": [
        "Para el análisis I de Moran (global), primero que nada, definimos una matriz de pesos con el algoritmo del vecino más cercano (KNN): https://pysal.org/libpysal/generated/libpysal.weights.KNN.html#libpysal.weights.KNN\n",
        "\n",
        "Lo que queremos es plantear la hipótesis nula, esto es, suponer que hay una distribución aleatoria de puntos y luego comprobar estadísticamente que no es así, para ello usamos el método de Monte Carlo, simulando una distribución de puntos aleatoria miles de veces, para finalmente comparar los resultados obtenidos con nuestra distribución de puntos inicial, la idea es tomar el algoritmo KNN para tomar un promedio de los precios de viviendas agrupadas según el mismo ZIP CODE, para poder demostrar que existe un agrupamiento, donde viviendas con precio similar están cerca de viviendas con precio similar, y no simplemente se distribuyen espacialmente las viviendas con precios de venta aleatorios, sino que el sector importa. Esto es importante porque luego haremos una representación visual de los precios de ventas agrupados en sectores.\n",
        "\n",
        "También se ha consultado: https://pysal.org/libpysal/api.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZnPJ3o6nkvo"
      },
      "outputs": [],
      "source": [
        "# Usamos el algoritmo del vecino más cercano promediando la distancia a los 4 más cercanos.\n",
        "k_neighbors = 4\n",
        "wq = libpysal.weights.KNN.from_dataframe(gdf_zipcodes, k=k_neighbors)\n",
        "\n",
        "# Con esto, calculamos el promedio de los precios de la matriz de pesos, no la suma total de precios para un mismo ZIP CODE:\n",
        "wq.transform = 'R'\n",
        "\n",
        "# Calculamos la I de Moran Global\n",
        "y = gdf_zipcodes['MEDIAN_SALE_PRICE']\n",
        "moran = Moran(y, wq, permutations=9999) # Hacemos 10mil simulaciones con el método de Monte Carlo.\n",
        "\n",
        "print(f\"Análisis con k = {k_neighbors} vecinos más cercanos: \\n\")\n",
        "print(f\"Valor I de Moran: {moran.I}, Valor P: {moran.p_sim}\")\n",
        "\n",
        "if moran.p_sim < 0.05 and moran.I > 0:\n",
        "    print(\"Existe una autocorrelación estadísticamente positiva entre los precios de las propiedades según su ZIP CODE\")\n",
        "else:\n",
        "    print(\"Los puntos siguen una distribución aleatoria.\")\n",
        "\n",
        "plot_moran(moran, figsize=(10,6))\n",
        "plt.title(f'Gráfico de Moran para k={k_neighbors}')\n",
        "plt.xlabel('Precio promedio de una vivienda')\n",
        "plt.ylabel('Precio promedio de un agrupamiento de viviendas')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT30nh_UH0Hc"
      },
      "source": [
        "Entonces, obtenidos resultados estadísticamente representativos, concluimos que nuestra hipótesis nula no es cierta, así que demostramos que hay agrupamiento espacial (clustering) de los puntos con sus atributos de precio, así que el precio no es aleatoriamente distribuido espacialmente en Manhattan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMC_YVnILS4t"
      },
      "source": [
        "Ahora, usamos osmnx para poder usar OpenStreetMap, la idea es descargar las vias peatonales de Manhattan, para así calcular la distancia de cada propiedad a la entrada del metro de NYC más cercana, y agregarla al dataset de propiedades como una nueva columna.\n",
        "\n",
        "https://osmnx.readthedocs.io/en/stable/user-reference.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpkOKFipJFGv"
      },
      "outputs": [],
      "source": [
        "# Verificamos que los CRS a trabajar estén en latitud/longitud (EPSG:4326):\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "print(f\"CRS de estaciones de metro: {gdf_subway.crs}\")\n",
        "\n",
        "# Usamos network_type=walk para descargar la red peatonal, porque nos interesa llegar al subway caminando.\n",
        "walk_map_NYC_OSMNX = ox.graph_from_place(\"Manhattan, New York City, USA\", network_type='walk')\n",
        "\n",
        "# Creamos los nodos más cercanos entre las propiedades y el mapa:\n",
        "property_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_properties.geometry.x, Y=gdf_properties.geometry.y)\n",
        "subway_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_subway.geometry.x, Y=gdf_subway.geometry.y) # Lo mismo, pero con subway entrances.\n",
        "\n",
        "# Añadimos el nodo de la propiedad a la calle más cercano como un nuevo atributo del GDF de propiedades.\n",
        "gdf_properties['nearest_node'] = property_nodes\n",
        "\n",
        "# Para posteriormente evitar hacer cálculos con nodos repetidos, sacamos sólo los que son únicos:\n",
        "unique_subway_nodes = set(subway_nodes)\n",
        "\n",
        "print(gdf_properties[['ADDRESS', 'SALE PRICE', 'nearest_node']].head())\n",
        "# Las unidades no aparecen, pero no importa porque las reemplazaremos en la siguiente celda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Q3mRjUPAxz"
      },
      "source": [
        "Aquí, usamos el algoritmo de búsqueda de caminos Dijkstra https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.weighted.multi_source_dijkstra_path_length.html. Al principio habíamos planteado el uso del algoritmo A*, pero éste último realiza la búsqueda entre cada nodo, lo que es computacionalmente muy costoso y tomaba mucho tiempo. El algoritmo Dijkstra, en cambio, recorre, para cada propiedad, los caminos hasta los nodos de una sola vez, así que hace un cálculo más complejo pero lo realiza todo de una sola vez, lo que es más rápido en este caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5ZI00MFJHRN"
      },
      "outputs": [],
      "source": [
        "# Calculamos las distancias desde todas las estaciones del metro de NYC hacia sus nodos más cercanos.\n",
        "# Entonces, se almacenarán las todos los nodos de Manhattan con la distancia de cada nodo al metro más cercano:\n",
        "distances_from_subways = nx.multi_source_dijkstra_path_length(walk_map_NYC_OSMNX, sources=unique_subway_nodes, weight='length')\n",
        "\n",
        "# Una vez obtenidas las distancias, hacemos un loop de cada una de ellas con .map y las agregamos a dist_real_subway\n",
        "gdf_properties['dist_real_subway'] = gdf_properties['nearest_node'].map(distances_from_subways)\n",
        "\n",
        "# Reemplazamos dist_real_subway con dist_real_subway en el GDF, y luego guardamos el archivo para cachear.\n",
        "gdf_properties_dist_subway = gdf_properties.copy()\n",
        "gdf_properties_dist_subway['dist_real_subway'] = gdf_properties['dist_real_subway']\n",
        "\n",
        "gdf_properties_dist_subway['dist_real_subway'].fillna(-1, inplace=True) # En caso de errores, rellenamos vacios con -1\n",
        "\n",
        "print(gdf_properties_dist_subway[['ADDRESS', 'SALE PRICE', 'dist_real_subway']].head(), '\\n')\n",
        "\n",
        "output_file = os.path.join(\".\", \"datasets\", \"gdf_rollingsales_with_subway_dist.gpkg\")\n",
        "gdf_properties_dist_subway.to_file(output_file, driver='GPKG', layer='properties')\n",
        "print(f\"Nuevo GDF guardado como '{output_file}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5w6AZlJtCdZ"
      },
      "source": [
        "Ahora, formaremos una segmentación de los datos, representaremos mediante hexágonos de colores el precio promedio de las casas que están en esa área aproximada, usaremos h3 (https://h3geo.org/) para poder crear fácilmente los hexágonos, la idea es crear la propiedad \"hex_id\" en el GDF de propiedades, con las coordenadas, para luego poder usar eso como agrupamiento, esto es, agrupar según el hex_id, y calcular el precio promedio y la cantidad de viviendas o propiedades que caen dentro de ese hexágono."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn61fj8mQ-wT"
      },
      "outputs": [],
      "source": [
        "# h3 requiere coordenadas en latitud/longitud, por lo que nos aseguramos de tener el CRS correcto (EPSG:4326)\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "\n",
        "# Asignamos cada vivienda a un hexágono\n",
        "resolution = 8\n",
        "gdf_properties['hex_id'] = gdf_properties.apply(\n",
        "    lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, 8), # 8 es la resolution de los hexagonos\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# usamos .groupby para agrupar las viviendas que pertenecen a un mismo hexágono, y luego con\n",
        "# .agg hacemos un agregado donde calculamos el avg_price y la cantidad de propiedades en ese hexágono:\n",
        "hex_aggregated = gdf_properties.groupby('hex_id').agg(\n",
        "    avg_price=('SALE PRICE', 'mean'),\n",
        "    property_count=('SALE PRICE', 'size')\n",
        ").reset_index() # Reseteamos el índice del nuevo GDF porque hubieron cambios importantes.\n",
        "\n",
        "# Hacemos un filtro de n propiedades mínimas que deben existir en cada hexágono:\n",
        "min_properties = 4\n",
        "hex_aggregated_filtered = hex_aggregated[hex_aggregated['property_count'] >= min_properties]\n",
        "\n",
        "# En base a cada id de un hexágono, esta función generará un polígono, que será la geometría del hexágono:\n",
        "def hex_id_to_polygon(hex_id):\n",
        "    points = h3.h3_to_geo_boundary(hex_id, geo_json=True)\n",
        "    return Polygon(points)\n",
        "\n",
        "# Usamos la función, para cada vivienda del GDF, pasamos el hex_id por la función y agregamos el resultado como geometry.\n",
        "hex_aggregated_filtered['geometry'] = hex_aggregated_filtered['hex_id'].apply(hex_id_to_polygon)\n",
        "\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Preparación para los popups\n",
        "properties_for_popup = gdf_properties.merge(hex_aggregated_filtered[['hex_id']], on='hex_id', how='inner')\n",
        "\n",
        "def create_popup_html(group):\n",
        "    html = f\"<b>Propiedades en esta zona: {len(group)}</b><br><ul>\"\n",
        "    for _, row in group.head(10).iterrows():\n",
        "        price_formatted = f\"${row['SALE PRICE']:,.0f}\"\n",
        "        html += f\"<li>{row['ADDRESS']}: {price_formatted}</li>\"\n",
        "    if len(group) > 10:\n",
        "        html += f\"<li>... y {len(group) - 10} más.</li>\"\n",
        "    html += \"</ul>\"\n",
        "    return html\n",
        "\n",
        "popup_data = properties_for_popup.groupby('hex_id').apply(create_popup_html)\n",
        "popup_data.name = 'popup_html'\n",
        "hex_aggregated_filtered = hex_aggregated_filtered.merge(popup_data, on='hex_id', how='left')\n",
        "\n",
        "print(f\"GeoDataFrame final con {len(hex_aggregated_filtered)} hexágonos\")\n",
        "hex_aggregated_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Pl90NtuTT1"
      },
      "source": [
        "Una vez calculado todo, creamos un folium para poder poner los hexágonos en la visualización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bCFRu8yQ_6R"
      },
      "outputs": [],
      "source": [
        "# Creamos un folium centrado en Manhattan, para poder ver mejor los hexágonos\n",
        "map_final = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Según el precio min y max, creamos los valores max y min del mapa de colores.\n",
        "min_price = hex_aggregated_filtered['avg_price'].min()\n",
        "max_price = hex_aggregated_filtered['avg_price'].max()\n",
        "colormap = linear.Reds_09.scale(min_price, max_price)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Iteración por cada entrada del dataset para agregar el hexágono con su color al folium.\n",
        "for _, row in hex_aggregated_filtered.iterrows():\n",
        "    geojson_layer = folium.GeoJson(\n",
        "        row.geometry.__geo_interface__,\n",
        "        style_function=lambda feature, color=colormap(row['avg_price']): {\n",
        "            'color': 'black',\n",
        "            'weight': 0.5,\n",
        "            'fillColor': color,\n",
        "            'fillOpacity': 0.8,\n",
        "        }\n",
        "    )\n",
        "    popup = folium.Popup(row['popup_html'], max_width=400)\n",
        "    popup.add_to(geojson_layer)\n",
        "    geojson_layer.add_to(map_final)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "legend_title = 'Precio Promedio ($)'\n",
        "legend_html = f'''\n",
        "     <div style=\"\n",
        "     position: fixed;\n",
        "     bottom: 50px;\n",
        "     right: 50px;\n",
        "     width: 150px;\n",
        "     height: auto;\n",
        "     border:2px solid grey;\n",
        "     z-index:9999;\n",
        "     font-size:14px;\n",
        "     background-color:white;\n",
        "     padding: 10px;\n",
        "     \">\n",
        "     <b>{legend_title}</b><br>\n",
        "     '''\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Iteramos sobre 6 pasos en nuestra escala de colores para crear las etiquetas, una por cada color.\n",
        "for val in np.linspace(colormap.vmin, colormap.vmax, 6):\n",
        "    color = colormap(val)\n",
        "    val_formatted = f\"${val:,.0f}\"\n",
        "    legend_html += f'<i class=\"fa fa-circle\" style=\"color:{color}\"></i>&nbsp;{val_formatted}<br>'\n",
        "\n",
        "legend_html += '</div>'\n",
        "map_final.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "display(map_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMV1kTbizG3E"
      },
      "source": [
        "Creamos un buffer (utilizado en las primeras ayudantías) https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.buffer.html de 500 metros, para poder ver cuántos tiroteos han ocurrido cerca de cada propiedad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hIiVDRSVlYc"
      },
      "outputs": [],
      "source": [
        "radius_m = 500 # radio de metros para buscar incidentes\n",
        "CRS_METERS = \"EPSG:32118\" # Definimos el CRS para trabajar en metros en NYC\n",
        "\n",
        "# Hacemos copias para no alterar la proyección de los GDF originales.\n",
        "properties_to_analyze = gdf_properties.copy()\n",
        "shootings_to_analyze = gdf_incidents.copy()\n",
        "\n",
        "# Proyectamos al CRS en metros (EPSG:32118 para NYC)\n",
        "properties_proj = properties_to_analyze.to_crs(CRS_METERS)\n",
        "shootings_proj = shootings_to_analyze.to_crs(CRS_METERS)\n",
        "\n",
        "# Creamos un buffer que rodee a cada propiedad, de radius_m metros de medida:\n",
        "property_with_buffer = properties_proj.copy()\n",
        "property_with_buffer['geometry'] = property_with_buffer.geometry.buffer(radius_m)\n",
        "\n",
        "# Hacemos un SPATIAL JOIN del GDF que contiene los tiroteos con el de las propiedades con el buffer.\n",
        "join_result = gpd.sjoin(shootings_proj, property_with_buffer, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Contamos la cantidad de tiroteos que caen en cada propiedad con el buffer.\n",
        "shootings_count = join_result.groupby('index_right').size()\n",
        "shootings_count.name = 'shootings_in_radius'\n",
        "\n",
        "# Unimos el GDF con el conteo de tiroteos que caen en cada vivienda con el buffer.\n",
        "gdf_properties_final = properties_to_analyze.merge(\n",
        "    shootings_count,\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Como limpieza, si no hay tiroteos en una propiedad, le ponemos 0\n",
        "gdf_properties_final['shootings_in_radius'].fillna(0, inplace=True)\n",
        "gdf_properties_final['shootings_in_radius'] = gdf_properties_final['shootings_in_radius'].astype(int) #parseo necesario.\n",
        "\n",
        "propiedades_con_tiroteos = gdf_properties_final[gdf_properties_final['shootings_in_radius'] > 0].shape[0]\n",
        "print(f\"Hay {propiedades_con_tiroteos} propiedades con 1 o más tiroteos en un radio de {radius_m} metros.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkyL_K0n1rei"
      },
      "source": [
        "Luego, hacemos lo mismo que en celdas previas para los hexágonos con precios de propiedades, pero en vez del precio, ahora usamos la cantidad de tiroteos, el código es casi totalmente copiar y pegar, así que se ha usado AI para poder generarlo más rápido, pero se ha revisado con mucho cuidado línea a línea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXEMVkbcvFRq"
      },
      "outputs": [],
      "source": [
        "# h3 requiere coordenadas en latitud/longitud.\n",
        "gdf_incidents_h3 = gdf_incidents.to_crs(\"EPSG:4326\")\n",
        "# También necesitamos las propiedades para los popups.\n",
        "gdf_properties_h3 = gdf_properties.to_crs(\"EPSG:4326\")\n",
        "\n",
        "resolution = 8\n",
        "gdf_incidents_h3['hex_id'] = gdf_incidents_h3.apply(\n",
        "    lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution), axis=1\n",
        ")\n",
        "\n",
        "# Contar los tiroteos por hexágono\n",
        "shooting_counts = gdf_incidents_h3.groupby('hex_id').size().reset_index(name='shooting_count')\n",
        "\n",
        "# GDF de Hexágonos con los Conteos\n",
        "def hex_id_to_polygon(hex_id):\n",
        "    points = h3.h3_to_geo_boundary(hex_id, geo_json=True)\n",
        "    return Polygon(points)\n",
        "\n",
        "# Creamos las geometrías y el GeoDataFrame base\n",
        "shooting_counts['geometry'] = shooting_counts['hex_id'].apply(hex_id_to_polygon)\n",
        "gdf_hexagons_shootings = gpd.GeoDataFrame(shooting_counts, crs=\"EPSG:4326\")\n",
        "\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Asignamos también las propiedades a los hexágonos para los popups\n",
        "gdf_properties_h3['hex_id'] = gdf_properties_h3.apply(\n",
        "    lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution), axis=1\n",
        ")\n",
        "\n",
        "# Unimos para saber qué propiedades están en los hexágonos con tiroteos\n",
        "properties_for_popup = gdf_properties_h3.merge(gdf_hexagons_shootings[['hex_id']], on='hex_id', how='inner')\n",
        "\n",
        "def create_popup_html(group):\n",
        "    html = f\"<b>Propiedades en esta zona de riesgo: {len(group)}</b><br><ul>\"\n",
        "    for _, row in group.head(10).iterrows():\n",
        "        price_formatted = f\"${row['SALE PRICE']:,.0f}\"\n",
        "        html += f\"<li>{row['ADDRESS']}: {price_formatted}</li>\"\n",
        "    if len(group) > 10:\n",
        "        html += f\"<li>... y {len(group) - 10} más.</li>\"\n",
        "    html += \"</ul>\"\n",
        "    return html\n",
        "\n",
        "popup_data = properties_for_popup.groupby('hex_id').apply(create_popup_html)\n",
        "popup_data.name = 'popup_html'\n",
        "# Unimos los popups al GDF de hexágonos de tiroteos\n",
        "gdf_hexagons_shootings = gdf_hexagons_shootings.merge(popup_data, on='hex_id', how='left')\n",
        "gdf_hexagons_shootings['popup_html'].fillna(\"No hay propiedades registradas en esta zona.\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1fvugpIvHN3"
      },
      "outputs": [],
      "source": [
        "# Mapa folium centrado en Manhattan, como antes.\n",
        "map_shootings = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Creamos la misma paleta que antes, con min y max segun cantidad de incidentes.\n",
        "min_count = gdf_hexagons_shootings['shooting_count'].min()\n",
        "max_count = gdf_hexagons_shootings['shooting_count'].max()\n",
        "colormap = linear.YlOrRd_09.scale(min_count, max_count)\n",
        "\n",
        "\n",
        "# Agregamos cada hexágono al folium\n",
        "for _, row in gdf_hexagons_shootings.iterrows():\n",
        "    geojson_layer = folium.GeoJson(\n",
        "        row.geometry.__geo_interface__,\n",
        "        # El color ahora depende de la cantidad de tiroteos, hay una paleta.\n",
        "        style_function=lambda feature, color=colormap(row['shooting_count']): {\n",
        "            'color': 'black',\n",
        "            'weight': 0.5,\n",
        "            'fillColor': color,\n",
        "            'fillOpacity': 0.8,\n",
        "        }\n",
        "    )\n",
        "    # El popup muestra la lista de propiedades en esa zona\n",
        "    popup = folium.Popup(row['popup_html'], max_width=400)\n",
        "    popup.add_to(geojson_layer)\n",
        "    geojson_layer.add_to(map_shootings)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "legend_title = 'Cantidad de Tiroteos'\n",
        "legend_html = f'''\n",
        "     <div style=\"\n",
        "     position: fixed; bottom: 50px; right: 50px; width: 150px; height: auto;\n",
        "     border:2px solid grey; z-index:9999; font-size:14px;\n",
        "     background-color:white; padding: 10px;\">\n",
        "     <b>{legend_title}</b><br>\n",
        "     '''\n",
        "for val in np.linspace(colormap.vmin, colormap.vmax, 6):\n",
        "    color = colormap(val)\n",
        "    # Formateamos el número como un entero\n",
        "    val_formatted = f\"{int(val)}\"\n",
        "    legend_html += f'<i class=\"fa fa-circle\" style=\"color:{color}\"></i>&nbsp;{val_formatted}<br>'\n",
        "legend_html += '</div>'\n",
        "map_shootings.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "display(map_shootings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92HNr3i7Eog"
      },
      "source": [
        "Hemos intentado, pero no hay suficientes datos de incidentes para poder realizar un análisis estadístico con Moran, los resultados son inconsistentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inundaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caso 2024\n",
        "- 6 de Agosto de 2024\n",
        "- https://spectrumnoticias.com/ny/nyc/noticias/2024/08/06/ha-comenzado-la-lluvia--advierten-de-posibles-inundaciones--advertencia-de-viaje-por-tormentas-electricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función que transforma en dB\n",
        "# Según la propia página de Sentinel 1 GRD:\n",
        "# \"se convierten en decibeles mediante la escala de registro (10*log10(x)).\"\n",
        "# https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD?hl=es-419\n",
        "\n",
        "def dB_VV(img):\n",
        "    return img.select('VV').log10().multiply(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "#Pasamos un GDF a FeatureCollection de GEE, para poder utilizarlo en GEE posteriormente:\n",
        "#ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "\n",
        "\n",
        "# Aquí se usa el código extraído de \"floodMapping_Maule.ipynb\".\n",
        "pre_flood_start_date = '2024-08-03'\n",
        "pre_flood_end_date = '2024-08-05'\n",
        "flood_start_date = '2024-08-06'\n",
        "flood_end_date = '2024-08-16'\n",
        "\n",
        "s1_col_pre = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(pre_flood_start_date, pre_flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "\n",
        "s1_col_post= (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(flood_start_date, flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "if s1_col_pre.size().getInfo() > 0 and s1_col_post.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s1_col_pre = s1_col_pre.map(dB_VV)\n",
        "    s1_col_post = s1_col_post.map(dB_VV)\n",
        "    \n",
        "    sar_pre = s1_col_pre.reduce(ee.Reducer.percentile([20]))\n",
        "    sar_post = s1_col_post.reduce(ee.Reducer.percentile([20]))\n",
        "\n",
        "    threshold = -17\n",
        "\n",
        "    sar_pre_filtered = sar_pre.focal_median(radius=1, units='pixels')\n",
        "    sar_post_filtered = sar_post.focal_median(radius=1, units='pixels')\n",
        "\n",
        "    water_pre = sar_pre_filtered.lt(threshold)\n",
        "    water_post = sar_post_filtered.lt(threshold)\n",
        "\n",
        "    flood_extent = water_post.unmask().subtract(water_pre.unmask()).gt(0).selfMask()\n",
        "\n",
        "    flood_points = flood_extent.sample(\n",
        "        region=manhattan_ee,\n",
        "        scale=10,\n",
        "        projection='EPSG:4326',\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # Lo convertimos en gdf:\n",
        "    gdf_fp_24 = geemap.ee_to_gdf(flood_points)\n",
        "\n",
        "    area = flood_extent.reduceRegion(\n",
        "        reducer= ee.Reducer.sum(),\n",
        "        geometry= manhattan_ee,\n",
        "        scale= 10,\n",
        "        bestEffort= True\n",
        "    )\n",
        "\n",
        "    area = area.getInfo()\n",
        "\n",
        "    print(f\"El área inundada es de {str(area['VV_p20'] * 100)} m^2\")\n",
        "\n",
        "    Map.addLayer(sar_pre.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR pre')\n",
        "    Map.addLayer(sar_post.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR post')\n",
        "\n",
        "    Map.addLayer(flood_extent.clip(manhattan_ee), {'palette': 'magenta'}, 'Flood Extent')\n",
        "    Map.addLayer(flood_points, {'color': 'red'}, 'Flood Points 2024')\n",
        "\n",
        "    Map.addLayerControl()\n",
        "    Map\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado imagenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caso 2023:\n",
        "- 29 de Septiembre de 2023\n",
        "- https://www.lanacion.com.ar/estados-unidos/las-intensas-lluvias-provocan-inundaciones-repentinas-en-nueva-york-e-interrupciones-de-servicios-nid29092023/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "#Pasamos un GDF a FeatureCollection de GEE, para poder utilizarlo en GEE posteriormente:\n",
        "#ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "\n",
        "\n",
        "# Aquí se usa el código extraído de \"floodMapping_Maule.ipynb\".\n",
        "pre_flood_start_date = '2023-09-25'\n",
        "pre_flood_end_date = '2023-09-28'\n",
        "flood_start_date = '2023-09-29'\n",
        "flood_end_date = '2023-10-09'\n",
        "\n",
        "s1_col_pre = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(pre_flood_start_date, pre_flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "\n",
        "s1_col_post= (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(flood_start_date, flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "if s1_col_pre.size().getInfo() > 0 and s1_col_post.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s1_col_pre = s1_col_pre.map(dB_VV)\n",
        "    s1_col_post = s1_col_post.map(dB_VV)\n",
        "\n",
        "    sar_pre = s1_col_pre.reduce(ee.Reducer.percentile([20]))\n",
        "    sar_post = s1_col_post.reduce(ee.Reducer.percentile([20]))\n",
        "\n",
        "    threshold = -17\n",
        "\n",
        "    sar_pre_filtered = sar_pre.focal_median(radius=1, units='pixels')\n",
        "    sar_post_filtered = sar_post.focal_median(radius=1, units='pixels')\n",
        "\n",
        "    water_pre = sar_pre_filtered.lt(threshold)\n",
        "    water_post = sar_post_filtered.lt(threshold)\n",
        "\n",
        "    flood_extent = water_post.unmask().subtract(water_pre.unmask()).gt(0).selfMask()\n",
        "\n",
        "    flood_points = flood_extent.sample(\n",
        "        region=manhattan_ee,\n",
        "        scale=10,\n",
        "        projection='EPSG:4326',\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # Lo convertimos en gdf:\n",
        "    gdf_fp_23 = geemap.ee_to_gdf(flood_points)\n",
        "\n",
        "    area = flood_extent.reduceRegion(\n",
        "        reducer= ee.Reducer.sum(),\n",
        "        geometry= manhattan_ee,\n",
        "        scale= 10,\n",
        "        bestEffort= True\n",
        "    )\n",
        "\n",
        "    area = area.getInfo()\n",
        "\n",
        "    print(f\"El área inundada es de {str(area['VV_p20'] * 100)} m^2\")\n",
        "\n",
        "    Map.addLayer(sar_pre.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR pre')\n",
        "    Map.addLayer(sar_post.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR post')\n",
        "\n",
        "    Map.addLayer(flood_extent.clip(manhattan_ee), {'palette': 'magenta'}, 'Flood Extent')\n",
        "    Map.addLayer(flood_points, {'color': 'blue'}, 'Flood Points 2023')\n",
        "\n",
        "    Map.addLayerControl()\n",
        "    Map\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado imagenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Revisando ambos casos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gdf_fp_23['year'] = '2023'\n",
        "gdf_fp_24['year'] = '2024'\n",
        "\n",
        "gdf = gpd.GeoDataFrame(pd.concat([gdf_fp_24, gdf_fp_23], ignore_index=True))\n",
        "\n",
        "gdf = gdf.to_crs(epsg=2263)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "gdf.plot(ax=ax, column='year', categorical=True, markersize=5, legend=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uso de Clustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "coords = np.array(list(zip(gdf.geometry.x, gdf.geometry.y)))\n",
        "\n",
        "db = DBSCAN(eps=328.084, min_samples=3).fit(coords) \n",
        "gdf['cluster'] = db.labels_\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "gdf.plot(ax=ax, column='cluster', categorical=True, markersize=5, legend=True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
