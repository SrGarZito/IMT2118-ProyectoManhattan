{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaf1reU6AQOf"
      },
      "source": [
        "Hemos optado por sólo codificar en inglés, pero las explicaciones y comentarios del código estarán en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6b0gs4K-iKZ"
      },
      "outputs": [],
      "source": [
        "# Importamos todas las librerías a utilizar:\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm # Para barras de progreso, ayuda mucho ya que la API tiene delay de 1s por cada request!!\n",
        "from shapely.geometry import Point\n",
        "from shapely import wkt # Lo usaremos para parsear algunas columnas de algunos datasets a geometry, en un GDF.\n",
        "import geemap,ee\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYl6tJA4PEbB"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-aesmatias')\n",
        "#Aqui, por google colab no tuve que usar un token ni nada, pero se requiere autenticacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqT6Q5pQGd2"
      },
      "source": [
        "Descargamos el Shapefile de USA desde https://gadm.org/download_country.html, eligiendo United States y descomprimiendo el .zip, eso nos dará los archivos necesarios, luego cargamos el Shapefile de USA y filtramos el AOI en Manhattan, para finalmente de transformarlo a GEOJSON y poder utilizarlo en GEE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "664WxEU8QKRE"
      },
      "outputs": [],
      "source": [
        "# Nivel 2 para elegir los condados, luego lo pasamos a EPSG:4326, compatible con GEE\n",
        "gdf_USA = gpd.read_file(\"gadm41_USA_2.shp\").to_crs(epsg=4326)\n",
        "\n",
        "ny_TO_GDF = gdf_USA[gdf_USA['NAME_2'] == 'New York'] # Seleccionamos New York\n",
        "ny_TO_GDF.to_file(\"manhattan.geojson\", driver=\"GeoJSON\") # Hacemos un .geojson y lo guardamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dF4zmYQSSLx"
      },
      "outputs": [],
      "source": [
        "# Cargamos el geojson creado, para que GEE lo pueda utilizar\n",
        "gdf = gpd.read_file(\"manhattan.geojson\")\n",
        "manhattan_geojson = json.loads(gdf.to_json())\n",
        "manhattan_ee = ee.FeatureCollection(manhattan_geojson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvfjXaAHUpXI"
      },
      "source": [
        "Visualizamos manhattan con GEE, podemos ajustar los parámetros como la opacidad del AOI en el mapa interactivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpxhsKuLRQYq"
      },
      "outputs": [],
      "source": [
        "manhattanCollection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") # Mayo 2024 - Abril 2025\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con menos de 20% de nubes\n",
        "    .median() # Usamos la mediana de las imágenes de momento, sólo queremos apreciar el mapa\n",
        "    .clip(manhattan_ee)) # Recortamos en Manhattan, la AOI\n",
        "\n",
        "Map = geemap.Map(center=[40.783, -73.971], zoom=12)\n",
        "\n",
        "Map.addLayer(manhattanCollection, {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0,\n",
        "    'max': 3000\n",
        "}, 'RGB')\n",
        "\n",
        "Map.addLayer(manhattan_ee.style(**{\n",
        "    'width': 1,\n",
        "    'color': 'red', # El borde será CYAN\n",
        "    #'fillColor': '00000000',  # Color transparente de relleno\n",
        "}), {}, 'AOI Manhattan')\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6oi1iEb_D6S"
      },
      "source": [
        "Se ha utilizado el dataset de Manhattan, obtenido en https://www.nyc.gov/site/finance/property/property-rolling-sales-data.page, la fecha de los registros del dataset está entre Mayo 2024 - Abril 2025."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM8Dd5KBLAhL"
      },
      "source": [
        "Aquí, en las celdas iniciales que siguen, se muestra como se han geocodificado algunos datos con ayuda de una API, pero no hace falta usarlo, porque los datos procesados ya han sido guardados en un .csv (rollingsales_manhattan_geocoded.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgztuOHI0EU"
      },
      "outputs": [],
      "source": [
        "'''La API que usamos para geocodificar en su capa gratuita es para testing, este proyecto\n",
        "Universitario no califica como un proyecto de producción, no hay usuario final que lo utilizará.\n",
        "Además, sólo hicimos uso de la API en pocas ocasiones'''\n",
        "\n",
        "OPENCAGE_API_KEY = 'API_KEY'\n",
        "OPENCAGE_BASE_URL = 'https://api.opencagedata.com/geocode/v1/json'\n",
        "\n",
        "input_excel_file = './rollingsales_manhattan.xlsx' # XLSX con rolling sales\n",
        "\n",
        "# Output y failed logs, para poder tener una reanudación cada 2500 request en la geocodificación\n",
        "output_csv_file = './rollingsales_manhattan_geocoded.csv'\n",
        "failed_addresses_file = './rollingsales_manhattan_geocoding_failures.csv'\n",
        "\n",
        "MAX_DAILY_REQUESTS = 2500\n",
        "REQUEST_DELAY = 1.0 # seconds\n",
        "\n",
        "REQUIRED_COLUMNS = ['ADDRESS', 'BOROUGH', 'ZIP CODE', 'SALE PRICE'] # Cols necesarias del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgsut25JEsu"
      },
      "source": [
        "Creamos la función que procesará cada solicitud a la API para poder geocodificar las direcciones y los ZIP codes en coordenadas geográficas que usaremos en los GDF posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuDChyt3I0bJ"
      },
      "outputs": [],
      "source": [
        "# Esta función hace un llamado a la API, y retorna un array con la latitud, longitud y estado\n",
        "def geocode_address(address_str, api_key, borough=None, zip_code=None):\n",
        "    query = f\"{address_str}, {borough}, New York, NY {zip_code}\" if borough and zip_code else address_str\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'key': api_key,\n",
        "        'language': 'en',\n",
        "        'no_annotations': 1,\n",
        "        'limit': 1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(OPENCAGE_BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data and data['results']:\n",
        "            lat = data['results'][0]['geometry']['lat']\n",
        "            lng = data['results'][0]['geometry']['lng']\n",
        "\n",
        "            components = data['results'][0].get('components', {})\n",
        "            is_nyc = False # Empieza como false, y si lo encontramos, lo cambiamos a True:\n",
        "            if 'state_code' in components and components['state_code'] == 'NY':\n",
        "                if 'city' in components and components['city'] in ['New York', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', 'Manhattan']:\n",
        "                    is_nyc = True\n",
        "                elif 'county' in components and ('New York County' in components['county'] or \\\n",
        "                                                 'Kings County' in components['county'] or \\\n",
        "                                                 'Queens County' in components['county'] or \\\n",
        "                                                 'Bronx County' in components['county'] or \\\n",
        "                                                 'Richmond County' in components['county']):\n",
        "                    is_nyc = True\n",
        "\n",
        "            if is_nyc:\n",
        "                return lat, lng, \"Success\"\n",
        "            else:\n",
        "                return None, None, \"Not_NYC_Result\"\n",
        "        else:\n",
        "            return None, None, \"No_Results\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if response.status_code == 429:\n",
        "            return None, None, \"Rate_Limit_Exceeded\"\n",
        "        elif response.status_code == 401:\n",
        "            print(f\"API Key Error!!\")\n",
        "            return None, None, \"API_Key_Error\"\n",
        "        elif response.status_code == 402: # Entonces, llegamos al límite de la quota diaria gratis\n",
        "            print(f\"ERROR, Quota exceded!\")\n",
        "            return None, None, \"Payment_Required_Error\"\n",
        "        else:\n",
        "            print(f\"Request error: {e}\")\n",
        "            return None, None, f\"Request_Error: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None, f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yyyEAuxJYMY"
      },
      "source": [
        "Comenzamos a procesar las direcciones y ZIP codes a coordenadas geográficas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGnrrVuQ34uk",
        "outputId": "87d13f24-7693-43c8-b34b-53d66eb7cace"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Geocoding:   8%|▊         | 233/2754 [08:16<1:26:54,  2.07s/it]"
          ]
        }
      ],
      "source": [
        "print(f\"*** Procesando {input_excel_file} ***\")\n",
        "\n",
        "df = None\n",
        "# Agregamos las columnas requeridas que fallaron a entradas del df fallido, para re intentar si se quisiera:\n",
        "failed_df = pd.DataFrame(columns=REQUIRED_COLUMNS + ['Reason'])\n",
        "\n",
        "try:\n",
        "    if pd.io.common.file_exists(output_csv_file):\n",
        "        print(f\"Fichero '{output_csv_file}' con progreso encontrado - Resumiendo...\")\n",
        "        df = pd.read_csv(output_csv_file)\n",
        "        # Si no hay latitud, longitud o estado de la geocodificación, sabemos que la entrada no ha sido procesada:\n",
        "        for col in ['LATITUDE', 'LONGITUDE', 'GEOCODING_STATUS']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "            df[col] = df[col].astype(object)\n",
        "\n",
        "    else:\n",
        "        print(f\"No se encontró el fichero con progreso, cargando el fichero inicial XLSX: '{input_excel_file}'.\")\n",
        "        found_header = False\n",
        "        # Probamos con headers desde el 0 al 10, porque algunos datasets XLSX tienen las primeras entradas con información,\n",
        "        # hay que evitar las primeras entradas que no son los headers, para no obtener errores:\n",
        "        for header_row_index in range(10):\n",
        "            try:\n",
        "                print(f\"Intentando cargar con el header {header_row_index}\")\n",
        "                df_temp = pd.read_excel(input_excel_file, header=header_row_index)\n",
        "\n",
        "                if all(col in df_temp.columns for col in REQUIRED_COLUMNS):\n",
        "                    df = df_temp\n",
        "                    found_header = True\n",
        "                    print(f\"Header encontrado en el índice {header_row_index}!\")\n",
        "                    print(\"Columnass encontradas en el DF:\", df.columns.tolist())\n",
        "                    break # Si el header es encontrado, dejamos de loopear\n",
        "                else:\n",
        "                    print(f\"No hay header en el índice {header_row_index}, probando el siguiente...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "        if not found_header: # Si no hay header, hay un error en el fichero\n",
        "            print(f\"No se ha encontrado un header, error en el fichero XLSX\")\n",
        "            exit()\n",
        "\n",
        "        # Limpiamos las columnas del DF y las parseamos\n",
        "        df['ADDRESS'] = df['ADDRESS'].fillna('').astype(str)\n",
        "        df['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].fillna('').astype(str)\n",
        "        df['BOROUGH'] = df['BOROUGH'].fillna('').astype(str)\n",
        "        df['ZIP CODE'] = df['ZIP CODE'].fillna(0).astype(int).astype(str).replace('0', '')\n",
        "\n",
        "        # Agregamos la latitud, longitud, y estado de la geocodificación, como nuevas columnas en el DF:\n",
        "        df['LATITUDE'] = None\n",
        "        df['LONGITUDE'] = None\n",
        "        df['GEOCODING_STATUS'] = None\n",
        "\n",
        "    already_geocoded_count = df['LATITUDE'].notna().sum()\n",
        "    requests_made_now = 0\n",
        "    print(f\"Los registros geocodificados hasta el momento son: {already_geocoded_count}\")\n",
        "\n",
        "    # La siguiente línea veririfica si la latitud está vacía y el \"status\" es diferente de \"Not_NYC_Result\"\n",
        "    # Si el registro tiene GEOCODIG_STATUS = 'Not_NYC_Result', entonces ha fallado la geocodificación.\n",
        "    rows_to_geocode = df[(df['LATITUDE'].isna()) & (df['GEOCODING_STATUS'] != 'Not_NYC_Result')]\n",
        "\n",
        "    print(f\"Han fallado: {len(rows_to_geocode)} registros\")\n",
        "\n",
        "    for index, row in tqdm(rows_to_geocode.iterrows(), total=len(rows_to_geocode), desc=\"Geocoding\"): #tqdm para barra de progrso\n",
        "        if requests_made_now >= MAX_DAILY_REQUESTS:\n",
        "            print(f\"Se ha llegado al límite de {MAX_DAILY_REQUESTS} requests diarias!\")\n",
        "            break\n",
        "\n",
        "        address = row['ADDRESS']\n",
        "        borough = row['BOROUGH']\n",
        "        zip_code = row['ZIP CODE'] if row['ZIP CODE'] != '0' else ''\n",
        "\n",
        "        if not address:\n",
        "            df.loc[index, 'GEOCODING_STATUS'] = \"Empty_Address\"\n",
        "            continue\n",
        "\n",
        "        lat, lon, status = geocode_address(address, OPENCAGE_API_KEY, borough, zip_code)\n",
        "        requests_made_now += 1\n",
        "\n",
        "        df.loc[index, 'LATITUDE'] = lat\n",
        "        df.loc[index, 'LONGITUDE'] = lon\n",
        "        df.loc[index, 'GEOCODING_STATUS'] = status\n",
        "\n",
        "        if status in [\"Rate_Limit_Exceeded\", \"API_Key_Error\", \"Payment_Required_Error\"]:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY) # La documentación de la API indica un delay de 1 segundo entre cada request\n",
        "\n",
        "        # Guardamos el progreso en chunks de cada 100 solicitudes a la API:\n",
        "        if requests_made_now % 100 == 0:\n",
        "            print(f\"Guardando progreso en la request número {requests_made_now}\")\n",
        "            df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Geocodificación finalizada!!\")\n",
        "\n",
        "    failed_rows = df[df['LATITUDE'].isna()] # Si no tiene latitud, es una row fallida\n",
        "    if not failed_rows.empty:\n",
        "        failed_df_to_save = failed_rows[['BOROUGH', 'NEIGHBORHOOD', 'ADDRESS', 'ZIP CODE', 'GEOCODING_STATUS']].copy()\n",
        "        # Guardamos en failed_addresses_file sólo las rows de la variable de arriba, para poder procesarlas luego.\n",
        "        failed_df_to_save.to_csv(failed_addresses_file, index=False)\n",
        "        print(f\"Los registros fallidos se guardaron en: {failed_addresses_file}\")\n",
        "    else:\n",
        "        print(\"Atención! Finalización inesperada, posiblemente ha ocurrido un error.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Archivo '{input_excel_file}' no hallado.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyojXTMXc6L"
      },
      "source": [
        "Cargamos el fichero con la latitud y longitud agregadas en la geocodificación, para luego, filtrar los registros que tengan NaN en latitud y longitud, ya que eso es producto de errores en la geocodificación de dichos valores. También cribamos y sólo tomamos como válidos valores con precio de venta mayor a 0, ya que hay varios valores en el dataset con propiedades que se han vendido a costo 0, lo que no tiene representación estadística en nuestro contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxJdB0e94hCo"
      },
      "outputs": [],
      "source": [
        "properties_geocoded_file = './rollingsales_manhattan_geocoded.csv'\n",
        "\n",
        "try:\n",
        "    df_properties = pd.read_csv(properties_geocoded_file)\n",
        "    print(f\"***Cargando fichero: {properties_geocoded_file}*** \\n\")\n",
        "    print(f\"Cantidad de registros encontrados en {str(properties_geocoded_file)}: {len(df_properties)} \\n\")\n",
        "    print(\"df_properties.info(): \\n\")\n",
        "    print(df_properties.info())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminamos los registros que sean NaN en latitud y longitud.\n",
        "df_properties.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
        "print(f\"Registros después de eliminar NaN en lat/lon: {len(df_properties)}\")\n",
        "\n",
        "# Transformamos todo SALE PRICE a numeric, para luego, con coerce, reemplazar los valores no numericos a NaN\n",
        "df_properties['SALE PRICE'] = pd.to_numeric(df_properties['SALE PRICE'], errors='coerce')\n",
        "# Luego,eliminamos todas esas filas que contienen NaN\n",
        "df_properties.dropna(subset=['SALE PRICE'], inplace=True)\n",
        "print(f\"Registros después de filtrar por NaN: {len(df_properties)}\")\n",
        "\n",
        "# Eliminamos los registros que tengan precio de venta menor o igual a 0, sin representación estadística.\n",
        "df_properties = df_properties[df_properties['SALE PRICE'] > 0]\n",
        "print(f\"Registros después de filtrar por precio de venta > 0: {len(df_properties)}\")\n",
        "\n",
        "# Filtramos duplicados\n",
        "initial_rows_before_deduplication = len(df_properties)\n",
        "df_properties.drop_duplicates(inplace=True)\n",
        "print(f\"Registros después de filtrar por posibles duplicados: {len(df_properties)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1kLfLf6cMl5"
      },
      "source": [
        "Luego, para una posterior manipulación y tener una mayor compatibilidad con GEE y librerías para graficar, \"transformamos\" el dataframe a un geodataframe, y le agregamos una nueva columna de tipo geometry, que contendrá puntos generados a través de las propiedades de latitud y longitud, los cuales están definidos en la variable geometry, haciendo uso del métetodo zip y list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAhuFKN4lFW"
      },
      "outputs": [],
      "source": [
        "# A partir de las coordenadas, creamos objetos de tipo Point, de lalibrería shapely.geometry, para luego manipular mejor:\n",
        "geometry = [Point(xy) for xy in zip(df_properties['LONGITUDE'], df_properties['LATITUDE'])]\n",
        "\n",
        "# Creamos el nuevo geodataframe para, a partir del dataframe anterior, llenarlo con los datos:\n",
        "gdf_properties = gpd.GeoDataFrame(df_properties, geometry=geometry, crs=\"EPSG:4326\") #CRS es EPSG:4326 para latitud y longitud.\n",
        "\n",
        "print(\"Nuevo GeoDataFrame:\")\n",
        "gdf_properties.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgWEfpVligZX"
      },
      "source": [
        "Cargamos el dataset https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic que contiene el histórico de tiroteos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1--5td5E3i"
      },
      "outputs": [],
      "source": [
        "shooting_incident_historic = 'NYPD_Shooting_Incident_Data__Historic_.csv'\n",
        "df_shooting_incident_historic = None\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado \\n\")\n",
        "    print(f'Información del fichero: \\n')\n",
        "    df_shooting_incident_historic.info()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjdic-wQqglJ"
      },
      "source": [
        " Al igual que antes, este dataset también debe ser filtrado por fecha de interés y ser convertido a GDF, la columna con el Point, que contiene la geometría de la latitud y longitud, está en una columna llamada \"Lon_Lat\", y contiene valores de tipo Point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RFXeYKI5Spj"
      },
      "outputs": [],
      "source": [
        "LATITUDE_COL = 'Latitude'\n",
        "LONGITUDE_COL = 'Longitude'\n",
        "INCIDENT_DATE_COL = 'OCCUR_DATE'\n",
        "BORO_COL = 'BORO'\n",
        "year_of_preference = 2024 # Nos interesan datos de tiroteos en los últimos 2 años\n",
        "\n",
        "# Cargamos el archivo y definimos una variable para su dataframe\n",
        "shooting_incident_historic = 'NYPD_Shooting_Incident_Data__Historic_.csv'\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado. Su longitud es: {len(df_shooting_incident_historic)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Eliminamos las filas con NaN en las coordenadas\n",
        "df_shooting_incident_historic.dropna(subset=[LATITUDE_COL, LONGITUDE_COL], inplace=True)\n",
        "\n",
        "# Eliminamos las filas duplicadas\n",
        "df_shooting_incident_historic.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_shooting_incident_historic)}\")\n",
        "\n",
        "# Si la columna con fecha del incidente existe, filtramos por BORO y AÑO:\n",
        "if INCIDENT_DATE_COL in df_shooting_incident_historic.columns:\n",
        "    # Convertimos la columna de OCCUR_DATE a datatime de pandas, para trabajarla con el formato de USA:\n",
        "    df_shooting_incident_historic[INCIDENT_DATE_COL] = pd.to_datetime(\n",
        "        df_shooting_incident_historic[INCIDENT_DATE_COL],\n",
        "        format='%m/%d/%Y',\n",
        "        errors='coerce' # Los valores sin fecha válida serán NaT\n",
        "    )\n",
        "    # Dropeamos valores sin fecha válida (NaT)\n",
        "    df_shooting_incident_historic.dropna(subset=[INCIDENT_DATE_COL], inplace=True)\n",
        "    df_shooting_incident_historic['INCIDENT_YEAR'] = df_shooting_incident_historic[INCIDENT_DATE_COL].dt.year\n",
        "\n",
        "    # Filtramos para sólo obtener datos de Manhattan (en la col BORO)\n",
        "    if BORO_COL in df_shooting_incident_historic.columns:\n",
        "        # Usamos .copy() para definir la variable por valor, y no por referencia en memoria:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_historic[df_shooting_incident_historic[BORO_COL] == 'MANHATTAN'].copy()\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes en Manhattan\")\n",
        "\n",
        "        # Filtramos según el año deseado:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_manhattan[df_shooting_incident_manhattan['INCIDENT_YEAR'] >= year_of_preference]\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes posteriores al año {year_of_preference}. \\n\")\n",
        "\n",
        "        # Antes de convertir el DF a GDF, necesitamos col geometry que contiene puntos, los cuales están en la\n",
        "        # columna Lon_Lat, por lo que la parseamos:\n",
        "        df_shooting_incident_manhattan['Lon_Lat'] = df_shooting_incident_manhattan['Lon_Lat'].apply(wkt.loads)\n",
        "\n",
        "        # Creamos el GeoDataFrame de los incidentes y lo asignamos en una nueva variable:\n",
        "        geometry_incidents = [Point(xy) for xy in zip(df_shooting_incident_manhattan[LONGITUDE_COL], df_shooting_incident_manhattan[LATITUDE_COL])]\n",
        "\n",
        "        # Renombramos la columna Lon_Lat a geometry, ya que sabemos que existe Lon_Lat, que es un tipo de dato Point\n",
        "        df_shooting_incident_manhattan.rename(columns={'Lon_Lat': 'geometry'}, inplace=True)\n",
        "\n",
        "        # Creamos el GDF utilizando la col geometry:\n",
        "        gdf_incidents = gpd.GeoDataFrame(df_shooting_incident_manhattan, geometry='geometry', crs=\"EPSG:4326\")  # CRS 4326 para lat/lon\n",
        "\n",
        "        print(\"Información del GDF:\")\n",
        "        gdf_incidents.info()\n",
        "    else:\n",
        "      raise KeyError(f\"Error: La columna '{BORO_COL}' no se encontró!! \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PnqnbA94xwJ"
      },
      "source": [
        "Cargaremos los datasets del Sentinel-5P, para poder utilizar la fórmula del índice de calidad del aire (AQI) obtenida en https://document.airnow.gov/technical-assistance-document-for-the-reporting-of-daily-air-quailty.pdf\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HkWgKzf4yDg"
      },
      "outputs": [],
      "source": [
        "# Mayo 2024 - Abril 2025:\n",
        "date_start = '2024-05-01'\n",
        "date_end = '2025-04-30'\n",
        "\n",
        "# Obtenemos las bandas para cada gas a través de esta función, entre la fecha inicial y final:\n",
        "def get_band(collection_id, band, date_start, date_end):\n",
        "    image_collection = ee.ImageCollection(collection_id) \\\n",
        "        .select(band) \\\n",
        "        .filterDate(date_start, date_end)\n",
        "\n",
        "    # Promedio de las pasadas de ese día, pero tengo entendido que es una cada 24 horas:\n",
        "    daily_image = image_collection.mean()\n",
        "\n",
        "    print(f\"Imágnees encontradas para {date_start}-{date_end}: {image_collection.size().getInfo()}.\")\n",
        "    return daily_image.clip(manhattan_ee) # Clip en AOI\n",
        "\n",
        "# Para cada gas, usamos un dataset diferente correspondiente a dicho gas, y obtenemos su banda:\n",
        "no2_raw = get_band('COPERNICUS/S5P/NRTI/L3_NO2', 'NO2_column_number_density', date_start, date_end)\n",
        "o3_raw  = get_band('COPERNICUS/S5P/NRTI/L3_O3', 'O3_column_number_density', date_start, date_end)\n",
        "so2_raw = get_band('COPERNICUS/S5P/NRTI/L3_SO2', 'SO2_column_number_density', date_start, date_end)\n",
        "co_raw  = get_band('COPERNICUS/S5P/NRTI/L3_CO', 'CO_column_number_density', date_start, date_end)\n",
        "\n",
        "# Factores de conversión dados por ChatGPT, hay que re ajustar en la segunda entrega del proyecto, según el pdf del estudio:\n",
        "# ESTO ES PARA VISUALIZACIÓN, NO PARA PRECISIÓN CIENTÍFICA.\n",
        "NO2_CONVERSION_FACTOR = 7e6\n",
        "O3_CONVERSION_FACTOR  = 6e6\n",
        "SO2_CONVERSION_FACTOR = 4e6\n",
        "CO_CONVERSION_FACTOR  = 1e6\n",
        "\n",
        "no2_ppb_calculated = no2_raw.multiply(NO2_CONVERSION_FACTOR)\n",
        "o3_ppb_calculated  = o3_raw.multiply(O3_CONVERSION_FACTOR)\n",
        "so2_ppb_calculated = so2_raw.multiply(SO2_CONVERSION_FACTOR)\n",
        "co_ppm_calculated  = co_raw.multiply(CO_CONVERSION_FACTOR)\n",
        "\n",
        "# Reglas de truncado para la visualización\n",
        "no2_truncated = no2_ppb_calculated.round()\n",
        "o3_truncated = o3_ppb_calculated.round()\n",
        "so2_truncated = so2_ppb_calculated.round()\n",
        "co_truncated = co_ppm_calculated.multiply(10).floor().divide(10)\n",
        "\n",
        "# Breakpoints del AQI del estudio (US EPA - CRÍTICOS)\n",
        "def calculate_single_pollutant_aqi(concentration_image, pollutant_name):\n",
        "    if pollutant_name == 'NO2':\n",
        "        breakpoints = [\n",
        "            {'C_low': 0,   'C_high': 53,  'I_low': 0,   'I_high': 50},\n",
        "            {'C_low': 54,  'C_high': 100, 'I_low': 51,  'I_high': 100},\n",
        "            {'C_low': 101, 'C_high': 360, 'I_low': 101, 'I_high': 150},\n",
        "            {'C_low': 361, 'C_high': 649, 'I_low': 151, 'I_high': 200},\n",
        "            {'C_low': 650, 'C_high': 1249,'I_low': 201, 'I_high': 300},\n",
        "        ]\n",
        "    elif pollutant_name == 'O3':\n",
        "        breakpoints = [\n",
        "            {'C_low': 0,   'C_high': 54,  'I_low': 0,   'I_high': 50},\n",
        "            {'C_low': 55,  'C_high': 70,  'I_low': 51,  'I_high': 100},\n",
        "            {'C_low': 71,  'C_high': 85,  'I_low': 101, 'I_high': 150},\n",
        "            {'C_low': 86,  'C_high': 105, 'I_low': 151, 'I_high': 200},\n",
        "            {'C_low': 106, 'C_high': 200, 'I_low': 201, 'I_high': 300},\n",
        "        ]\n",
        "    elif pollutant_name == 'SO2':\n",
        "        breakpoints = [\n",
        "            {'C_low': 0,   'C_high': 35,  'I_low': 0,   'I_high': 50},\n",
        "            {'C_low': 36,  'C_high': 75,  'I_low': 51,  'I_high': 100},\n",
        "            {'C_low': 76,  'C_high': 185, 'I_low': 101, 'I_high': 150},\n",
        "            {'C_low': 186, 'C_high': 304, 'I_low': 151, 'I_high': 200},\n",
        "            {'C_low': 305, 'C_high': 604, 'I_low': 201, 'I_high': 300},\n",
        "        ]\n",
        "    elif pollutant_name == 'CO':\n",
        "        breakpoints = [\n",
        "            {'C_low': 0.0, 'C_high': 4.4, 'I_low': 0,   'I_high': 50},\n",
        "            {'C_low': 4.5, 'C_high': 9.4, 'I_low': 51,  'I_high': 100},\n",
        "            {'C_low': 9.5, 'C_high': 12.4,'I_low': 101, 'I_high': 150},\n",
        "            {'C_low': 12.5,'C_high': 15.4,'I_low': 151, 'I_high': 200},\n",
        "            {'C_low': 15.5,'C_high': 30.4,'I_low': 201, 'I_high': 300},\n",
        "        ]\n",
        "    else:\n",
        "        return ee.Image(0)\n",
        "\n",
        "    aqi_image = ee.Image(0).float() # Pasamos a punto flotante\n",
        "\n",
        "    for i, bp in enumerate(breakpoints):\n",
        "        c_low = ee.Number(bp['C_low'])\n",
        "        c_high = ee.Number(bp['C_high'])\n",
        "        i_low = ee.Number(bp['I_low'])\n",
        "        i_high = ee.Number(bp['I_high'])\n",
        "\n",
        "        # Cálculo del AQI mediante la fórmula del estudio:\n",
        "        current_aqi_segment = concentration_image.subtract(c_low) \\\n",
        "            .divide(c_high.subtract(c_low)) \\\n",
        "            .multiply(i_high.subtract(i_low)) \\\n",
        "            .add(i_low)\n",
        "\n",
        "        if i == len(breakpoints) - 1:\n",
        "            aqi_image = aqi_image.where(concentration_image.gte(c_low), current_aqi_segment)\n",
        "        else:\n",
        "            aqi_image = aqi_image.where(\n",
        "                concentration_image.gte(c_low).And(concentration_image.lt(c_high)),\n",
        "                current_aqi_segment\n",
        "            )\n",
        "\n",
        "    return aqi_image.round()\n",
        "\n",
        "no2_aqi = calculate_single_pollutant_aqi(no2_truncated, 'NO2')\n",
        "o3_aqi  = calculate_single_pollutant_aqi(o3_truncated, 'O3')\n",
        "so2_aqi = calculate_single_pollutant_aqi(so2_truncated, 'SO2')\n",
        "co_aqi  = calculate_single_pollutant_aqi(co_truncated, 'CO')\n",
        "\n",
        "aqi = no2_aqi.max(o3_aqi).max(so2_aqi).max(co_aqi)\n",
        "\n",
        "Map = geemap.Map(center=[40.7, -74.0], zoom=10)\n",
        "Map.addLayer(aqi, {'min': 0, 'max': 300, 'palette': ['green', 'yellow', 'orange', 'red', 'purple', 'maroon']}, 'AQI General')\n",
        "Map.addLayer(manhattan_ee, {'color': 'blue', 'opacity': 0.3}, 'Manhattan_AOI')\n",
        "\n",
        "# Agregamos las layers de las concentraciones de gases calculadas\n",
        "Map.addLayer(no2_ppb_calculated, {'min': 0, 'max': 150, 'palette': ['blue', 'cyan', 'green', 'yellow', 'red']}, 'NO2_ppb Calculado')\n",
        "Map.addLayer(o3_ppb_calculated, {'min': 0, 'max': 150, 'palette': ['purple', 'pink', 'white', 'orange', 'red']}, 'O3_ppb Calculado')\n",
        "Map.addLayer(so2_ppb_calculated, {'min': 0, 'max': 80, 'palette': ['gray', 'silver', 'white', 'yellow']}, 'SO2_ppb Calculado')\n",
        "Map.addLayer(co_ppm_calculated, {'min': 0, 'max': 20, 'palette': ['brown', 'tan', 'white', 'orange']}, 'CO_ppm Calculado')\n",
        "\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVzp_4QLGPD0"
      },
      "source": [
        "Cargamos y limpiamos el dataset de universidades y college:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD26vj-n_g5Z"
      },
      "outputs": [],
      "source": [
        "# Definimos la ruta del dataset\n",
        "college_university_file = 'COLLEGE_UNIVERSITY_20250609.csv'\n",
        "df_college_university = None\n",
        "\n",
        "# Esta propiedad en el dataset es un Punto siempre, según la página que lo provee:\n",
        "GEOMETRY_COL = 'the_geom'\n",
        "\n",
        "try:\n",
        "    df_college_university = pd.read_csv(college_university_file)\n",
        "    print(f\"El archivo {college_university_file} ha sido cargado. Su longitud es: {len(df_college_university)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "#Limpieza de nulos:\n",
        "df_college_university.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_college_university)}\")\n",
        "\n",
        "#Limpieza de duplicados\n",
        "df_college_university.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_college_university)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_college_university.columns:\n",
        "    #Parseamos los datos de la columna the_geom y los agregamos a la llamada 'geometry' para poder trabajarlas:\n",
        "    df_college_university['geometry'] = df_college_university[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_college_university.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_college_university)}\")\n",
        "\n",
        "    gdf_universities = gpd.GeoDataFrame(df_college_university, geometry='geometry', crs=\"EPSG:4326\") #Aplicamos la proyección correcta\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Universidades:\")\n",
        "    gdf_universities.info()\n",
        "\n",
        "    # Mostramos el mapa\n",
        "    Map = geemap.Map(center=[40.7, -74.0], zoom=12)\n",
        "    ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "    try:\n",
        "\n",
        "            # Cargamos el dataset Sentinel-2 L2A (surface reflectance) para mostrar las bandas RGB, y filtramos por AOI y fecha: Mayo 2024 - Abril 2025\n",
        "            s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "                .filterBounds(manhattan_ee) \\\n",
        "                .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con porcentaje de nubes menor a 20\n",
        "\n",
        "            # Verificamos que la colección no esté vacía para el intervalo de fechas solicitada:\n",
        "            if s2_collection.size().getInfo() > 0:\n",
        "                s2_image = s2_collection.median().clip(manhattan_ee) # Calculamos la mediana de imágenes y recortamos al AOI\n",
        "\n",
        "                s2_vis_params = {\n",
        "                    'bands': ['B4', 'B3', 'B2'], # B4=Red, B3=Green, B2=Blue\n",
        "                    'min': 0,\n",
        "                    'max': 3000,\n",
        "                    'gamma': 1.4\n",
        "                }\n",
        "\n",
        "                Map.addLayer(s2_image, s2_vis_params, 'Sentinel-2 RGB') # Agregamos la layer al mapa\n",
        "            else:\n",
        "                print(\"El dataset no tiene imágnes para la fecha de interés.\")\n",
        "\n",
        "            # Agregamos una layer con los puntos, que representan las ubicaciones de las ubicaciones de los centros de estudio:\n",
        "            Map.addLayer(ee_universities, {'color': 'red', 'opacity': 0.9, 'point_size': 2}, 'University Locations')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map) # A veces hay que usar display para mostrar el mapa\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: '{GEOMETRY_COL}' no existe en el CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtOPK1XQDVp"
      },
      "source": [
        "Al igual que en la tarea 2, usamos el NDBI.\n",
        "(Sitio web de interés: https://www.gisandbeers.com/calculo-indice-ndbi-analisis-urbanisticos/)\n",
        "\n",
        "Revisando el estudio de\n",
        "https://revistas.uptc.edu.co/index.php/ingenieria_sogamoso/article/view/15018/12232 Me inclino a considerar al NDBI como un buen índice espectral para utilizar. Aunque se concluye del estudio en Colombia que, si bien éste índice es el que mostró mejor resultados en las zonas mencionadas ahí, no es una regla general.\n",
        "\n",
        "El NDBI (Normalized Difference Built-up Index) es un índice espectral diseñado para resaltar las zonas construidas, de ahí el nombre de las siglas. Se basa en el cálculo mediante la fórmula presente en el sitio web de interés, como también en el estudio, ambos enlaces más arriba, la fórmula se basa en el contraste entre la reflectancia del infrarrojo de onda corta (SWIR) y del infrarrojo cercano (NIR), que en el caso de nuestro dataset utilizado (Sentinel-2), equivalen a las bandas B11 y B8, respectivamente. Si bien para el caso de una representación visual corriente representamos las capas (layers) en un mapa con las bandas RGB, que equivalen a las bandas B4, B3 y B2 respectivamente, en este caso utilizamos estas otras bandas para poder estudiar de mejor manera las zonas construidas o edificaciones. Diferentes datasets provienen de diferentes satélites con diferentes sensores, así que no todos los satélites trabajan con las mismas bandas, por lo que en imágenes muy antiguas puede ser que no estén disponbles las bandas de los datasets actuales, como las del Sentinel-2.\n",
        "\n",
        "Fórmulas:\n",
        "\n",
        "NDBI = (SWIR - NIR) / (SWIR + NIR)\n",
        "\n",
        "Misma fórmula, pero reemplazando las radiaciones electromagnéticas SWIR y NIR por sus equivalentes en bandas para el Sentinel-2:\n",
        "\n",
        "NDBI = (Banda 11 - Banda 8) / (Banda 11 + Banda 8)\n",
        "\n",
        "Las equivalencias entre SWIR y NIR con las bandas 11 y 8 fueron obtenidas de https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED#bands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsTjOEHlQDmt"
      },
      "outputs": [],
      "source": [
        "Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "#Pasamos un GDF a FeatureCollection de GEE, para poder utilizarlo en GEE posteriormente:\n",
        "#ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "\n",
        "# Esto es boilerplate de la tarea 2, código genérico para calcular el NDBI:\n",
        "def getNDBI(image):\n",
        "    # Calculamos el NDBI con las Bandas B11 (SWIR 1) y B8 (NIR). Formula: (SWIR - NIR) / (SWIR + NIR)\n",
        "    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
        "    return ndbi\n",
        "\n",
        "s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "    .filterBounds(manhattan_ee) \\\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con nubosidad menor al 20%\n",
        "\n",
        "if s2_collection.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s2_image_median = s2_collection.median().clip(manhattan_ee)\n",
        "\n",
        "    # Calculamos el NDBI para la mediana de las imágnes del dataset, previamente clippeado por el AOI.\n",
        "    ndbi_image = getNDBI(s2_image_median)\n",
        "\n",
        "    # Paleta: de áreas no urbanizadas (ej. verde/azul) a áreas urbanizadas (ej. gris/blanco/morado)\n",
        "    ndbi_vis_params = {\n",
        "        'min': -0.5,\n",
        "        'max': 0.5,\n",
        "        'palette': [\n",
        "            'blue',    # Agua (NDBI muy bajo)\n",
        "            'green',   # Vegetación sana\n",
        "            'yellow',  # Suelo desnudo / vegetación dispersa\n",
        "            'red',     # Transición / áreas urbanizadas menos densas\n",
        "            'white'    # Áreas altamente urbanizadas (NDBI alto)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    s2_rgb_vis_params = {\n",
        "        'bands': ['B4', 'B3', 'B2'],\n",
        "        'min': 0,\n",
        "        'max': 3000,\n",
        "        'gamma': 1.4\n",
        "    }\n",
        "\n",
        "    # Agregamos la layer RGB y la del NDBI para mostrar el mapa\n",
        "    Map.addLayer(s2_image_median, s2_rgb_vis_params, 'Sentinel-2 RGB')\n",
        "    Map.addLayer(ndbi_image, ndbi_vis_params, 'NDBI ')\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado im[agenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
