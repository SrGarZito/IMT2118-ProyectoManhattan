{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hemos optado por sólo codificar en inglés, pero las explicaciones y comentarios del código estarán en español."
      ],
      "metadata": {
        "id": "kaf1reU6AQOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero que nada, instalamos lo necesario e importamos las librerías necesarias. Si algún módulo no se ha importado totalmente, es porque el trabajo está desarrollado en Google Colab: por el entorno, a veces no es necesario importar todo."
      ],
      "metadata": {
        "id": "Pv8oEM-z3l_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osmnx networkx geopandas pandas shapely pyproj"
      ],
      "metadata": {
        "id": "qyVS1dwR3VC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install esda splot"
      ],
      "metadata": {
        "id": "qUy2Mjqr3Oup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"h3<4\"\n",
        "!pip install h3"
      ],
      "metadata": {
        "id": "0E_-lo4-3Ghx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium"
      ],
      "metadata": {
        "id": "SqiXiwk62_ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ykJkfc8U0p3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos todas las librerías a utilizar:\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm # Para barras de progreso, ayuda mucho ya que la API tiene delay de 1s por cada request!!\n",
        "from shapely.geometry import Point, Polygon\n",
        "from shapely import wkt # Lo usaremos para parsear algunas columnas de algunos datasets a geometry, en un GDF.\n",
        "import geemap,ee\n",
        "import json\n",
        "\n",
        "import h3\n",
        "import numpy as np\n",
        "import folium\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "import libpysal.weights\n",
        "from esda.moran import Moran\n",
        "from splot.esda import plot_moran\n",
        "\n",
        "import osmnx as ox\n",
        "from tqdm.auto import tqdm\n",
        "import networkx as nx\n",
        "\n",
        "from branca.colormap import linear # Para la paleta de colores\n",
        "\n",
        "import os # Para compatibilidad del SO, unión de rutas con path.join"
      ],
      "metadata": {
        "id": "7s8kWpVR3jEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-aesmatias')\n",
        "# Aqui, por google colab no tuve que usar un token, pero se requiere autenticacion."
      ],
      "metadata": {
        "id": "qYl6tJA4PEbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto fuerza a que se actualicen los directorios y sus contenidos en Google Colab\n",
        "!ls shapefiles\n",
        "!ls datasets"
      ],
      "metadata": {
        "id": "lX4_dJ46bfm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos el Shapefile de USA desde https://gadm.org/download_country.html, eligiendo United States y descomprimiendo el .zip, eso nos dará los archivos necesarios, luego cargamos el Shapefile de USA y filtramos el AOI en Manhattan, para finalmente de transformarlo a GEOJSON y poder utilizarlo en GEE:"
      ],
      "metadata": {
        "id": "-wqT6Q5pQGd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nivel 2 para elegir los condados, luego lo pasamos a EPSG:4326, compatible con GEE\n",
        "os_current_path = os.path.join(\".\", \"shapefiles\", \"gadm41_USA_2.shp\")\n",
        "gdf_USA = gpd.read_file(os_current_path).to_crs(epsg=4326)\n",
        "\n",
        "# Ruta al archivo de salida\n",
        "output_dir = os.path.join(\".\", \"shapefiles\")\n",
        "output_path = os.path.join(output_dir, \"manhattan.geojson\")\n",
        "\n",
        "# Creamos el directorio si no existe\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "ny_TO_GDF = gdf_USA[gdf_USA['NAME_2'] == 'New York'] # Seleccionamos New York\n",
        "ny_TO_GDF.to_file(output_path, driver=\"GeoJSON\") # Hacemos un .geojson y lo guardamos"
      ],
      "metadata": {
        "id": "664WxEU8QKRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el geojson creado, para que GEE lo pueda utilizar\n",
        "os_current_path = os.path.join(\".\", \"shapefiles\", \"manhattan.geojson\")\n",
        "gdf = gpd.read_file(os_current_path)\n",
        "manhattan_geojson = json.loads(gdf.to_json())\n",
        "manhattan_ee = ee.FeatureCollection(manhattan_geojson)"
      ],
      "metadata": {
        "id": "7dF4zmYQSSLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos manhattan con GEE, podemos ajustar los parámetros como la opacidad del AOI en el mapa interactivo:"
      ],
      "metadata": {
        "id": "JvfjXaAHUpXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manhattanCollection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") # Mayo 2024 - Abril 2025\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con menos de 20% de nubes\n",
        "    .median() # Usamos la mediana de las imágenes de momento, sólo queremos apreciar el mapa\n",
        "    .clip(manhattan_ee)) # Recortamos en Manhattan, la AOI\n",
        "\n",
        "Map = geemap.Map(center=[40.783, -73.971], zoom=12)\n",
        "\n",
        "Map.addLayer(manhattanCollection, {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0,\n",
        "    'max': 3000\n",
        "}, 'RGB')\n",
        "\n",
        "Map.addLayer(manhattan_ee.style(**{\n",
        "    'width': 1,\n",
        "    'color': 'red', # El borde será CYAN\n",
        "    #'fillColor': '00000000',  # Color transparente de relleno\n",
        "}), {}, 'AOI Manhattan')\n",
        "Map"
      ],
      "metadata": {
        "id": "TpxhsKuLRQYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se ha utilizado el dataset de Manhattan, obtenido en https://www.nyc.gov/site/finance/property/property-rolling-sales-data.page, la fecha de los registros del dataset está entre Mayo 2024 - Abril 2025."
      ],
      "metadata": {
        "id": "p6oi1iEb_D6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, en las celdas iniciales que siguen, se muestra como se han geocodificado algunos datos con ayuda de una API, pero no hace falta usarlo, porque los datos procesados ya han sido guardados en un .csv (rollingsales_manhattan_geocoded.csv)"
      ],
      "metadata": {
        "id": "LM8Dd5KBLAhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''La API que usamos para geocodificar en su capa gratuita es para testing, este proyecto\n",
        "Universitario no califica como un proyecto de producción, no hay usuario final que lo utilizará.\n",
        "Además, sólo hicimos uso de la API en pocas ocasiones'''\n",
        "\n",
        "OPENCAGE_API_KEY = 'KEY'\n",
        "OPENCAGE_BASE_URL = 'https://api.opencagedata.com/geocode/v1/json'\n",
        "\n",
        "# Ruta al archivo de salida\n",
        "output_dir = os.path.join(\".\", \"datasets\")\n",
        "\n",
        "# Creamos el directorio si no existe\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "input_excel_file = os.path.join(output_dir, \"rollingsales_manhattan.xlsx\") # XLSX con rolling sales\n",
        "\n",
        "# Output y failed logs, para poder tener una reanudación cada 2500 request en la geocodificación\n",
        "output_csv_file = os.path.join(output_dir, \"rollingsales_manhattan_geocoded.csv\")\n",
        "failed_addresses_file = output_csv_file = os.path.join(output_dir, \"rollingsales_manhattan_geocoding_failures.csv\")\n",
        "\n",
        "MAX_DAILY_REQUESTS = 2500\n",
        "REQUEST_DELAY = 1.0 # seconds\n",
        "\n",
        "REQUIRED_COLUMNS = ['ADDRESS', 'BOROUGH', 'ZIP CODE', 'SALE PRICE'] # Cols necesarias del dataset"
      ],
      "metadata": {
        "id": "zYgztuOHI0EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la función que procesará cada solicitud a la API para poder geocodificar las direcciones y los ZIP codes en coordenadas geográficas que usaremos en los GDF posteriormente:"
      ],
      "metadata": {
        "id": "wHgsut25JEsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función hace un llamado a la API, y retorna un array con la latitud, longitud y estado\n",
        "def geocode_address(address_str, api_key, borough=None, zip_code=None):\n",
        "    query = f\"{address_str}, {borough}, New York, NY {zip_code}\" if borough and zip_code else address_str\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'key': api_key,\n",
        "        'language': 'en',\n",
        "        'no_annotations': 1,\n",
        "        'limit': 1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(OPENCAGE_BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data and data['results']:\n",
        "            lat = data['results'][0]['geometry']['lat']\n",
        "            lng = data['results'][0]['geometry']['lng']\n",
        "\n",
        "            components = data['results'][0].get('components', {})\n",
        "            is_nyc = False # Empieza como false, y si lo encontramos, lo cambiamos a True:\n",
        "            if 'state_code' in components and components['state_code'] == 'NY':\n",
        "                if 'city' in components and components['city'] in ['New York', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', 'Manhattan']:\n",
        "                    is_nyc = True\n",
        "                elif 'county' in components and ('New York County' in components['county'] or \\\n",
        "                                                 'Kings County' in components['county'] or \\\n",
        "                                                 'Queens County' in components['county'] or \\\n",
        "                                                 'Bronx County' in components['county'] or \\\n",
        "                                                 'Richmond County' in components['county']):\n",
        "                    is_nyc = True\n",
        "\n",
        "            if is_nyc:\n",
        "                return lat, lng, \"Success\"\n",
        "            else:\n",
        "                return None, None, \"Not_NYC_Result\"\n",
        "        else:\n",
        "            return None, None, \"No_Results\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if response.status_code == 429:\n",
        "            return None, None, \"Rate_Limit_Exceeded\"\n",
        "        elif response.status_code == 401:\n",
        "            print(f\"API Key Error!!\")\n",
        "            return None, None, \"API_Key_Error\"\n",
        "        elif response.status_code == 402: # Entonces, llegamos al límite de la quota diaria gratis\n",
        "            print(f\"ERROR, Quota exceded!\")\n",
        "            return None, None, \"Payment_Required_Error\"\n",
        "        else:\n",
        "            print(f\"Request error: {e}\")\n",
        "            return None, None, f\"Request_Error: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None, f\"Error: {e}\""
      ],
      "metadata": {
        "id": "EuDChyt3I0bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos a procesar las direcciones y ZIP codes a coordenadas geográficas:"
      ],
      "metadata": {
        "id": "1yyyEAuxJYMY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGnrrVuQ34uk"
      },
      "outputs": [],
      "source": [
        "print(f\"*** Procesando {input_excel_file} ***\")\n",
        "\n",
        "df = None\n",
        "# Agregamos las columnas requeridas que fallaron a entradas del df fallido, para re intentar si se quisiera:\n",
        "failed_df = pd.DataFrame(columns=REQUIRED_COLUMNS + ['Reason'])\n",
        "\n",
        "try:\n",
        "    if pd.io.common.file_exists(output_csv_file):\n",
        "        print(f\"Fichero '{output_csv_file}' con progreso encontrado - Resumiendo...\")\n",
        "        df = pd.read_csv(output_csv_file)\n",
        "        # Si no hay latitud, longitud o estado de la geocodificación, sabemos que la entrada no ha sido procesada:\n",
        "        for col in ['LATITUDE', 'LONGITUDE', 'GEOCODING_STATUS']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "            df[col] = df[col].astype(object)\n",
        "\n",
        "    else:\n",
        "        print(f\"No se encontró el fichero con progreso, cargando el fichero inicial XLSX: '{input_excel_file}'.\")\n",
        "        found_header = False\n",
        "        # Probamos con headers desde el 0 al 10, porque algunos datasets XLSX tienen las primeras entradas con información,\n",
        "        # hay que evitar las primeras entradas que no son los headers, para no obtener errores:\n",
        "        for header_row_index in range(10):\n",
        "            try:\n",
        "                print(f\"Intentando cargar con el header {header_row_index}\")\n",
        "                df_temp = pd.read_excel(input_excel_file, header=header_row_index)\n",
        "\n",
        "                if all(col in df_temp.columns for col in REQUIRED_COLUMNS):\n",
        "                    df = df_temp\n",
        "                    found_header = True\n",
        "                    print(f\"Header encontrado en el índice {header_row_index}!\")\n",
        "                    print(\"Columnass encontradas en el DF:\", df.columns.tolist())\n",
        "                    break # Si el header es encontrado, dejamos de loopear\n",
        "                else:\n",
        "                    print(f\"No hay header en el índice {header_row_index}, probando el siguiente...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "        if not found_header: # Si no hay header, hay un error en el fichero\n",
        "            print(f\"No se ha encontrado un header, error en el fichero XLSX\")\n",
        "            exit()\n",
        "\n",
        "        # Limpiamos las columnas del DF y las parseamos\n",
        "        df['ADDRESS'] = df['ADDRESS'].fillna('').astype(str)\n",
        "        df['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].fillna('').astype(str)\n",
        "        df['BOROUGH'] = df['BOROUGH'].fillna('').astype(str)\n",
        "        df['ZIP CODE'] = df['ZIP CODE'].fillna(0).astype(int).astype(str).replace('0', '')\n",
        "\n",
        "        # Agregamos la latitud, longitud, y estado de la geocodificación, como nuevas columnas en el DF:\n",
        "        df['LATITUDE'] = None\n",
        "        df['LONGITUDE'] = None\n",
        "        df['GEOCODING_STATUS'] = None\n",
        "\n",
        "    already_geocoded_count = df['LATITUDE'].notna().sum()\n",
        "    requests_made_now = 0\n",
        "    print(f\"Los registros geocodificados hasta el momento son: {already_geocoded_count}\")\n",
        "\n",
        "    # La siguiente línea veririfica si la latitud está vacía y el \"status\" es diferente de \"Not_NYC_Result\"\n",
        "    # Si el registro tiene GEOCODIG_STATUS = 'Not_NYC_Result', entonces ha fallado la geocodificación.\n",
        "    rows_to_geocode = df[(df['LATITUDE'].isna()) & (df['GEOCODING_STATUS'] != 'Not_NYC_Result')]\n",
        "\n",
        "    print(f\"Han fallado: {len(rows_to_geocode)} registros\")\n",
        "\n",
        "    for index, row in tqdm(rows_to_geocode.iterrows(), total=len(rows_to_geocode), desc=\"Geocoding\"): #tqdm para barra de progrso\n",
        "        if requests_made_now >= MAX_DAILY_REQUESTS:\n",
        "            print(f\"Se ha llegado al límite de {MAX_DAILY_REQUESTS} requests diarias!\")\n",
        "            break\n",
        "\n",
        "        address = row['ADDRESS']\n",
        "        borough = row['BOROUGH']\n",
        "        zip_code = row['ZIP CODE'] if row['ZIP CODE'] != '0' else ''\n",
        "\n",
        "        if not address:\n",
        "            df.loc[index, 'GEOCODING_STATUS'] = \"Empty_Address\"\n",
        "            continue\n",
        "\n",
        "        lat, lon, status = geocode_address(address, OPENCAGE_API_KEY, borough, zip_code)\n",
        "        requests_made_now += 1\n",
        "\n",
        "        df.loc[index, 'LATITUDE'] = lat\n",
        "        df.loc[index, 'LONGITUDE'] = lon\n",
        "        df.loc[index, 'GEOCODING_STATUS'] = status\n",
        "\n",
        "        if status in [\"Rate_Limit_Exceeded\", \"API_Key_Error\", \"Payment_Required_Error\"]:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY) # La documentación de la API indica un delay de 1 segundo entre cada request\n",
        "\n",
        "        # Guardamos el progreso en chunks de cada 100 solicitudes a la API:\n",
        "        if requests_made_now % 100 == 0:\n",
        "            print(f\"Guardando progreso en la request número {requests_made_now}\")\n",
        "            df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Geocodificación finalizada!!\")\n",
        "\n",
        "    failed_rows = df[df['LATITUDE'].isna()] # Si no tiene latitud, es una row fallida\n",
        "    if not failed_rows.empty:\n",
        "        failed_df_to_save = failed_rows[['BOROUGH', 'NEIGHBORHOOD', 'ADDRESS', 'ZIP CODE', 'GEOCODING_STATUS']].copy()\n",
        "        # Guardamos en failed_addresses_file sólo las rows de la variable de arriba, para poder procesarlas luego.\n",
        "        failed_df_to_save.to_csv(failed_addresses_file, index=False)\n",
        "        print(f\"Los registros fallidos se guardaron en: {failed_addresses_file}\")\n",
        "    else:\n",
        "        print(\"Atención! Finalización inesperada, posiblemente ha ocurrido un error.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Archivo '{input_excel_file}' no hallado.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el fichero con la latitud y longitud agregadas en la geocodificación, para luego, filtrar los registros que tengan NaN en latitud y longitud, ya que eso es producto de errores en la geocodificación de dichos valores. También cribamos y sólo tomamos como válidos valores con precio de venta mayor a 0, ya que hay varios valores en el dataset con propiedades que se han vendido a costo 0, lo que no tiene representación estadística en nuestro contexto."
      ],
      "metadata": {
        "id": "WHyojXTMXc6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "properties_geocoded_file = os.path.join(\".\", \"datasets\", \"rollingsales_manhattan_geocoded.csv\")\n",
        "\n",
        "try:\n",
        "    df_properties = pd.read_csv(properties_geocoded_file)\n",
        "    print(f\"***Cargando fichero: {properties_geocoded_file}*** \\n\")\n",
        "    print(f\"Cantidad de registros encontrados en {str(properties_geocoded_file)}: {len(df_properties)} \\n\")\n",
        "    print(\"df_properties.info(): \\n\")\n",
        "    print(df_properties.info())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminamos los registros que sean NaN en latitud y longitud.\n",
        "df_properties.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
        "print(f\"Registros después de eliminar NaN en lat/lon: {len(df_properties)}\")\n",
        "\n",
        "# Transformamos todo SALE PRICE a numeric, para luego, con coerce, reemplazar los valores no numericos a NaN\n",
        "df_properties['SALE PRICE'] = pd.to_numeric(df_properties['SALE PRICE'], errors='coerce')\n",
        "# Luego,eliminamos todas esas filas que contienen NaN\n",
        "df_properties.dropna(subset=['SALE PRICE'], inplace=True)\n",
        "print(f\"Registros después de filtrar por NaN: {len(df_properties)}\")\n",
        "\n",
        "# Eliminamos los registros que tengan precio de venta menor o igual a 0, sin representación estadística.\n",
        "df_properties = df_properties[df_properties['SALE PRICE'] > 0]\n",
        "print(f\"Registros después de filtrar por precio de venta > 0: {len(df_properties)}\")\n",
        "\n",
        "# Filtramos duplicados\n",
        "initial_rows_before_deduplication = len(df_properties)\n",
        "df_properties.drop_duplicates(inplace=True)\n",
        "print(f\"Registros después de filtrar por posibles duplicados: {len(df_properties)}\")"
      ],
      "metadata": {
        "id": "qxJdB0e94hCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, para una posterior manipulación y tener una mayor compatibilidad con GEE y librerías para graficar, \"transformamos\" el dataframe a un geodataframe, y le agregamos una nueva columna de tipo geometry, que contendrá puntos generados a través de las propiedades de latitud y longitud, los cuales están definidos en la variable geometry, haciendo uso del métetodo zip y list comprehension."
      ],
      "metadata": {
        "id": "s1kLfLf6cMl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A partir de las coordenadas, creamos objetos de tipo Point, de la librería shapely.geometry, para luego manipular mejor:\n",
        "geometry = [Point(xy) for xy in zip(df_properties['LONGITUDE'], df_properties['LATITUDE'])]\n",
        "\n",
        "# Creamos el nuevo geodataframe para, a partir del dataframe anterior, llenarlo con los datos:\n",
        "gdf_properties = gpd.GeoDataFrame(df_properties, geometry=geometry, crs=\"EPSG:4326\") #CRS es EPSG:4326 para latitud y longitud.\n",
        "\n",
        "print(\"Nuevo GeoDataFrame:\")\n",
        "gdf_properties.info()"
      ],
      "metadata": {
        "id": "mSAhuFKN4lFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el dataset https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic que contiene el histórico de tiroteos."
      ],
      "metadata": {
        "id": "HgWEfpVligZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shooting_incident_historic = os.path.join(output_dir, \"NYPD_Shooting_Incident_Data__Historic_.csv\")\n",
        "\n",
        "df_shooting_incident_historic = None\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado \\n\")\n",
        "    print(f'Información del fichero: \\n')\n",
        "    df_shooting_incident_historic.info()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "ZT1--5td5E3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Al igual que antes, este dataset también debe ser filtrado por fecha de interés y ser convertido a GDF, la columna con el Point, que contiene la geometría de la latitud y longitud, está en una columna llamada \"Lon_Lat\", y contiene valores de tipo Point."
      ],
      "metadata": {
        "id": "gjdic-wQqglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LATITUDE_COL = 'Latitude'\n",
        "LONGITUDE_COL = 'Longitude'\n",
        "INCIDENT_DATE_COL = 'OCCUR_DATE'\n",
        "BORO_COL = 'BORO'\n",
        "year_of_preference = 2024 # Nos interesan datos de tiroteos en los últimos 2 años\n",
        "\n",
        "# Cargamos el archivo y definimos una variable para su dataframe\n",
        "shooting_incident_historic = os.path.join(\".\", \"datasets\", \"NYPD_Shooting_Incident_Data__Historic_.csv\")\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado. Su longitud es: {len(df_shooting_incident_historic)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Eliminamos las filas con NaN en las coordenadas\n",
        "df_shooting_incident_historic.dropna(subset=[LATITUDE_COL, LONGITUDE_COL], inplace=True)\n",
        "\n",
        "# Eliminamos las filas duplicadas\n",
        "df_shooting_incident_historic.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_shooting_incident_historic)}\")\n",
        "\n",
        "# Si la columna con fecha del incidente existe, filtramos por BORO y AÑO:\n",
        "if INCIDENT_DATE_COL in df_shooting_incident_historic.columns:\n",
        "    # Convertimos la columna de OCCUR_DATE a datatime de pandas, para trabajarla con el formato de USA:\n",
        "    df_shooting_incident_historic[INCIDENT_DATE_COL] = pd.to_datetime(\n",
        "        df_shooting_incident_historic[INCIDENT_DATE_COL],\n",
        "        format='%m/%d/%Y',\n",
        "        errors='coerce' # Los valores sin fecha válida serán NaT\n",
        "    )\n",
        "    # Dropeamos valores sin fecha válida (NaT)\n",
        "    df_shooting_incident_historic.dropna(subset=[INCIDENT_DATE_COL], inplace=True)\n",
        "    df_shooting_incident_historic['INCIDENT_YEAR'] = df_shooting_incident_historic[INCIDENT_DATE_COL].dt.year\n",
        "\n",
        "    # Filtramos para sólo obtener datos de Manhattan (en la col BORO)\n",
        "    if BORO_COL in df_shooting_incident_historic.columns:\n",
        "        # Usamos .copy() para definir la variable por valor, y no por referencia en memoria:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_historic[df_shooting_incident_historic[BORO_COL] == 'MANHATTAN'].copy()\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes en Manhattan\")\n",
        "\n",
        "        # Filtramos según el año deseado:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_manhattan[df_shooting_incident_manhattan['INCIDENT_YEAR'] >= year_of_preference]\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes posteriores al año {year_of_preference}. \\n\")\n",
        "\n",
        "        # Antes de convertir el DF a GDF, necesitamos col geometry que contiene puntos, los cuales están en la\n",
        "        # columna Lon_Lat, por lo que la parseamos:\n",
        "        df_shooting_incident_manhattan['Lon_Lat'] = df_shooting_incident_manhattan['Lon_Lat'].apply(wkt.loads)\n",
        "\n",
        "        # Creamos el GeoDataFrame de los incidentes y lo asignamos en una nueva variable:\n",
        "        geometry_incidents = [Point(xy) for xy in zip(df_shooting_incident_manhattan[LONGITUDE_COL], df_shooting_incident_manhattan[LATITUDE_COL])]\n",
        "\n",
        "        # Renombramos la columna Lon_Lat a geometry, ya que sabemos que existe Lon_Lat, que es un tipo de dato Point\n",
        "        df_shooting_incident_manhattan.rename(columns={'Lon_Lat': 'geometry'}, inplace=True)\n",
        "\n",
        "        # Creamos el GDF utilizando la col geometry:\n",
        "        gdf_incidents = gpd.GeoDataFrame(df_shooting_incident_manhattan, geometry='geometry', crs=\"EPSG:4326\")  # CRS 4326 para lat/lon\n",
        "\n",
        "        print(\"Información del GDF:\")\n",
        "        gdf_incidents.info()\n",
        "    else:\n",
        "      raise KeyError(f\"Error: La columna '{BORO_COL}' no se encontró!! \")"
      ],
      "metadata": {
        "id": "6RFXeYKI5Spj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos y limpiamos el dataset de universidades y college. Es importante notar que hay puntos por fuera de la AOI de Manhattan en algunos datasets, como por ejemplo en la universidades, esto es porque es del todo natural que hayan personas viviendo en el distrito de Manhattan que quieran estudiar en las cercanías del distrito próximo, por eso no se ha limitado el conjunto de datos de éstos datasets, tiene mucho sentido dejarlo como está."
      ],
      "metadata": {
        "id": "oVzp_4QLGPD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la ruta del dataset\n",
        "college_university_file = os.path.join(\".\", \"datasets\", \"COLLEGE_UNIVERSITY_20250609.csv\")\n",
        "df_college_university = None\n",
        "\n",
        "# Esta propiedad en el dataset es un Punto siempre, según la página que lo provee:\n",
        "GEOMETRY_COL = 'the_geom'\n",
        "\n",
        "try:\n",
        "    df_college_university = pd.read_csv(college_university_file)\n",
        "    print(f\"El archivo {college_university_file} ha sido cargado. Su longitud es: {len(df_college_university)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "#Limpieza de nulos:\n",
        "df_college_university.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_college_university)}\")\n",
        "\n",
        "#Limpieza de duplicados\n",
        "df_college_university.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_college_university)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_college_university.columns:\n",
        "    #Parseamos los datos de la columna the_geom y los agregamos a la llamada 'geometry' para poder trabajarlas:\n",
        "    df_college_university['geometry'] = df_college_university[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_college_university.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_college_university)}\")\n",
        "\n",
        "    gdf_universities = gpd.GeoDataFrame(df_college_university, geometry='geometry', crs=\"EPSG:4326\") #Aplicamos la proyección correcta\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Universidades:\")\n",
        "    gdf_universities.info()\n",
        "\n",
        "    # Mostramos el mapa\n",
        "    Map = geemap.Map(center=[40.7, -74.0], zoom=12)\n",
        "    ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "    try:\n",
        "\n",
        "            # Cargamos el dataset Sentinel-2 L2A (surface reflectance) para mostrar las bandas RGB, y filtramos por AOI y fecha: Mayo 2024 - Abril 2025\n",
        "            s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "                .filterBounds(manhattan_ee) \\\n",
        "                .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con porcentaje de nubes menor a 20\n",
        "\n",
        "            # Verificamos que la colección no esté vacía para el intervalo de fechas solicitada:\n",
        "            if s2_collection.size().getInfo() > 0:\n",
        "                s2_image = s2_collection.median().clip(manhattan_ee) # Calculamos la mediana de imágenes y recortamos al AOI\n",
        "\n",
        "                s2_vis_params = {\n",
        "                    'bands': ['B4', 'B3', 'B2'], # B4=Red, B3=Green, B2=Blue\n",
        "                    'min': 0,\n",
        "                    'max': 3000,\n",
        "                    'gamma': 1.4\n",
        "                }\n",
        "\n",
        "                Map.addLayer(s2_image, s2_vis_params, 'RGB') # Agregamos la layer al mapa\n",
        "            else:\n",
        "                print(\"El dataset no tiene imágnes para la fecha de interés.\")\n",
        "\n",
        "            # Agregamos una layer con los puntos, que representan las ubicaciones de las ubicaciones de los centros de estudio:\n",
        "            Map.addLayer(ee_universities, {'color': 'red', 'opacity': 0.9, 'point_size': 2}, 'University Locations')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map) # A veces hay que usar display para mostrar el mapa\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: '{GEOMETRY_COL}' no existe en el CSV.\")"
      ],
      "metadata": {
        "id": "yD26vj-n_g5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos y limpiamos el dataset de hospitales en NYC."
      ],
      "metadata": {
        "id": "pPyVE1R5sifw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hospital_filename = os.path.join(\".\", \"datasets\", \"hospital_20250704.csv\")\n",
        "df_hospitals = None\n",
        "\n",
        "# Según el dataset, los puntos de ubicación están en el atributo llamado \"Location 1\", luego lo agregaremos como geometry:\n",
        "GEOMETRY_COL = 'Location 1'\n",
        "\n",
        "try:\n",
        "    df_hospitals = pd.read_csv(hospital_filename)\n",
        "    print(f\"El archivo {hospital_filename} ha sido cargado. Su longitud es: {len(df_hospitals)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Limpieza de nulos en la columna de geometría:\n",
        "df_hospitals.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_hospitals)}\")\n",
        "\n",
        "# Limpieza de duplicados\n",
        "df_hospitals.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_hospitals)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_hospitals.columns:\n",
        "    # Parseamos los datos de la columna de geometría y los agregamos a la nueva columna 'geometry'\n",
        "    df_hospitals['geometry'] = df_hospitals[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_hospitals.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_hospitals)}\")\n",
        "\n",
        "    # Creamos el GeoDataFrame con la proyección correcta (EPSG:4326)\n",
        "    gdf_hospitals = gpd.GeoDataFrame(df_hospitals, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Hospitales:\")\n",
        "    gdf_hospitals.info()\n",
        "\n",
        "    Map = geemap.Map(center=[40.76, -73.98], zoom=12) # Centrado en Manhattan\n",
        "\n",
        "    # Convertimos el GeoDataFrame a un Earth Engine FeatureCollection\n",
        "    ee_hospitals = geemap.geopandas_to_ee(gdf_hospitals)\n",
        "\n",
        "    try:\n",
        "        # Cargamos el dataset Sentinel-2 L2A y filtramos por AOI y fecha: Mayo 2024 - Abril 2025\n",
        "        s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "            .filterBounds(manhattan_ee) \\\n",
        "            .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con < 20% de nubes\n",
        "\n",
        "        # Verificamos que la colección no esté vacía\n",
        "        if s2_collection.size().getInfo() > 0:\n",
        "            s2_image = s2_collection.median().clip(manhattan_ee) # Calculamos la mediana y recortamos al AOI\n",
        "\n",
        "            s2_vis_params = {\n",
        "                'bands': ['B4', 'B3', 'B2'], # R, G, B\n",
        "                'min': 0,\n",
        "                'max': 3000,\n",
        "                'gamma': 1.4\n",
        "            }\n",
        "\n",
        "            Map.addLayer(s2_image, s2_vis_params, 'RGB') # Agregamos la capa de imagen satelital\n",
        "        else:\n",
        "            print(\"El dataset no tiene imágenes para la fecha de interés.\")\n",
        "\n",
        "        # Agregamos la capa con los puntos de los hospitales\n",
        "        Map.addLayer(ee_hospitals, {'color': 'blue', 'opacity': 0.9, 'point_size': 2}, 'Hospital Locations')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map)\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: La columna '{GEOMETRY_COL}' no existe en el archivo CSV.\")"
      ],
      "metadata": {
        "id": "kaAskNc0slCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos y limpiamos el dataset de las entradas al metro de NYC."
      ],
      "metadata": {
        "id": "p7G9uoqLwiGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subway_filename = os.path.join(\".\", \"datasets\", \"MTA_Subway_Entrances_and_Exits__2024.csv\")\n",
        "df_subway = None\n",
        "\n",
        "# Según el dataset, los puntos de ubicación están en el atributo llamado \"entrance_georeference\", luego lo agregaremos como geometry:\n",
        "GEOMETRY_COL = 'entrance_georeference'\n",
        "\n",
        "try:\n",
        "    df_subway = pd.read_csv(subway_filename)\n",
        "    print(f\"El archivo {subway_filename} ha sido cargado. Su longitud es: {len(df_subway)} \\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Limpieza de nulos en la columna de geometría:\n",
        "df_subway.dropna(subset=[GEOMETRY_COL], inplace=True)\n",
        "print(f\"Registros luego de eliminar NaNs en la columna '{GEOMETRY_COL}': {len(df_subway)}\")\n",
        "\n",
        "# Limpieza de duplicados\n",
        "df_subway.drop_duplicates(inplace=True)\n",
        "print(f\"Registros luego de eliminar posibles duplicados: {len(df_subway)}\")\n",
        "\n",
        "if GEOMETRY_COL in df_subway.columns:\n",
        "    # Parseamos los datos de la columna de geometría y los agregamos a la nueva columna 'geometry'\n",
        "    df_subway['geometry'] = df_subway[GEOMETRY_COL].apply(lambda x: wkt.loads(x) if pd.notna(x) else None)\n",
        "    df_subway.dropna(subset=['geometry'], inplace=True)\n",
        "    print(f\"Registros luego de parsear la col '{GEOMETRY_COL}' a la col geometry: {len(df_subway)}\")\n",
        "\n",
        "    # Creamos el GeoDataFrame con la proyección correcta (WGS84)\n",
        "    gdf_subway = gpd.GeoDataFrame(df_subway, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "    print(\"\\nInformación del GeoDataFrame de Entradas de Subterráneo:\")\n",
        "    gdf_subway.info()\n",
        "\n",
        "    Map = geemap.Map(center=[40.76, -73.98], zoom=12) # Centrado en Manhattan\n",
        "\n",
        "    # Convertimos el GeoDataFrame a un Earth Engine FeatureCollection\n",
        "    ee_subway = geemap.geopandas_to_ee(gdf_subway)\n",
        "\n",
        "    try:\n",
        "        # Cargamos el dataset Sentinel-2 L2A y filtramos por AOI y fecha\n",
        "        s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "            .filterBounds(manhattan_ee) \\\n",
        "            .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
        "\n",
        "        # Verificamos que la colección no esté vacía\n",
        "        if s2_collection.size().getInfo() > 0:\n",
        "            s2_image = s2_collection.median().clip(manhattan_ee)\n",
        "\n",
        "            s2_vis_params = {\n",
        "                'bands': ['B4', 'B3', 'B2'], # R, G, B\n",
        "                'min': 0,\n",
        "                'max': 3000,\n",
        "                'gamma': 1.4\n",
        "            }\n",
        "            Map.addLayer(s2_image, s2_vis_params, 'RGB')\n",
        "        else:\n",
        "            print(\"El dataset no tiene imágenes para la fecha de interés.\")\n",
        "\n",
        "        # Agregamos la capa con los puntos de las entradas al metro\n",
        "        Map.addLayer(ee_subway, {'color': 'orange', 'pointSize': 2}, 'Subway entrances')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    display(Map)\n",
        "\n",
        "else:\n",
        "    raise KeyError(f\"Error: La columna '{GEOMETRY_COL}' no existe en el archivo CSV.\")"
      ],
      "metadata": {
        "id": "iaf4IHmpwiXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que en la tarea 2, usamos el NDBI.\n",
        "(Sitio web de interés: https://www.gisandbeers.com/calculo-indice-ndbi-analisis-urbanisticos/)\n",
        "\n",
        "Revisando el estudio de\n",
        "https://revistas.uptc.edu.co/index.php/ingenieria_sogamoso/article/view/15018/12232 Me inclino a considerar al NDBI como un buen índice espectral para utilizar. Aunque se concluye del estudio en Colombia que, si bien éste índice es el que mostró mejor resultados en las zonas mencionadas ahí, no es una regla general.\n",
        "\n",
        "El NDBI (Normalized Difference Built-up Index) es un índice espectral diseñado para resaltar las zonas construidas, de ahí el nombre de las siglas. Se basa en el cálculo mediante la fórmula presente en el sitio web de interés, como también en el estudio, ambos enlaces más arriba, la fórmula se basa en el contraste entre la reflectancia del infrarrojo de onda corta (SWIR) y del infrarrojo cercano (NIR), que en el caso de nuestro dataset utilizado (Sentinel-2), equivalen a las bandas B11 y B8, respectivamente. Si bien para el caso de una representación visual corriente representamos las capas (layers) en un mapa con las bandas RGB, que equivalen a las bandas B4, B3 y B2 respectivamente, en este caso utilizamos estas otras bandas para poder estudiar de mejor manera las zonas construidas o edificaciones. Diferentes datasets provienen de diferentes satélites con diferentes sensores, así que no todos los satélites trabajan con las mismas bandas, por lo que en imágenes muy antiguas puede ser que no estén disponbles las bandas de los datasets actuales, como las del Sentinel-2.\n",
        "\n",
        "Fórmulas:\n",
        "\n",
        "NDBI = (SWIR - NIR) / (SWIR + NIR)\n",
        "\n",
        "Misma fórmula, pero reemplazando las radiaciones electromagnéticas SWIR y NIR por sus equivalentes en bandas para el Sentinel-2:\n",
        "\n",
        "NDBI = (Banda 11 - Banda 8) / (Banda 11 + Banda 8)\n",
        "\n",
        "Las equivalencias entre SWIR y NIR con las bandas 11 y 8 fueron obtenidas de https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED#bands"
      ],
      "metadata": {
        "id": "eYtOPK1XQDVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aprovechamos de hacer el mismo cálculo para el NDVI (índice de vegetación).\n",
        "\n",
        "https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index\n",
        "\n",
        "\n",
        "https://www.auravant.com/en/articles/precision-agriculture/vegetation-indices-and-their-interpretation-ndvi-gndvi-msavi2-ndre-and-ndwi/\n",
        "\n",
        "\n",
        "https://mappinggis.com/2015/06/ndvi-que-es-y-como-calcularlo-con-saga-desde-qgis/"
      ],
      "metadata": {
        "id": "h-Sl082n-9g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "#Pasamos un GDF a FeatureCollection de GEE, para poder utilizarlo en GEE posteriormente:\n",
        "#ee_universities = geemap.geopandas_to_ee(gdf_universities)\n",
        "\n",
        "# Esto es boilerplate de la tarea 2, código genérico para calcular el NDBI:\n",
        "def getNDBI(image):\n",
        "    # Calculamos el NDBI con las Bandas B11 (SWIR 1) y B8 (NIR). Formula: (SWIR - NIR) / (SWIR + NIR)\n",
        "    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
        "    return ndbi\n",
        "\n",
        "## NDVI (que corresponde al indice de Vegetación)\n",
        "def getNDVI(image):\n",
        "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    return ndvi\n",
        "\n",
        "s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "    .filterBounds(manhattan_ee) \\\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-30\") \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) # Imágenes con nubosidad menor al 20%\n",
        "\n",
        "if s2_collection.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s2_image_median = s2_collection.median().clip(manhattan_ee)\n",
        "\n",
        "    # Calculamos el NDBI y el NDVI para la mediana de las imágnes del dataset, previamente clippeado por el AOI.\n",
        "    ndbi_image = getNDBI(s2_image_median)\n",
        "    ndvi_image = getNDVI(s2_image_median) ## NUEVO\n",
        "\n",
        "    # Parámetros para NDBI\n",
        "    ndbi_vis_params = {\n",
        "        'min': -0.5, 'max': 0.5,\n",
        "        'palette': [\n",
        "            'blue',    # Agua (NDBI muy bajo)\n",
        "            'green',   # Vegetación sana\n",
        "            'yellow',  # Suelo desnudo / vegetación dispersa\n",
        "            'red',     # Transición / áreas urbanizadas menos densas\n",
        "            'white'    # Áreas altamente urbanizadas (NDBI alto)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Parámetros para NDVI\n",
        "    ndvi_vis_params = {\n",
        "        'min': -0.1, 'max': 0.8, # Hay que cambiar el min y max con respecto al NDBI\n",
        "        # La paleta va desde el suelo sin vegetación (brown) hasta mucha vegetación (darkgreen)\n",
        "        'palette': ['brown', 'yellow', 'lightgreen', 'green', 'darkgreen']\n",
        "    }\n",
        "\n",
        "    # Parámetros para la imagen RGB\n",
        "    s2_rgb_vis_params = {\n",
        "        'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000, 'gamma': 1.4\n",
        "    }\n",
        "\n",
        "    # Agregamos la layer RGB, NDVI y NDBI para mostrar en el mapa\n",
        "    Map.addLayer(s2_image_median, s2_rgb_vis_params, 'RGB')\n",
        "    Map.addLayer(ndbi_image, ndbi_vis_params, 'NDBI - Índice de urbanización')\n",
        "    Map.addLayer(ndvi_image, ndvi_vis_params, 'NDVI - Índice de vegetación')\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado imágenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ],
      "metadata": {
        "id": "tsTjOEHlQDmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, hacemos reproyecciones de los datasets para pasarlos del sistema angular a unidades medidas en pies(ft), para así hacer las mediciones posteriores de distancias más facilmente. Para ello, hemos utilizado el sitio web https://epsg.io/, que nos ayuda a encontrar la proyección adecuada para cada lugar del mundo, en este caso New York City. Usamos, en vez de la la proyeccion EPSG:32118 que mide en metros, la proyecciön 2263 medida en pies, ya que está diseñada específicamente para NYC. Probando con el CRS EPSG:32118 los resultados fueron mucho menos precisos."
      ],
      "metadata": {
        "id": "xDogZMoThlSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crs_meters_nyc = \"EPSG:2263\"\n",
        "\n",
        "print(f\"Haciendo reproyección de los datasets a {crs_meters_nyc}...\")\n",
        "\n",
        "gdf_properties_proj_ft = gdf_properties.to_crs(crs_meters_nyc)\n",
        "gdf_incidents_proj_ft = gdf_incidents.to_crs(crs_meters_nyc)\n",
        "gdf_subway_proj_ft = gdf_subway.to_crs(crs_meters_nyc)\n",
        "gdf_hospitals_proj_ft = gdf_hospitals.to_crs(crs_meters_nyc)"
      ],
      "metadata": {
        "id": "xJhjzS5oo6xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos un agrupamiento (clustering) de los lugares donde han ocurrido tiroteos registrados en el dataset cargado anteriormente (gdf_incidents*). Para ello, usamos el algoritmo DBSCAN.\n",
        "\n",
        "Se debe limpiar el GDF de incidentes con .drop_duplicates() por duplicados, y también para nulos, pero el dataset ya ha sido limpiado al cargarse, por lo que no es necesario, se ha puesto un print para verificar que no hay duplicados.\n",
        "\n",
        "Recordar que la región de agrupamiento epsilon=eps está en ft (foots, pies), no en metros.\n",
        "\n",
        "Es importante considerar que los verdaderos repetidos no necesariamente consisten en las coordenadas repetidas, sino en registros que pueden tener fechas similares, ya que no es para nada imposible que se registren tiroteos en zonas donde ya han ocurrido previamente tiroteos, lo correcto sería asegurarse que no hay repetición temporal sobre la misma zona, idealmente con un epsilon en cada punto para verificar si hay duplicado del mismo registro, pero en nuestra pequeña base de datos de incidentes no es relevante."
      ],
      "metadata": {
        "id": "XZw3DO4JffFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_incidents_proj_ft_cleaned = gdf_incidents_proj_ft.drop_duplicates() # Eliminamos duplicados\n",
        "gdf_incidents_proj_ft_cleaned.reset_index(drop=True) # Reseteamos el índice luego de dropear duplicados.\n",
        "\n",
        "print(f\"Número de filas completamente duplicadas eliminadas: {len(gdf_incidents_proj_ft) - len(gdf_incidents_proj_ft_cleaned)}\")\n",
        "\n",
        "# Iniciamos el algoritmo de clustering DBSCAN para un epsilon de 700ft y un mínimo de 2 incidentes.\n",
        "model = DBSCAN(eps=700, min_samples=2)\n",
        "labels = model.fit_predict(\n",
        "    pd.DataFrame({\n",
        "        'x': gdf_incidents_proj_ft_cleaned.geometry.x,\n",
        "        'y': gdf_incidents_proj_ft_cleaned.geometry.y\n",
        "    })\n",
        ")\n",
        "\n",
        "gdf_incidents_proj_ft_cleaned['label'] = labels\n",
        "\n",
        "set_labels = set(labels) # El set elimina duplicados\n",
        "#Al set de labels, le restamos 1 si es que existen puntos sin cluster (label -1), de lo contrario, no.\n",
        "number_clusters = len(set_labels) - (1 if -1 in labels else 0)\n",
        "\n",
        "list_labels = list(labels) # La lista conserva el orden y no elimina duplicados.\n",
        "number_noises = list_labels.count(-1) # Contamos cantidad de labels que son -1\n",
        "\n",
        "print(f\"Se han hallado {number_clusters} clusters.\") # Los valores sin -1 como label, son clusters\n",
        "print(f\"Numero de puntos ruido: {number_noises}\")"
      ],
      "metadata": {
        "id": "0QkRs09VdKFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de haber aplicado el algoritmo para los puntos y obtenido los labels para cada punto, podemos hacer uso de los puntos que están agrupados en clusters y los que son ruido, para poder visualizarlos."
      ],
      "metadata": {
        "id": "m_Y_5RP6pW4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproyectamos el GDF para usar ángulos (latitud, longitud)\n",
        "gdf_incidents_proj_ft_cleaned_epsg4326 = gdf_incidents_proj_ft_cleaned.to_crs(\"EPSG:4326\")\n",
        "gdf_map_epsg4326 = gdf_incidents_proj_ft_cleaned_epsg4326.copy() # Copiamos para acortar el nombre\n",
        "\n",
        "# Creamos el mapa centrado en NYC para mostrar los puntos de tiroteos sobre Manhattan.\n",
        "mapa = folium.Map(location=[40.73, -73.9], zoom_start=11, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Seleccionamos nuevamente los puntos ruido para poder graficarlos en gris en el folium.\n",
        "noise_points = gdf_map_epsg4326[gdf_map_epsg4326['label'] == -1]\n",
        "\n",
        "for _, row in noise_points.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=2,\n",
        "        color='gray',\n",
        "        fill=True,\n",
        "        fill_color='gray',\n",
        "        fill_opacity=0.5,\n",
        "        popup='Puntos de ruido'\n",
        "    ).add_to(mapa)\n",
        "\n",
        "# Extraemos los labels únicos, excluyendo los que son -1 (ruido)\n",
        "unique_labels = gdf_map_epsg4326['label'].unique()\n",
        "unique_labels = unique_labels[unique_labels != -1]\n",
        "\n",
        "# Creamos una paleta de 20 colores para ver los clusters.\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_labels))\n",
        "\n",
        "# Folium necesita que se parseen los colores a hexadecimal, loopeamos con enumerate y parseamos:\n",
        "label_to_color = {label: mcolors.rgb2hex(colors(i)) for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Hacemos lo mismo que con los puntos de ruido pero con los clusters, los seleccionamos y los graficamos en el folium:\n",
        "cluster_points = gdf_map_epsg4326[gdf_map_epsg4326['label'] != -1]\n",
        "\n",
        "for _, row in cluster_points.iterrows():\n",
        "\n",
        "    # El color dependerá de la label del punto, a cuál cluster pertenece:\n",
        "    color = label_to_color[row['label']]\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=1,\n",
        "        popup=f\"Cluster perteneciente al label {row['label']}\"\n",
        "    ).add_to(mapa)\n",
        "\n",
        "display(mapa)"
      ],
      "metadata": {
        "id": "i-i0LreJdKco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, se ha tomado como referencia esta explicación de Moran:\n",
        "https://pro.arcgis.com/es/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm, además de https://pro.arcgis.com/es/pro-app/latest/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm, junto con la clase online grabada donde se menciona el algoritmo K-means y el DBSCAN, como también HDBSCAN y diferentes variantes de algoritmos de clustering o agrupación de la librería [scikit-learn](https://scikit-learn.org). Parte de este código fue tomado tomando como referencia la tarea 3."
      ],
      "metadata": {
        "id": "oiu2xLkWplEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entonces, para la verificación estadística, usamos los datos del dataset de propiedades; primero que nada, agrupamos el promedio de los precios de venta de las propiedades bajo el 'ZIP CODE', y lo guardamos en median_prices: muchas casas pueden tener el mismo código postal, pero el código postal cambia según cambia la ubicación geográfica, así que podemos agruparlas, pero calculamos el promedio para tener una representación estadística del precio promedio de esa agrupación de propiedades.\n",
        "\n",
        "Luego, creamos un nuevo DF y, con el método .dissolve (https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.dissolve.html), asignamos una geometría a cada entrada que tenga un \"ZIP CODE\" distinto, y lo almacenamos en la variable gdf_zipcodes, hay que tener en cuenta que este nuevo dataset es de tipo GeoDataFrame, ya que contiene la geometría.\n",
        "\n",
        "Finalmente, unimos los dos DF en uno solo, renombrando la columna de \"SALE PRICE\" por \"MEDIAN_SALE_PRICE\" para evitar confusiones, esto lo utilizaremos en la siguiente celda para el análisis I de Moran."
      ],
      "metadata": {
        "id": "7KYzcs_S_Hmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza preventiva, probablemente no es necesario porque ya se hizo limpieza antes.\n",
        "df = gdf_properties.copy()\n",
        "df['SALE PRICE'] = pd.to_numeric(df['SALE PRICE'], errors='coerce')\n",
        "df = df.dropna(subset=['SALE PRICE'])\n",
        "df = df[df['SALE PRICE'] > 0]\n",
        "\n",
        "# Agregamos los datos por código postal para obtener el precio promedio y la geometría\n",
        "median_prices = df.groupby('ZIP CODE')['SALE PRICE'].median()\n",
        "gdf_zipcodes = df[['ZIP CODE', 'geometry']].dissolve(by='ZIP CODE')\n",
        "gdf_zipcodes = gdf_zipcodes.merge(median_prices, on='ZIP CODE').rename(columns={'SALE PRICE': 'MEDIAN_SALE_PRICE'})"
      ],
      "metadata": {
        "id": "ltVPsVBKniCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el análisis I de Moran (global), primero que nada, definimos una matriz de pesos con el algoritmo del vecino más cercano (KNN): https://pysal.org/libpysal/generated/libpysal.weights.KNN.html#libpysal.weights.KNN\n",
        "\n",
        "Lo que queremos es plantear la hipótesis nula, esto es, suponer que hay una distribución aleatoria de puntos y luego comprobar estadísticamente que no es así, para ello usamos el método de Monte Carlo, simulando una distribución de puntos aleatoria miles de veces, para finalmente comparar los resultados obtenidos con nuestra distribución de puntos inicial, la idea es tomar el algoritmo KNN para tomar un promedio de los precios de viviendas agrupadas según el mismo ZIP CODE, para poder demostrar que existe un agrupamiento, donde viviendas con precio similar están cerca de viviendas con precio similar, y no simplemente se distribuyen espacialmente las viviendas con precios de venta aleatorios, sino que el sector importa. Esto es importante porque luego haremos una representación visual de los precios de ventas agrupados en sectores.\n",
        "\n",
        "También se ha consultado: https://pysal.org/libpysal/api.html"
      ],
      "metadata": {
        "id": "zOz4_6-nEwex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el algoritmo del vecino más cercano promediando la distancia a los 4 más cercanos.\n",
        "k_neighbors = 4\n",
        "wq = libpysal.weights.KNN.from_dataframe(gdf_zipcodes, k=k_neighbors)\n",
        "\n",
        "# Con esto, calculamos el promedio de los precios de la matriz de pesos, no la suma total de precios para un mismo ZIP CODE:\n",
        "wq.transform = 'R'\n",
        "\n",
        "# Calculamos la I de Moran Global\n",
        "y = gdf_zipcodes['MEDIAN_SALE_PRICE']\n",
        "moran = Moran(y, wq, permutations=9999) # Hacemos 10mil simulaciones con el método de Monte Carlo.\n",
        "\n",
        "print(f\"Análisis con k = {k_neighbors} vecinos más cercanos: \\n\")\n",
        "print(f\"Valor I de Moran: {moran.I}, Valor P: {moran.p_sim}\")\n",
        "\n",
        "if moran.p_sim < 0.05 and moran.I > 0:\n",
        "    print(\"Existe una autocorrelación estadísticamente positiva entre los precios de las propiedades según su ZIP CODE\")\n",
        "else:\n",
        "    print(\"Los puntos siguen una distribución aleatoria.\")\n",
        "\n",
        "plot_moran(moran, figsize=(10,6))\n",
        "plt.title(f'Gráfico de Moran para k={k_neighbors}')\n",
        "plt.xlabel('Precio promedio de una vivienda')\n",
        "plt.ylabel('Precio promedio de un agrupamiento de viviendas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fZnPJ3o6nkvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entonces, obtenidos resultados estadísticamente representativos, concluimos que nuestra hipótesis nula no es cierta, así que demostramos que hay agrupamiento espacial (clustering) de los puntos con sus atributos de precio, así que el precio no es aleatoriamente distribuido espacialmente en Manhattan."
      ],
      "metadata": {
        "id": "HT30nh_UH0Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, usamos osmnx para poder usar OpenStreetMap, la idea es descargar las vias peatonales de Manhattan, para así calcular la distancia de cada propiedad a la entrada del metro de NYC más cercana, y agregarla al dataset de propiedades como una nueva columna.\n",
        "\n",
        "https://osmnx.readthedocs.io/en/stable/user-reference.html"
      ],
      "metadata": {
        "id": "IMC_YVnILS4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos que los CRS a trabajar estén en latitud/longitud (EPSG:4326):\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "print(f\"CRS de estaciones de metro: {gdf_subway.crs}\")\n",
        "\n",
        "# Usamos network_type=walk para descargar la red peatonal, porque nos interesa llegar al subway caminando.\n",
        "walk_map_NYC_OSMNX = ox.graph_from_place(\"Manhattan, New York City, USA\", network_type='walk')\n",
        "\n",
        "# Creamos los nodos más cercanos entre las propiedades y el mapa:\n",
        "property_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_properties.geometry.x, Y=gdf_properties.geometry.y)\n",
        "subway_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_subway.geometry.x, Y=gdf_subway.geometry.y) # Lo mismo, pero con subway entrances.\n",
        "\n",
        "# Añadimos el nodo de la propiedad a la calle más cercano como un nuevo atributo del GDF de propiedades.\n",
        "gdf_properties['nearest_node'] = property_nodes\n",
        "\n",
        "# Para posteriormente evitar hacer cálculos con nodos repetidos, sacamos sólo los que son únicos:\n",
        "unique_subway_nodes = set(subway_nodes)\n",
        "\n",
        "print(gdf_properties[['ADDRESS', 'SALE PRICE', 'nearest_node']].head())\n",
        "# Las unidades no aparecen, pero no importa porque las reemplazaremos en la siguiente celda."
      ],
      "metadata": {
        "id": "IpkOKFipJFGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, usamos el algoritmo de búsqueda de caminos Dijkstra https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.weighted.multi_source_dijkstra_path_length.html. Al principio habíamos planteado el uso del algoritmo A*, pero éste último realiza la búsqueda entre cada nodo, lo que es computacionalmente muy costoso y tomaba mucho tiempo. El algoritmo Dijkstra, en cambio, recorre, para cada propiedad, los caminos hasta los nodos de una sola vez, así que hace un cálculo más complejo pero lo realiza todo de una sola vez, lo que es más rápido en este caso."
      ],
      "metadata": {
        "id": "r4Q3mRjUPAxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las distancias desde todas las estaciones del metro de NYC hacia sus nodos más cercanos.\n",
        "# Entonces, se almacenarán las todos los nodos de Manhattan con la distancia de cada nodo al metro más cercano:\n",
        "distances_from_subways = nx.multi_source_dijkstra_path_length(walk_map_NYC_OSMNX, sources=unique_subway_nodes, weight='length')\n",
        "\n",
        "# Una vez obtenidas las distancias, hacemos un loop de cada una de ellas con .map y las agregamos a dist_real_subway\n",
        "gdf_properties['dist_real_subway'] = gdf_properties['nearest_node'].map(distances_from_subways)\n",
        "\n",
        "# Reemplazamos dist_real_subway con dist_real_subway en el GDF, y luego guardamos el archivo para cachear.\n",
        "gdf_properties_dist_subway = gdf_properties.copy()\n",
        "gdf_properties_dist_subway['dist_real_subway'] = gdf_properties['dist_real_subway']\n",
        "\n",
        "gdf_properties_dist_subway['dist_real_subway'].fillna(-1, inplace=True) # En caso de errores, rellenamos vacios con -1\n",
        "\n",
        "print(gdf_properties_dist_subway[['ADDRESS', 'SALE PRICE', 'dist_real_subway']].head(), '\\n')\n",
        "\n",
        "output_file = os.path.join(\".\", \"datasets\", \"gdf_rollingsales_with_subway_dist.gpkg\")\n",
        "gdf_properties_dist_subway.to_file(output_file, driver='GPKG', layer='properties')\n",
        "print(f\"Nuevo GDF guardado como '{output_file}'\")"
      ],
      "metadata": {
        "id": "J5ZI00MFJHRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haremos lo mismo con el dataset de hospitales, calcularemos la distancia mínima de cada propiedad al hospital más cercano, y lo agregaremos como columna nueva al dataset."
      ],
      "metadata": {
        "id": "3d-_h4H4YRjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos los CRS de los nuevos datos a trabajar.\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "print(f\"CRS de hospitales: {gdf_hospitals.crs}\")\n",
        "\n",
        "hospital_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_hospitals.geometry.x, Y=gdf_hospitals.geometry.y)\n",
        "\n",
        "# Para posteriormente evitar hacer cálculos con nodos repetidos, sacamos sólo los que son únicos.\n",
        "unique_hospital_nodes = set(hospital_nodes)\n",
        "\n",
        "print(f\"Se calculará la distancia desde cada propiedad a {len(unique_hospital_nodes)} ubicaciones de hospitales únicos.\")"
      ],
      "metadata": {
        "id": "TkhBUVRDYa5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las distancias desde todos los hospitales de NYC hacia sus nodos más cercanos.\n",
        "# El resultado es un diccionario con todos los nodos de Manhattan y la distancia de cada uno al hospital más cercano.\n",
        "distances_from_hospitals = nx.multi_source_dijkstra_path_length(walk_map_NYC_OSMNX, sources=unique_hospital_nodes, weight='length')\n",
        "\n",
        "# Usamos el 'nearest_node' de cada propiedad que ya calculamos para buscar su distancia.\n",
        "gdf_properties['dist_real_hospital'] = gdf_properties['nearest_node'].map(distances_from_hospitals)\n",
        "\n",
        "gdf_properties_dist_hospital = gdf_properties.copy()\n",
        "# Rellenamos vacíos con -1 en caso de errores o nodos aislados sin ruta conectada a las otras!!!\n",
        "gdf_properties_dist_hospital['dist_real_hospital'].fillna(-1, inplace=True)\n",
        "\n",
        "print(\"\\nCálculo de distancias de red a hospitales finalizado.\")\n",
        "print(gdf_properties_dist_hospital[['ADDRESS', 'SALE PRICE', 'dist_real_hospital']].head(), '\\n')\n",
        "\n",
        "output_file_hospitals = os.path.join(\".\", \"datasets\", \"gdf_rollingsales_with_hospital_dist.gpkg\")\n",
        "gdf_properties_dist_hospital.to_file(output_file_hospitals, driver='GPKG', layer='properties')\n",
        "print(f\"Nuevo GDF con distancias a hospitales guardado como '{output_file_hospitals}'\")"
      ],
      "metadata": {
        "id": "3-qBcNekYftT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos otra columna más para las distancias más cercanas a los centros educativos."
      ],
      "metadata": {
        "id": "iPlsH709Y7TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import networkx as nx\n",
        "\n",
        "# Verificamos los CRS de los nuevos datos a trabajar.\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "print(f\"CRS de universidades: {gdf_universities.crs}\")\n",
        "\n",
        "university_nodes = ox.distance.nearest_nodes(walk_map_NYC_OSMNX, X=gdf_universities.geometry.x, Y=gdf_universities.geometry.y)\n",
        "\n",
        "# Para posteriormente evitar hacer cálculos con nodos repetidos, sacamos sólo los que son únicos.\n",
        "unique_university_nodes = set(university_nodes)\n",
        "\n",
        "print(\"\\nNodos de red más cercanos para universidades identificados.\")\n",
        "print(f\"Se calculará la distancia desde cada propiedad a {len(unique_university_nodes)} ubicaciones de universidades únicas.\")"
      ],
      "metadata": {
        "id": "7XNeeFgsZCPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las distancias desde todas las universidades de NYC hacia sus nodos más cercanos.\n",
        "distances_from_universities = nx.multi_source_dijkstra_path_length(walk_map_NYC_OSMNX, sources=unique_university_nodes, weight='length')\n",
        "print(\"Distancias desde cada nodo de la red a la universidad más cercana calculadas.\")\n",
        "\n",
        "# Usamos el 'nearest_node' de cada propiedad para buscar su distancia en el diccionario.\n",
        "gdf_properties['dist_real_university'] = gdf_properties['nearest_node'].map(distances_from_universities)\n",
        "gdf_properties_dist_university = gdf_properties.copy()\n",
        "\n",
        "# Rellenamos vacíos con -1 en caso de errores o nodos aislados sin ruta conectada a las otras!!!\n",
        "gdf_properties_dist_university['dist_real_university'].fillna(-1, inplace=True)\n",
        "\n",
        "print(\"Ya tenemos el cálculo de distancias de red a universidades!\")\n",
        "print(gdf_properties_dist_university[['ADDRESS', 'SALE PRICE', 'dist_real_university']].head(), '\\n')\n",
        "\n",
        "output_file_universities = os.path.join(\".\", \"datasets\", \"gdf_rollingsales_with_university_dist.gpkg\")\n",
        "gdf_properties_dist_university.to_file(output_file_universities, driver='GPKG', layer='properties')\n",
        "print(f\"Nuevo GDF con distancias a universidades guardado como '{output_file_universities}'\")"
      ],
      "metadata": {
        "id": "Mu3jFyFNZDT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, sabemos que tenemos el ndvi calculado anteriormente en la variable ndvi_image, así que lo utilizamos para poder calcular el área en metros cuadrados de vegetación presente en cada pixel. La idea es, teniendo el procesado del raster en un nuevo GDF, unirlo con el gdf_properties, y agregar el área verde en cierto RADIUS_M alrededor de cada propiedad, como una nueva columna del GDF."
      ],
      "metadata": {
        "id": "oSACyliMb1FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RADIUS_M = 1000 # Radio circular en metros.\n",
        "THRESHOLD = 0.3 # Aproximadamente\n",
        "CRS_METERS = \"EPSG:32118\" # CRS Para metros en NYC utilizado en todo el proyecto.\n",
        "\n",
        "#ndvi = s2_image_median.normalizedDifference(['B8', 'B4']).rename('NDVI') # Ya definido arriba!\n",
        "\n",
        "# Obtenemos los puntos de vegetación a partir del raster ndvi calculado\n",
        "vegetation_mask = ndvi_image.gt(THRESHOLD) # ndvi_image está calculado en celdas anteriores.\n",
        "vegetation_points = vegetation_mask\n",
        "\n",
        "vegetation_points = vegetation_mask.updateMask(vegetation_mask).sample(\n",
        "    region=manhattan_ee, # Lo aplicamos solo al area de Manhattan definida al inicio.\n",
        "    scale=10,\n",
        "    geometries=True\n",
        ")\n",
        "\n",
        "# Pasamos todos los puntos a un GDF, para luego poder hacer un join con el GDF de propiedades:\n",
        "gdf_vegetation = geemap.ee_to_gdf(vegetation_points)\n",
        "\n",
        "# Calculamos el área verde para cada propiedad del GDF de propiedades, en un radio de RADIUS_M metros.\n",
        "print(f\"Calculando área verde en un radio de {RADIUS_M}m para cada propiedad...\")\n",
        "properties_proj = gdf_properties.to_crs(CRS_METERS)\n",
        "vegetation_proj = gdf_vegetation.to_crs(CRS_METERS)\n",
        "\n",
        "property_buffers = properties_proj.copy()\n",
        "property_buffers['geometry'] = property_buffers.geometry.buffer(RADIUS_M)\n",
        "\n",
        "join_result = gpd.sjoin(vegetation_proj, property_buffers, how=\"inner\", predicate=\"within\")\n",
        "green_points_count = join_result.groupby('index_right').size()\n",
        "\n",
        "# Calculamos el area de cada punto del raster:\n",
        "pixel_area_m2 = 100 # Cada pixel tiene resolución de 10m, así que el área al cuadrado es 10m x 10m = 100m^2\n",
        "green_area = green_points_count * pixel_area_m2\n",
        "green_area.name = 'green_area_in_radius' # El nombre que tendra la columna nueva de cada propiedad.\n",
        "\n",
        "# Para evitar errores, si la celda ya ha sido ejecutada y quremos probar nuevamente, debemos\n",
        "# eliminarla para volver a calcular todo, para eliminarlo de la memoria temporal del ámbito de ejecución\n",
        "# de Google Colab, ya que hemos tenido problemas sin este condicional:\n",
        "if 'green_area_in_radius' in gdf_properties.columns:\n",
        "    gdf_properties = gdf_properties.drop(columns=['green_area_in_radius'])\n",
        "\n",
        "# Hacemos un join de el area verde calculada para cada pixel en un raster, con el GDF principal.\n",
        "gdf_properties = gdf_properties.join(green_area).fillna(0) # Rellenamos areas sin nada con 0\n",
        "# Parseo a entero para evitar errores en el modelo predictivo posterior:\n",
        "gdf_properties['green_area_in_radius'] = gdf_properties['green_area_in_radius'].astype(int)\n",
        "\n",
        "display(gdf_properties[['ADDRESS', 'SALE PRICE', 'green_area_in_radius']].head()) # Comprobamos que se haya calculado bien"
      ],
      "metadata": {
        "id": "6mZRltg5f_5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez calculadas las áreas para cada pixel y guardados en el gdf_vegetation, hacemos la proyección adecuada en metros para cada dataset, y hacemos el cálculo para cada pixel por separado, y contamos cuánta área hay en un radio de RADIUS_M metros para cada propiedad. Finalmente, el área obtenida se agrega como una nueva columna del GDF de propiedades."
      ],
      "metadata": {
        "id": "-hzYM-RDe_F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un buffer de 500metros para cada propiedad y realizamos un spatial join para guardar esos datos.\n",
        "print(f\"Calculando área verde en un radio de {RADIUS_M}m para cada propiedad...\")\n",
        "property_buffers = properties_proj.copy()\n",
        "property_buffers['geometry'] = property_buffers.geometry.buffer(RADIUS_M)\n",
        "join_result = gpd.sjoin(vegetation_proj, property_buffers, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Hacemos el cálculo de la cantidad de área verde por pixel\n",
        "green_points_count = join_result.groupby('index_right').size()\n",
        "pixel_area_m2 = 100 # Cada punto representa un píxel de 10m x 10m\n",
        "green_area = green_points_count * pixel_area_m2\n",
        "green_area.name = 'green_area_in_radius'\n",
        "\n",
        "# Para evitar errores, si la celda ya ha sido ejecutada y quremos probar nuevamente, debemos\n",
        "# eliminarla para volver a calcular todo, para eliminarlo de la memoria temporal del ámbito de ejecución\n",
        "# de Google Colab, ya que hemos tenido problemas sin este condicional:\n",
        "if 'green_area_in_radius' in gdf_properties.columns:\n",
        "    gdf_properties = gdf_properties.drop(columns=['green_area_in_radius'])\n",
        "\n",
        "# Hacemos un join de el area verde calculada para cada pixel en un raster, con el GDF principal.\n",
        "gdf_properties = gdf_properties.join(green_area).fillna(0) # Rellenamos areas sin nada con 0\n",
        "# Parseo a entero para evitar errores en el modelo predictivo posterior:\n",
        "gdf_properties['green_area_in_radius'] = gdf_properties['green_area_in_radius'].astype(int)\n",
        "\n",
        "# Comprobamos que se haya calculado bien en el GDF principal\n",
        "display(gdf_properties[['ADDRESS', 'SALE PRICE', 'green_area_in_radius']].head())"
      ],
      "metadata": {
        "id": "tXNi3Ey6C60o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las distancias calculadas están en metros, gracias al CRS utilizado.\n",
        "\n",
        "Como vemos aquí abajo, el dataset gdf_properties ahora tiene las distancias al subway, hospital y centro educativo más cercano, en las columnas dist_real_subway y dist_real_hospital, dist_real_university respectivamente, además del green_area_in_radius.\n",
        "\n",
        "En celdas posteriores sólo nos falta agregar una nueva columna al GDF de propiedades, que contendrá la cantidad de inundaciones que han ocurrido en el periodo de estudio, según los dos casos que se presentarán más abajo."
      ],
      "metadata": {
        "id": "OaL7LHCtZOci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gdf_properties.columns) # Miramos el gdf properties, solo sus columnas"
      ],
      "metadata": {
        "id": "6tm50kL_ZOsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, formaremos una segmentación de los datos, representaremos mediante hexágonos de colores el precio promedio de las casas que están en esa área aproximada, usaremos h3 (https://h3geo.org/) para poder crear fácilmente los hexágonos, la idea es crear la propiedad \"hex_id\" en el GDF de propiedades, con las coordenadas, para luego poder usar eso como agrupamiento, esto es, agrupar según el hex_id, y calcular el precio promedio y la cantidad de viviendas o propiedades que caen dentro de ese hexágono.\n",
        "\n",
        "https://datascience-alw.medium.com/lo-que-necesitas-saber-para-crear-visualizaciones-con-h3-en-r-y-analizar-datos-geo-espaciales-c223442d4367\n",
        "\n",
        "https://github.com/uber/H3"
      ],
      "metadata": {
        "id": "w5w6AZlJtCdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# h3 requiere coordenadas en latitud/longitud, por lo que nos aseguramos de tener el CRS correcto (EPSG:4326)\n",
        "print(f\"CRS de propiedades: {gdf_properties.crs}\")\n",
        "\n",
        "# Asignamos cada vivienda a un hexágono\n",
        "gdf_properties['hex_id'] = gdf_properties.apply(\n",
        "    lambda row: h3.latlng_to_cell(row.geometry.y, row.geometry.x, 8),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# usamos .groupby para agrupar las viviendas que pertenecen a un mismo hexágono, y luego con\n",
        "# .agg hacemos un agregado donde calculamos el avg_price y la cantidad de propiedades en ese hexágono:\n",
        "hex_aggregated = gdf_properties.groupby('hex_id').agg(\n",
        "    avg_price=('SALE PRICE', 'mean'),\n",
        "    property_count=('SALE PRICE', 'size')\n",
        ").reset_index() # Reseteamos el índice del nuevo GDF porque hubieron cambios importantes.\n",
        "\n",
        "# Hacemos un filtro de n propiedades mínimas que deben existir en cada hexágono:\n",
        "min_properties = 4\n",
        "hex_aggregated_filtered = hex_aggregated[hex_aggregated['property_count'] >= min_properties]\n",
        "\n",
        "# En base a cada id de un hexágono, esta función generará un polígono, que será la geometría del hexágono:\n",
        "# def hex_id_to_polygon(hex_id):\n",
        "#     points = h3.cell_to_boundary(hex_id, unit='deg')\n",
        "#     return Polygon(points)\n",
        "def hex_id_to_polygon(hex_id):\n",
        "    # CORRECCIÓN AQUÍ:\n",
        "    # 1. Se elimina el parámetro 'unit', ya que ahora no es necesario.\n",
        "    # 2. La nueva función devuelve tuplas (lat, lon), pero Polygon necesita (lon, lat),\n",
        "    #    así que invertimos el orden de cada tupla en la lista.\n",
        "    points_lat_lon = h3.cell_to_boundary(hex_id)\n",
        "    points_lon_lat = [(lon, lat) for lat, lon in points_lat_lon]\n",
        "    return Polygon(points_lon_lat)\n",
        "\n",
        "# Usamos la función, para cada vivienda del GDF, pasamos el hex_id por la función y agregamos el resultado como geometry.\n",
        "hex_aggregated_filtered['geometry'] = hex_aggregated_filtered['hex_id'].apply(hex_id_to_polygon)\n",
        "\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Preparación para los popups\n",
        "properties_for_popup = gdf_properties.merge(hex_aggregated_filtered[['hex_id']], on='hex_id', how='inner')\n",
        "\n",
        "def create_popup_html(group):\n",
        "    html = f\"<b>Propiedades en esta zona: {len(group)}</b><br><ul>\"\n",
        "    for _, row in group.head(10).iterrows():\n",
        "        price_formatted = f\"${row['SALE PRICE']:,.0f}\"\n",
        "        html += f\"<li>{row['ADDRESS']}: {price_formatted}</li>\"\n",
        "    if len(group) > 10:\n",
        "        html += f\"<li>... y {len(group) - 10} más.</li>\"\n",
        "    html += \"</ul>\"\n",
        "    return html\n",
        "\n",
        "popup_data = properties_for_popup.groupby('hex_id').apply(create_popup_html)\n",
        "popup_data.name = 'popup_html'\n",
        "hex_aggregated_filtered = hex_aggregated_filtered.merge(popup_data, on='hex_id', how='left')\n",
        "\n",
        "print(f\"GeoDataFrame final con {len(hex_aggregated_filtered)} hexágonos\")\n",
        "hex_aggregated_filtered.head()"
      ],
      "metadata": {
        "id": "Nn61fj8mQ-wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez calculado todo, creamos un folium para poder poner los hexágonos en la visualización."
      ],
      "metadata": {
        "id": "J0Pl90NtuTT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un folium centrado en Manhattan, para poder ver mejor los hexágonos\n",
        "map_final = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Según el precio min y max, creamos los valores max y min del mapa de colores.\n",
        "min_price = hex_aggregated_filtered['avg_price'].min()\n",
        "max_price = hex_aggregated_filtered['avg_price'].max()\n",
        "colormap = linear.Reds_09.scale(min_price, max_price)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Iteración por cada entrada del dataset para agregar el hexágono con su color al folium.\n",
        "for _, row in hex_aggregated_filtered.iterrows():\n",
        "    geojson_layer = folium.GeoJson(\n",
        "        row.geometry.__geo_interface__,\n",
        "        style_function=lambda feature, color=colormap(row['avg_price']): {\n",
        "            'color': 'black',\n",
        "            'weight': 0.5,\n",
        "            'fillColor': color,\n",
        "            'fillOpacity': 0.8,\n",
        "        }\n",
        "    )\n",
        "    popup = folium.Popup(row['popup_html'], max_width=400)\n",
        "    popup.add_to(geojson_layer)\n",
        "    geojson_layer.add_to(map_final)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "legend_title = 'Precio Promedio ($)'\n",
        "legend_html = f'''\n",
        "     <div style=\"\n",
        "     position: fixed;\n",
        "     bottom: 50px;\n",
        "     right: 50px;\n",
        "     width: 150px;\n",
        "     height: auto;\n",
        "     border:2px solid grey;\n",
        "     z-index:9999;\n",
        "     font-size:14px;\n",
        "     background-color:white;\n",
        "     padding: 10px;\n",
        "     \">\n",
        "     <b>{legend_title}</b><br>\n",
        "     '''\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Iteramos sobre 6 pasos en nuestra escala de colores para crear las etiquetas, una por cada color.\n",
        "for val in np.linspace(colormap.vmin, colormap.vmax, 6):\n",
        "    color = colormap(val)\n",
        "    val_formatted = f\"${val:,.0f}\"\n",
        "    legend_html += f'<i class=\"fa fa-circle\" style=\"color:{color}\"></i>&nbsp;{val_formatted}<br>'\n",
        "\n",
        "legend_html += '</div>'\n",
        "map_final.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "display(map_final)"
      ],
      "metadata": {
        "id": "6bCFRu8yQ_6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un buffer (utilizado en las primeras ayudantías) https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.buffer.html de 500 metros, para poder ver cuántos tiroteos han ocurrido cerca de cada propiedad."
      ],
      "metadata": {
        "id": "BMV1kTbizG3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "radius_m = 500 # radio de metros para buscar incidentes\n",
        "CRS_METERS = \"EPSG:32118\" # Definimos el CRS para trabajar en metros en NYC\n",
        "\n",
        "# Hacemos copias para no alterar la proyección de los GDF originales.\n",
        "properties_to_analyze = gdf_properties.copy()\n",
        "shootings_to_analyze = gdf_incidents.copy()\n",
        "\n",
        "# Proyectamos al CRS en metros (EPSG:32118 para NYC)\n",
        "properties_proj = properties_to_analyze.to_crs(CRS_METERS)\n",
        "shootings_proj = shootings_to_analyze.to_crs(CRS_METERS)\n",
        "\n",
        "# Creamos un buffer que rodee a cada propiedad, de radius_m metros de medida:\n",
        "property_with_buffer = properties_proj.copy()\n",
        "property_with_buffer['geometry'] = property_with_buffer.geometry.buffer(radius_m)\n",
        "\n",
        "# Hacemos un SPATIAL JOIN del GDF que contiene los tiroteos con el de las propiedades con el buffer.\n",
        "join_result = gpd.sjoin(shootings_proj, property_with_buffer, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Contamos la cantidad de tiroteos que caen en cada propiedad con el buffer.\n",
        "shootings_count = join_result.groupby('index_right').size()\n",
        "shootings_count.name = 'shootings_in_radius'\n",
        "\n",
        "# Para evitar errores, si la celda ya ha sido ejecutada y quremos probar nuevamente, debemos\n",
        "# eliminarla para volver a calcular todo, para eliminarlo de la memoria temporal del ámbito de ejecución\n",
        "# de Google Colab, ya que hemos tenido problemas sin este condicional:\n",
        "if 'shootings_in_radius' in gdf_properties.columns:\n",
        "    gdf_properties = gdf_properties.drop(columns=['shootings_in_radius'])\n",
        "\n",
        "# Hacemos un join de los datos del conteo de tiroteos con los datos del GDF principal\n",
        "# Como limpieza, si no hay tiroteos en una propiedad, le ponemos 0 y lo convertimos a tipo int\n",
        "gdf_properties = gdf_properties.join(shootings_count).fillna(0)\n",
        "# Como limpieza, convertimos la nueva columna a tipo entero.\n",
        "gdf_properties['shootings_in_radius'] = gdf_properties['shootings_in_radius'].astype(int) # Parseo necesario\n",
        "\n",
        "# Verificamos y mostramos para confirmar que todo esté bien\n",
        "propiedades_con_tiroteos = gdf_properties[gdf_properties['shootings_in_radius'] > 0].shape[0]\n",
        "print(f\"Hay {propiedades_con_tiroteos} propiedades con 1 o más tiroteos en un radio de {radius_m} metros.\")\n",
        "\n",
        "display(gdf_properties[['ADDRESS', 'SALE PRICE', 'shootings_in_radius']].head())"
      ],
      "metadata": {
        "id": "5hIiVDRSVlYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, hacemos lo mismo que en celdas previas para los hexágonos con precios de propiedades, pero en vez del precio, ahora usamos la cantidad de tiroteos, el código es casi totalmente copiar y pegar la lógica previa."
      ],
      "metadata": {
        "id": "VkyL_K0n1rei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# h3 requiere coordenadas en latitud/longitud.\n",
        "gdf_incidents_h3 = gdf_incidents.to_crs(\"EPSG:4326\")\n",
        "# También necesitamos las propiedades para los popups.\n",
        "gdf_properties_h3 = gdf_properties.to_crs(\"EPSG:4326\")\n",
        "\n",
        "resolution = 8\n",
        "gdf_incidents_h3['hex_id'] = gdf_incidents_h3.apply(\n",
        "    lambda row: h3.latlng_to_cell(row.geometry.y, row.geometry.x, resolution), axis=1\n",
        ")\n",
        "\n",
        "# Contar los tiroteos por hexágono\n",
        "shooting_counts = gdf_incidents_h3.groupby('hex_id').size().reset_index(name='shooting_count')\n",
        "\n",
        "# GDF de Hexágonos con los Conteos\n",
        "def hex_id_to_polygon(hex_id):\n",
        "    pts_lat_long = h3.cell_to_boundary(hex_id)\n",
        "    points_lon_lat = [(lon, lat) for lat, lon in pts_lat_long] # Cambiamos el orden para poder proyectar.\n",
        "    return Polygon(points_lon_lat)\n",
        "\n",
        "# Creamos las geometrías y el GeoDataFrame base\n",
        "shooting_counts['geometry'] = shooting_counts['hex_id'].apply(hex_id_to_polygon)\n",
        "gdf_hexagons_shootings = gpd.GeoDataFrame(shooting_counts, crs=\"EPSG:4326\")\n",
        "\n",
        "# Asignamos también las propiedades a los hexágonos para los popups\n",
        "gdf_properties_h3['hex_id'] = gdf_properties_h3.apply(\n",
        "    lambda row: h3.latlng_to_cell(row.geometry.y, row.geometry.x, resolution), axis=1\n",
        ")\n",
        "\n",
        "# Unimos para saber qué propiedades están en los hexágonos con tiroteos\n",
        "properties_for_popup = gdf_properties_h3.merge(gdf_hexagons_shootings[['hex_id']], on='hex_id', how='inner')\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "def create_popup_html(group):\n",
        "    html = f\"<b>Propiedades en esta zona de riesgo: {len(group)}</b><br><ul>\"\n",
        "    for _, row in group.head(10).iterrows():\n",
        "        price_formatted = f\"${row['SALE PRICE']:,.0f}\"\n",
        "        html += f\"<li>{row['ADDRESS']}: {price_formatted}</li>\"\n",
        "    if len(group) > 10:\n",
        "        html += f\"<li>... y {len(group) - 10} más.</li>\"\n",
        "    html += \"</ul>\"\n",
        "    return html\n",
        "\n",
        "popup_data = properties_for_popup.groupby('hex_id').apply(create_popup_html)\n",
        "popup_data.name = 'popup_html'\n",
        "# Unimos los popups al GDF de hexágonos de tiroteos\n",
        "gdf_hexagons_shootings = gdf_hexagons_shootings.merge(popup_data, on='hex_id', how='left')\n",
        "gdf_hexagons_shootings['popup_html'].fillna(\"No hay propiedades registradas en esta zona.\", inplace=True)"
      ],
      "metadata": {
        "id": "aXEMVkbcvFRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representamos en un folium las zonas hexagonales, con su cantidad de tiroteos indicada en la leyenda y los popups."
      ],
      "metadata": {
        "id": "deN1Lujglgw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapa folium centrado en Manhattan, como antes.\n",
        "map_shootings = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Creamos la misma paleta que antes, con min y max segun cantidad de incidentes.\n",
        "min_count = gdf_hexagons_shootings['shooting_count'].min()\n",
        "max_count = gdf_hexagons_shootings['shooting_count'].max()\n",
        "colormap = linear.YlOrRd_09.scale(min_count, max_count)\n",
        "\n",
        "# Agregamos cada hexágono al folium\n",
        "for _, row in gdf_hexagons_shootings.iterrows():\n",
        "    geojson_layer = folium.GeoJson(\n",
        "        row.geometry.__geo_interface__,\n",
        "        # El color ahora depende de la cantidad de tiroteos, hay una paleta.\n",
        "        style_function=lambda feature, color=colormap(row['shooting_count']): {\n",
        "            'color': 'black',\n",
        "            'weight': 0.5,\n",
        "            'fillColor': color,\n",
        "            'fillOpacity': 0.8,\n",
        "        }\n",
        "    )\n",
        "    # El popup muestra la lista de propiedades en esa zona\n",
        "    popup = folium.Popup(row['popup_html'], max_width=400)\n",
        "    popup.add_to(geojson_layer)\n",
        "    geojson_layer.add_to(map_shootings)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "legend_title = 'Cantidad de tiroteos en la zona'\n",
        "legend_html = f'''\n",
        "     <div style=\"\n",
        "     position: fixed; bottom: 50px; right: 50px; width: 150px; height: auto;\n",
        "     border:2px solid grey; z-index:9999; font-size:14px;\n",
        "     background-color:white; padding: 10px;\">\n",
        "     <b>{legend_title}</b><br>\n",
        "     '''\n",
        "for val in np.linspace(colormap.vmin, colormap.vmax, 6):\n",
        "    color = colormap(val)\n",
        "    # Formateamos el número como un entero\n",
        "    val_formatted = f\"{int(val)}\"\n",
        "    legend_html += f'<i class=\"fa fa-circle\" style=\"color:{color}\"></i>&nbsp;{val_formatted}<br>'\n",
        "legend_html += '</div>'\n",
        "map_shootings.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "display(map_shootings)"
      ],
      "metadata": {
        "id": "Z1fvugpIvHN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se presentan dos casos de inundaciones en la isla de Manhattan, ocurridos en el 2023 y 2024, se han utilizado datos SAR y el dataset de Sentinel-1:"
      ],
      "metadata": {
        "id": "rCSgquE6muQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función que transforma en dB\n",
        "# Según la propia página de Sentinel 1 GRD:\n",
        "# \"se convierten en decibeles mediante la escala de registro (10*log10(x)).\"\n",
        "# https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD?hl=es-419\n",
        "\n",
        "def dB_VV(img):\n",
        "    return img.select('VV').log10().multiply(10)"
      ],
      "metadata": {
        "id": "ej6NstFQWcTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 2023:\n",
        "\n",
        "Durante el 29 de septiembre de 2023, el servicio meteorológico nacional emitió un aviso de inundación repentina para gran parte de la región, lo que afectó a 8,5 millones de personas.\n",
        "\n",
        "Este suceso fue tan potente que la gobernadora de New York, Kathy Hochul, declaró el “estado de emergencia” para Nueva York. Con respecto al distrito de Manhattan, se pudieron evidenciar autos varados por las inundaciones. Entonces es interesante revisarlo para nuestro estudio.\n",
        "\n",
        "\n",
        "- 29 de Septiembre de 2023\n",
        "- https://www.lanacion.com.ar/estados-unidos/las-intensas-lluvias-provocan-inundaciones-repentinas-en-nueva-york-e-interrupciones-de-servicios-nid29092023/"
      ],
      "metadata": {
        "id": "qeVvX40rmtmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí se usa el código extraído de \"floodMapping_Maule.ipynb\".\n",
        "pre_flood_start_date = '2023-09-25'\n",
        "pre_flood_end_date = '2023-09-28'\n",
        "flood_start_date = '2023-09-29'\n",
        "flood_end_date = '2023-10-09'\n",
        "\n",
        "s1_col_pre = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(pre_flood_start_date, pre_flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "\n",
        "s1_col_post= (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(flood_start_date, flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "if s1_col_pre.size().getInfo() > 0 and s1_col_post.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s1_col_pre = s1_col_pre.map(dB_VV)\n",
        "    s1_col_post = s1_col_post.map(dB_VV)\n",
        "\n",
        "    sar_pre = s1_col_pre.reduce(ee.Reducer.percentile([20]))\n",
        "    sar_post = s1_col_post.reduce(ee.Reducer.percentile([20]))\n",
        "\n",
        "    threshold = -17\n",
        "\n",
        "    sar_pre_filtered = sar_pre.focal_median(radius=1, units='pixels')\n",
        "    sar_post_filtered = sar_post.focal_median(radius=1, units='pixels')\n",
        "\n",
        "    water_pre = sar_pre_filtered.lt(threshold)\n",
        "    water_post = sar_post_filtered.lt(threshold)\n",
        "\n",
        "    flood_extent = water_post.unmask().subtract(water_pre.unmask()).gt(0).selfMask()\n",
        "\n",
        "    flood_points = flood_extent.sample(\n",
        "        region=manhattan_ee,\n",
        "        scale=10,\n",
        "        projection='EPSG:4326',\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # Lo convertimos en gdf:\n",
        "    gdf_fp_23 = geemap.ee_to_gdf(flood_points)\n",
        "\n",
        "    area = flood_extent.reduceRegion(\n",
        "        reducer= ee.Reducer.sum(),\n",
        "        geometry= manhattan_ee,\n",
        "        scale= 10,\n",
        "        bestEffort= True\n",
        "    )\n",
        "\n",
        "    area = area.getInfo()\n",
        "\n",
        "    print(f\"El área inundada es de {str(area['VV_p20'] * 100)} m^2\")\n",
        "\n",
        "    Map.addLayer(sar_pre.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR pre')\n",
        "    Map.addLayer(sar_post.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR post')\n",
        "\n",
        "    Map.addLayer(flood_extent.clip(manhattan_ee), {'palette': 'magenta'}, 'Flood Extent')\n",
        "    Map.addLayer(flood_points, {'color': 'blue'}, 'Flood Points 2023')\n",
        "\n",
        "    Map.addLayerControl()\n",
        "    Map\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado imagenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ],
      "metadata": {
        "id": "8WYtlki1Wl4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 2024\n",
        "\n",
        "Durante el 6 de Agosto de 2024, el usuario \"Mayor Eric Adams\" dio a conocer el siguiente mensaje: \"Neoyorquinos: se pronostican lluvias intensas para esta noche. Hay una alerta de inundaciones y una advertencia de viaje vigentes hasta mañana el mediodía por tormentas eléctricas. Tenga un plan para estar protegido.\"\n",
        "\n",
        "Estas lluvias intensas que menciona fueron tan severas que provocaron inundaciones por gran parte de la zona, por ende, es ideal para poder revisar como podrían afectar las inundaciones a Manhattan.\n",
        "\n",
        "- Fecha: 6 de Agosto de 2024\n",
        "- Enlace: https://spectrumnoticias.com/ny/nyc/noticias/2024/08/06/ha-comenzado-la-lluvia--advierten-de-posibles-inundaciones--advertencia-de-viaje-por-tormentas-electricas"
      ],
      "metadata": {
        "id": "Evqag2CMmhV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map(center=[40.7, -74.0], zoom=12) #Definimos el mapa y centro\n",
        "\n",
        "# Aquí se usa el código extraído de \"floodMapping_Maule.ipynb\".\n",
        "pre_flood_start_date = '2024-08-03'\n",
        "pre_flood_end_date = '2024-08-05'\n",
        "flood_start_date = '2024-08-06'\n",
        "flood_end_date = '2024-08-16'\n",
        "\n",
        "s1_col_pre = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(pre_flood_start_date, pre_flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "\n",
        "s1_col_post= (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .filterDate(flood_start_date, flood_end_date)\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .select('VV'))\n",
        "\n",
        "if s1_col_pre.size().getInfo() > 0 and s1_col_post.size().getInfo() > 0: #Verificamos que la colección tenga imágenes\n",
        "    s1_col_pre = s1_col_pre.map(dB_VV)\n",
        "    s1_col_post = s1_col_post.map(dB_VV)\n",
        "\n",
        "    sar_pre = s1_col_pre.reduce(ee.Reducer.percentile([20]))\n",
        "    sar_post = s1_col_post.reduce(ee.Reducer.percentile([20]))\n",
        "\n",
        "    threshold = -17\n",
        "\n",
        "    sar_pre_filtered = sar_pre.focal_median(radius=1, units='pixels')\n",
        "    sar_post_filtered = sar_post.focal_median(radius=1, units='pixels')\n",
        "\n",
        "    water_pre = sar_pre_filtered.lt(threshold)\n",
        "    water_post = sar_post_filtered.lt(threshold)\n",
        "\n",
        "    flood_extent = water_post.unmask().subtract(water_pre.unmask()).gt(0).selfMask()\n",
        "\n",
        "    flood_points = flood_extent.sample(\n",
        "        region=manhattan_ee,\n",
        "        scale=10,\n",
        "        projection='EPSG:4326',\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # Lo convertimos en gdf:\n",
        "    gdf_fp_24 = geemap.ee_to_gdf(flood_points)\n",
        "\n",
        "    area = flood_extent.reduceRegion(\n",
        "        reducer= ee.Reducer.sum(),\n",
        "        geometry= manhattan_ee,\n",
        "        scale= 10,\n",
        "        bestEffort= True\n",
        "    )\n",
        "\n",
        "    area = area.getInfo()\n",
        "\n",
        "    print(f\"El área inundada es de {str(area['VV_p20'] * 100)} m^2\")\n",
        "\n",
        "    Map.addLayer(sar_pre.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR pre')\n",
        "    Map.addLayer(sar_post.clip(manhattan_ee), {'min': -25, 'max': -5}, 'SAR post')\n",
        "\n",
        "    Map.addLayer(flood_extent.clip(manhattan_ee), {'palette': 'magenta'}, 'Flood Extent')\n",
        "    Map.addLayer(flood_points, {'color': 'red'}, 'Flood Points 2024')\n",
        "\n",
        "    Map.addLayerControl()\n",
        "    Map\n",
        "\n",
        "else:\n",
        "    print(\"No se han hallado imagenes para el dataset en las fechas de interés.\")\n",
        "\n",
        "display(Map)"
      ],
      "metadata": {
        "id": "9nB8kwmJWexL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisando ambos casos\n",
        "\n",
        "Decidimos revisar ambos casos en conjunto para poder manipular tan sólo un geodataframe y ver comportamientos que se podrían llegar a repetir con respecto a las inundaciones. Por ejemplo: ¿habrá alguna zona que NO suele tener inundaciones, o suele tener menos?\n",
        "\n",
        "La respuesta a esta pregunta podría ser efectiva para ver si afecta realmente al precio de las viviendas, lo que ha sido estudiado con el modelo de predicción."
      ],
      "metadata": {
        "id": "FZvJOBAbnLwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_fp_23['year'] = '2023'\n",
        "gdf_fp_24['year'] = '2024'\n",
        "\n",
        "gdf = gpd.GeoDataFrame(pd.concat([gdf_fp_24, gdf_fp_23], ignore_index=True))\n",
        "\n",
        "gdf = gdf.to_crs(epsg=2263) # Proyección en metros para NYC.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "gdf.plot(ax=ax, column='year', categorical=True, markersize=5, legend=True)\n",
        "plt.show()\n",
        "\n",
        "coords = np.array(list(zip(gdf.geometry.x, gdf.geometry.y)))\n",
        "\n",
        "db = DBSCAN(eps=328.084, min_samples=3).fit(coords)\n",
        "gdf['cluster'] = db.labels_\n",
        "\n",
        "# No nos interesa mostrar el clustering, esto es sólo para probar\n",
        "# fig, ax = plt.subplots(figsize=(8, 8))\n",
        "# gdf.plot(ax=ax, column='cluster', categorical=True, markersize=5, legend=False)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "fw0es7U5Wn1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentación hexagonal aplicada al análisis de inundaciones\n",
        "\n",
        "Como podemos ver, la segmentación hexagonal resultó una herramienta fundamental para visualizar de forma clara y detallada las zonas con mayor propensión a inundarse, al menos en el perido de estudio. Este tipo de representación espacial no solo mejora significativamente el atractivo visual del análisis en la presentación, sino que también aporta un gran valor informativo al facilitar la interpretación de patrones en los puntos. A través de una escala cromática con la paleta de ccolores, se identifican las áreas que presentaron signos de inundación según los datos obtenidos por el dataset Sentinel-1 en las dos fechas en que se registraron los eventos (2023 y 2024)."
      ],
      "metadata": {
        "id": "3kGD9uAInRgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# h3 requiere coordenadas en latitud/longitud.\n",
        "gdf_flood_h3 = gdf.to_crs(\"EPSG:4326\")\n",
        "# También necesitamos las propiedades para los popups.\n",
        "gdf_properties_h3 = gdf_properties.to_crs(\"EPSG:4326\")\n",
        "\n",
        "resolution = 8\n",
        "gdf_flood_h3['hex_id'] = gdf_flood_h3.geometry.apply(\n",
        "    lambda geom: h3.latlng_to_cell(geom.centroid.y, geom.centroid.x, resolution)\n",
        ")\n",
        "\n",
        "# Contar los tiroteos por hexágono\n",
        "flood_counts = gdf_flood_h3.groupby('hex_id').size().reset_index(name='flood_count')\n",
        "\n",
        "# GDF de Hexágonos con los Conteos\n",
        "def hex_id_to_polygon(hex_id):\n",
        "    latlngs = h3.cell_to_boundary(hex_id)\n",
        "    points = [(lng, lat) for lat, lng in latlngs]\n",
        "    return Polygon(points)\n",
        "\n",
        "# Creamos las geometrías y el GeoDataFrame base\n",
        "flood_counts['geometry'] = flood_counts['hex_id'].apply(hex_id_to_polygon)\n",
        "gdf_hexagons_flood = gpd.GeoDataFrame(flood_counts, crs=\"EPSG:4326\")\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "# Asignamos también las propiedades a los hexágonos para los popups\n",
        "gdf_properties_h3['hex_id'] = gdf_properties_h3.geometry.apply(\n",
        "    lambda geom: h3.latlng_to_cell(geom.y, geom.x, resolution)\n",
        ")\n",
        "\n",
        "# Unimos para saber qué propiedades están en los hexágonos con tiroteos\n",
        "properties_for_popup = gdf_properties_h3.merge(gdf_hexagons_flood[['hex_id']], on='hex_id', how='inner')\n",
        "\n",
        "def create_popup_html(group):\n",
        "    html = f\"<b>Propiedades en esta zona de riesgo: {len(group)}</b><br><ul>\"\n",
        "    for _, row in group.head(10).iterrows():\n",
        "        price_formatted = f\"${row['SALE PRICE']:,.0f}\"\n",
        "        html += f\"<li>{row['ADDRESS']}: {price_formatted}</li>\"\n",
        "    if len(group) > 10:\n",
        "        html += f\"<li>... y {len(group) - 10} más.</li>\"\n",
        "    html += \"</ul>\"\n",
        "    return html\n",
        "\n",
        "popup_data = properties_for_popup.groupby('hex_id').apply(create_popup_html)\n",
        "popup_data.name = 'popup_html'\n",
        "# Unimos los popups al GDF de hexágonos de tiroteos\n",
        "gdf_hexagons_flood = gdf_hexagons_flood.merge(popup_data, on='hex_id', how='left')\n",
        "gdf_hexagons_flood['popup_html'].fillna(\"No hay propiedades registradas en esta zona.\", inplace=True)\n",
        "# Mapa folium centrado en Manhattan, como antes.\n",
        "map_flood = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\")\n",
        "\n",
        "# Creamos la misma paleta que antes, con min y max segun cantidad de incidentes.\n",
        "min_count = gdf_hexagons_flood['flood_count'].min()\n",
        "max_count = gdf_hexagons_flood['flood_count'].max()\n",
        "colormap = linear.Blues_09.scale(min_count, max_count)\n",
        "\n",
        "# Agregamos cada hexágono al folium\n",
        "for _, row in gdf_hexagons_flood.iterrows():\n",
        "    geojson_layer = folium.GeoJson(\n",
        "        row.geometry.__geo_interface__,\n",
        "        # El color ahora depende de la cantidad de tiroteos, hay una paleta.\n",
        "        style_function=lambda feature, color=colormap(row['flood_count']): {\n",
        "            'color': 'black',\n",
        "            'weight': 0.5,\n",
        "            'fillColor': color,\n",
        "            'fillOpacity': 0.8,\n",
        "        }\n",
        "    )\n",
        "    # El popup muestra la lista de propiedades en esa zona\n",
        "    popup = folium.Popup(row['popup_html'], max_width=400)\n",
        "    popup.add_to(geojson_layer)\n",
        "    geojson_layer.add_to(map_flood)\n",
        "\n",
        "# --- Código de popup generado con AI ---\n",
        "legend_title = 'Cantidad de Inundaciones'\n",
        "legend_html = f'''\n",
        "     <div style=\"\n",
        "     position: fixed; bottom: 50px; right: 50px; width: 150px; height: auto;\n",
        "     border:2px solid grey; z-index:9999; font-size:14px;\n",
        "     background-color:white; padding: 10px;\">\n",
        "     <b>{legend_title}</b><br>\n",
        "     '''\n",
        "for val in np.linspace(colormap.vmin, colormap.vmax, 6):\n",
        "    color = colormap(val)\n",
        "    # Formateamos el número como un entero\n",
        "    val_formatted = f\"{int(val)}\"\n",
        "    legend_html += f'<i class=\"fa fa-circle\" style=\"color:{color}\"></i>&nbsp;{val_formatted}<br>'\n",
        "legend_html += '</div>'\n",
        "map_flood.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "display(map_flood)"
      ],
      "metadata": {
        "id": "ItuxSFzffsXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RADIUS_M = 500 # Radio en metros para el análisis\n",
        "CRS_METERS = \"EPSG:32118\" # Proyección en metros para NYC\n",
        "\n",
        "# Nos aseguramos que se utilice la proyección adecuada\n",
        "properties_proj = gdf_properties.to_crs(CRS_METERS)\n",
        "flood_points_proj = gdf_fp_24.to_crs(CRS_METERS)\n",
        "\n",
        "# Creamos los buffers con el radio definido en la variable arriba\n",
        "print(f\"Calculando puntos de inundación en un radio de {RADIUS_M}m para cada propiedad...\")\n",
        "property_buffers = properties_proj.copy()\n",
        "property_buffers['geometry'] = property_buffers.geometry.buffer(RADIUS_M)\n",
        "join_result = gpd.sjoin(flood_points_proj, property_buffers, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Hacemos el conteo de puntos\n",
        "flood_points_count = join_result.groupby('index_right').size()\n",
        "flood_points_count.name = 'flood_points_in_radius'\n",
        "\n",
        "# Para evitar errores, si la celda ya ha sido ejecutada y quremos probar nuevamente, debemos\n",
        "# eliminarla para volver a calcular todo, para eliminarlo de la memoria temporal del ámbito de ejecución\n",
        "# de Google Colab, ya que hemos tenido problemas sin este condicional:\n",
        "if 'flood_points_in_radius' in gdf_properties.columns:\n",
        "    gdf_properties = gdf_properties.drop(columns=['flood_points_in_radius'])\n",
        "\n",
        "# Como limpieza, convertimos la nueva columna a tipo entero.\n",
        "gdf_properties = gdf_properties.join(flood_points_count).fillna(0)\n",
        "gdf_properties['flood_points_in_radius'] = gdf_properties['flood_points_in_radius'].astype(int) # Parseo necesario\n",
        "\n",
        "display(gdf_properties[['ADDRESS', 'SALE PRICE', 'flood_points_in_radius']].head())"
      ],
      "metadata": {
        "id": "PhPfqPB0TZV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos intentado, pero no hay suficientes datos de incidentes para poder realizar un análisis estadístico con Moran para los datos anteriores, los resultados son inconsistentes."
      ],
      "metadata": {
        "id": "x92HNr3i7Eog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se ha tomado el último modelo de clasificación de la clase de Deep Learning para datos SAR, específicamente el modelo de clasificación **Random Forest**, pero aplicando el algoritmo a un modelo de predicción, tomando como base a los datos de ventas del dataset gdf_properties para el entrenamiento.\n",
        "\n",
        "Fuentes de información consultadas:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "https://www.geeksforgeeks.org/machine-learning/random-forest-regression-in-python/\n",
        "\n",
        "https://www.youtube.com/watch?v=YUsx5ZNlYWc\n",
        "\n",
        "https://youtu.be/AYICIq5jnhU\n",
        "\n",
        "https://youtu.be/kFwe2ZZU7yw?t=718"
      ],
      "metadata": {
        "id": "_dWtN4Lzh70P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero que nada, hacemos una copia del modelo para poder limpiarlo, eliminamos outliers de precios muy bajos y altos en el precio, para que no distorsionen los resultados del entrenamiento.\n",
        "\n",
        "Eliminaremos los datos de viviendas con precio de venta extremadamente altos, sólo nos quedaremos con las propiedades que tienen precio de venta igual o menor al 90% de los datos, puesto que en Manhattan hay propiedades con precios extremadamente altos en relación al promedio y mediana, eso distorsiona nuestro"
      ],
      "metadata": {
        "id": "67u3OYZIjuvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "df_model = gdf_properties.copy()\n",
        "\n",
        "df_model = df_model[df_model['SALE PRICE'] > 0] # Sólo queremos entradas con precios válidos\n",
        "\n",
        "# Eliminamos los outlayers con precio mayor al 90% de los datos.\n",
        "q90 = df_model['SALE PRICE'].quantile(0.90)\n",
        "df_model = df_model[df_model['SALE PRICE'] <= q90]\n",
        "\n",
        "# Hacemos una lista con las variables categóricas de interés que usaremos en el modelo\n",
        "categorical = ['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY']\n",
        "df_model = pd.get_dummies(df_model, columns=categorical, drop_first=True) #Transforma las categóricas a binarias"
      ],
      "metadata": {
        "id": "b8WT8o8LyFdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    'GROSS SQUARE FEET', 'YEAR BUILT',\n",
        "    'dist_real_subway', 'shootings_in_radius', 'dist_real_hospital', 'dist_real_university', 'green_area_in_radius', 'flood_points_in_radius'] \\\n",
        "     + [col for col in df_model.columns if any(PREFIJO in col for PREFIJO in ['BOROUGH_', 'NEIGHBORHOOD_', 'BUILDING CLASS CATEGORY_'])]\n",
        "# Para las columnas que pueden tener nombres similares, hacemos un prefijo en un list comprehension para poder\n",
        "# buscar de mejor manera, más compactamente y claro.\n",
        "\n",
        "target = 'SALE PRICE' # Nuestro objetivo va encaminado al precio  de venta!\n",
        "X = df_model[features]\n",
        "y = np.log(df_model[target])\n",
        "\n",
        "# Dividimos los datos y entranamos al modelo con ellos, código genérico:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "rf_model = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluamos con una función logarítmica, la linealidad no siempre funciona bien\n",
        "y_pred_log = rf_model.predict(X_test)\n",
        "y_pred_actual = np.exp(y_pred_log)\n",
        "y_test_actual = np.exp(y_test)\n",
        "\n",
        "# Calculamos el error absoluto y el coef de determinación para evaluar al final:\n",
        "mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
        "r2 = r2_score(y_test_actual, y_pred_actual)\n",
        "\n",
        "print(f\"Error absoluto medio (MAE): ${mae:,.2f}\")\n",
        "print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
        "\n",
        "# Si el R2 es negativo o cercano a 0, el modelo no puede explicar variaciones en los precios de las viviendas!\n",
        "print(f\"El modelo puede explicar aproximadamente el {r2:.0%} de la variación en los precios.\")"
      ],
      "metadata": {
        "id": "le9vkS8fjtTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para apreciar de mejor manera el error obtenido, vamos a mirar la media y mediana de los precios de venta de nuestro dataset principal:"
      ],
      "metadata": {
        "id": "xOkX21cFeYJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prices_filtered = gdf_properties[gdf_properties['SALE PRICE'] > 0] # Filtramos por precio mayor a 0.\n",
        "\n",
        "avg_price = df_prices_filtered['SALE PRICE'].mean()\n",
        "median_price = df_prices_filtered['SALE PRICE'].median()\n",
        "\n",
        "print(f\"Precio Promedio: ${avg_price:,.1f}\")\n",
        "print(f\"Precio Mediano:  ${median_price:,.1f}\")"
      ],
      "metadata": {
        "id": "0vIFqmOgbPIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos un gráfico de las variables del modelo, para visualizar las variables más determinantes y su impacto."
      ],
      "metadata": {
        "id": "l4GUNKZc5Kmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gráfico de variables y su importancia, generado con AI ---\n",
        "importances = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "importances.tail(20).plot(kind='barh', color='skyblue')\n",
        "plt.title('Top 20 variables determinantes')\n",
        "plt.xlabel('Importancia de de la variable')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xANxHiSm5Ufj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la librería IPhython.display, hacemos un mapa interactivo en el que el usuario puede mover los valores con los sliders, para poder encontrar las viviendas que se han vendido según ciertos parámetros, y ver como el modelo ajusta un precio aproximado, actualizando seguidamente el mapa con los puntos de las propiedades vendidas en ese rango de valores elegidos, con un threshold, esto es, un margen de libertad para la variable elegida, de 0.8. Es decir, el valor elegido para cada parámetro puede variar entre un 80% más o menos.\n",
        "\n",
        "https://en.moonbooks.org/Articles/How-to-clear-the-output-in-a-Jupyter-Notebook-cell-after-each-for-loop-iteration-/\n",
        "\n",
        "https://ipython.readthedocs.io/en/9.0.2/api/generated/IPython.display.html\n",
        "\n",
        "https://medium.com/datasciencearth/map-visualization-with-folium-d1403771717"
      ],
      "metadata": {
        "id": "n_8YyS2BqudW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Con la librerías importadas, hacemos unos sliders para\n",
        "sq_ft_slider = widgets.IntSlider(value=4300, min=300, max=5000, step=50, description='Superficie (sq ft):')\n",
        "subway_slider = widgets.IntSlider(value=400, min=0, max=3000, step=10, description='Dist. Metro (m):')\n",
        "hospital_slider = widgets.IntSlider(value=600, min=0, max=5000, step=50, description='Dist. Hospital (m):')\n",
        "university_slider = widgets.IntSlider(value=600, min=0, max=5000, step=50, description='Dist. Universidad (m):')\n",
        "\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def predict_and_map(sq_ft, dist_subway, dist_hospital, dist_university):\n",
        "    # El output será:\n",
        "    with output_widget:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        input_data = pd.DataFrame(0, index=[0], columns=X_train.columns)\n",
        "        input_data['GROSS SQUARE FEET'] = sq_ft\n",
        "        input_data['dist_real_subway'] = dist_subway\n",
        "        input_data['dist_real_hospital'] = dist_hospital\n",
        "        input_data['dist_real_university'] = dist_university\n",
        "\n",
        "        predicted_log_price = rf_model.predict(input_data)\n",
        "        predicted_price = np.exp(predicted_log_price)[0]\n",
        "\n",
        "        print(\"---------------------------------------------------------\")\n",
        "        print(f\"PRECIO ESTIMADO: ${predicted_price:,.2f}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "        tolerance = .8 # Definimos un rango de tolerancia para las variables numéricas, en este caso +-80% del valor elegido\n",
        "\n",
        "        min_price_range = predicted_price * (1 - tolerance)\n",
        "        max_price_range = predicted_price * (1 + tolerance)\n",
        "\n",
        "        # Filtramos el dataset para encontrar propiedades que coincidan con TODOS los criterios\n",
        "        similar_properties = gdf_properties[\n",
        "            (gdf_properties['SALE PRICE'] >= min_price_range) &\n",
        "            (gdf_properties['SALE PRICE'] <= max_price_range) &\n",
        "            (gdf_properties['GROSS SQUARE FEET'].between(sq_ft * (1-tolerance), sq_ft * (1+tolerance))) &\n",
        "            (gdf_properties['dist_real_subway'].between(dist_subway * (1-tolerance), dist_subway * (1+tolerance))) &\n",
        "            (gdf_properties['dist_real_hospital'].between(dist_hospital * (1-tolerance), dist_hospital * (1+tolerance))) &\n",
        "            (gdf_properties['dist_real_university'].between(dist_university * (1-tolerance), dist_university * (1+tolerance)))\n",
        "        ]\n",
        "\n",
        "        if similar_properties.empty:\n",
        "            print(\"No se encontraron propiedades con estos parámetros.\")\n",
        "        else:\n",
        "            print(f\"Se encontraron {len(similar_properties)} propiedades con estos parámetros!\")\n",
        "\n",
        "            # Creamos un folium y añadimos los puntos para las propiedades encontradas, con un popup para cada uno\n",
        "            map_results = folium.Map(location=[40.78, -73.96], zoom_start=12, tiles=\"CartoDB positron\") # Centrado en Manhattan, el centro!\n",
        "            for _, row in similar_properties.iterrows():\n",
        "                popup_html = f\"\"\"\n",
        "                    <b><u>Información de la propiedad vendida:</u></b><br>\n",
        "                    <b>Dirección:</b> {row['ADDRESS']}<br>\n",
        "                    <b>Barrio:</b> {row['NEIGHBORHOOD']}<br>\n",
        "                    <hr style='margin: 5px 0;'>\n",
        "                    <b>Precio de venta:</b> ${row['SALE PRICE']:,.0f}<br>\n",
        "                    <b>Superficie en pies cuadrados (FT):</b> {row['GROSS SQUARE FEET']:.0f}<br>\n",
        "                    <b>Año de construcción:</b> {row['YEAR BUILT']:.0f}<br>\n",
        "                    <hr style='margin: 5px 0;'>\n",
        "                    <b><u>Análisis Geoespacial:</u></b><br>\n",
        "                    <b>Dist. a Metro más cercano:</b> {row['dist_real_subway']:.0f} m<br>\n",
        "                    <b>Dist. a Hospital más cercano:</b> {row['dist_real_hospital']:.0f} m<br>\n",
        "                    <b>Dist. a Universidad más cercana:</b> {row['dist_real_university']:.0f} m<br>\n",
        "                    <b>Tiroteos cercanos en el periodo de estudio:</b> {int(row['shootings_in_radius'])}<br>\n",
        "                    <b>Puntos de inundación cercanos en el periodo de estudio:</b> {int(row['flood_points_in_radius'])}<br>\n",
        "                    <b>Área verde en un area de 1km:</b> {int(row['green_area_in_radius'])} m^2<br>\n",
        "                \"\"\"\n",
        "                                # Creamos el objeto Popup aquí, definiendo el ancho max como 600\n",
        "                popup = folium.Popup(popup_html, max_width=600)\n",
        "\n",
        "                # Agregamos el popup generado en este ciclo al objeto CircleMarker de folium\n",
        "                folium.CircleMarker(\n",
        "                    location=[row.geometry.y, row.geometry.x],\n",
        "                    radius=6, color='red', fill=True, fill_color='violet',\n",
        "                    popup=popup\n",
        "                ).add_to(map_results)\n",
        "\n",
        "            print(\"\\nMostrando mapa con las ubicaciones de las propiedades encontradas...\")\n",
        "            display(map_results)\n",
        "\n",
        "# Usamos los widgets generados, cada array contiene los sliders\n",
        "widgets_panel = widgets.VBox([\n",
        "    widgets.HBox([subway_slider, sq_ft_slider]),\n",
        "    widgets.HBox([hospital_slider, university_slider]),\n",
        "])\n",
        "\n",
        "widgets.interactive_output(predict_and_map, {\n",
        "    'sq_ft': sq_ft_slider, 'dist_subway': subway_slider,'dist_hospital': hospital_slider, 'dist_university': university_slider\n",
        "})\n",
        "\n",
        "display(widgets_panel, output_widget)"
      ],
      "metadata": {
        "id": "zMZbRGOm8UUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, mostramos una simple visualización dinámica que va cambiando en el tiempo según se agregan parámetros al modelo, la idea es ver qué tanto va a afectando, en el paso del tiempo, la agregación de parámetros determinantes en el modelo predictivo.\n",
        "\n",
        "https://matplotlib.org/stable/users/explain/animations/animations.html\n",
        "\n",
        "https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html"
      ],
      "metadata": {
        "id": "_tPY4r73JH8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Copiamos el código del modelo de arriba, para poder producirlo en una visualización dinámica.\n",
        "df_model = gdf_properties.copy()\n",
        "df_model.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_model.dropna(subset=['SALE PRICE', 'GROSS SQUARE FEET', 'YEAR BUILT', 'dist_real_subway', 'shootings_in_radius',\\\n",
        "                        'dist_real_hospital', 'BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY', 'dist_real_university',\\\n",
        "                        'green_area_in_radius', 'flood_points_in_radius'], inplace=True)\n",
        "df_model = df_model[df_model['SALE PRICE'] > 0]\n",
        "q90 = df_model['SALE PRICE'].quantile(0.90)\n",
        "df_model = df_model[df_model['SALE PRICE'] <= q90]\n",
        "categorical = ['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY']\n",
        "df_model_dummies = pd.get_dummies(df_model, columns=categorical, drop_first=True)\n",
        "\n",
        "\n",
        "# Organizamos los parámetros en orden, para ver cómo van afectando al modelo.\n",
        "parameters = [\n",
        "    ['GROSS SQUARE FEET'],\n",
        "    ['YEAR BUILT'],\n",
        "    ['dist_real_subway'],\n",
        "    ['shootings_in_radius'],\n",
        "    ['dist_real_hospital'],\n",
        "    ['dist_real_university'],\n",
        "    ['green_area_in_radius'],\n",
        "    ['flood_points_in_radius'],\n",
        "    [col for col in df_model_dummies.columns if 'BOROUGH_' in col],\n",
        "    [col for col in df_model_dummies.columns if 'NEIGHBORHOOD_' in col],\n",
        "    [col for col in df_model_dummies.columns if 'BUILDING CLASS CATEGORY_' in col]\n",
        "]"
      ],
      "metadata": {
        "id": "no4rNhavJNSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5)) # Definimos la visualización dinámica.\n",
        "current_parameters = [] # Este array irá cambiando con los parámetros\n",
        "\n",
        "# Hacemos una funci[on para cambiar producir el video con cada cambio.\n",
        "def update_vis(frame_index):\n",
        "    global current_parameters # No es buena práctica tener variables globales, pero en este caso no afecta negativamente.\n",
        "\n",
        "    #current_parameters.extend(parameters[frame_index])\n",
        "\n",
        "    # Este bucle for in nos permite eliminar duplicados que aparecían con la línea previa, comentada.\n",
        "    for param in parameters[frame_index]:\n",
        "        if param not in current_parameters:\n",
        "            current_parameters.append(param)\n",
        "\n",
        "    # Datos del modelo.\n",
        "    X = df_model_dummies[current_parameters]\n",
        "    y = np.log(df_model_dummies['SALE PRICE'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluamos nuevamente con la función logarítmica\n",
        "    y_pred_log = rf_model.predict(X_test)\n",
        "    y_pred_actual = np.exp(y_pred_log)\n",
        "    y_test_actual = np.exp(y_test)\n",
        "\n",
        "    # Calculamos el MAE y el Coeficiente\n",
        "    mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
        "    r2 = r2_score(y_test_actual, y_pred_actual)\n",
        "\n",
        "    ax.clear() # En cada loop, limpiamos el gráfico.\n",
        "\n",
        "    # El nuevo resultado de las variables\n",
        "    importances = pd.Series(rf_model.feature_importances_, index=current_parameters).sort_values(ascending=True)\n",
        "    importances.tail(20).plot(kind='barh', ax=ax, color='skyblue')\n",
        "\n",
        "    # Nueva vis\n",
        "    title = f\"Paso {frame_index + 1}/11\\n\"\n",
        "    title += f\"MAE: ${mae:,.0f}: {r2:.2f}\"\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.set_xlabel('Importancia relativa de las variables')\n",
        "\n",
        "# Lo siguiente no puede ir por separado, creamos la animación con el método FuncAnimation con un intervalo en ms\n",
        "anim = FuncAnimation(fig, update_vis, frames=len(parameters), interval=1500, repeat=False)\n",
        "\n",
        "html_video = anim.to_html5_video() # Lo pasamos a un vídeo con soporte en HTML para que se pueda ver\n",
        "plt.close() # Eliminamos la visualización estática porque se ve abajo y molesta.\n",
        "\n",
        "display(HTML(html_video))"
      ],
      "metadata": {
        "id": "305iom_WJZdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}